{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4j05XaIOUvcf"
      },
      "source": [
        "#  Assignment 1 - Language model foundations üí¨\n",
        "\n",
        "Welcome to the **1st assignment** for the **CS-552: Modern NLP course**!\n",
        "\n",
        "> - üòÄ Name: **Rein**\n",
        "> - ‚úâÔ∏è Email: **rein.bentdal@epfl.ch**\n",
        "> - ü™™ SCIPER: **366666**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0bVT0KPUvck"
      },
      "source": [
        "<div style=\"padding:15px 20px 20px 20px;border-left:3px solid orange;background-color:#fff5d6;border-radius: 20px;\">\n",
        "\n",
        "## How to implement this assignment\n",
        "\n",
        "Please read carefully the following points. All the information on how to read, implement and submit your assignment is explained in detail below.\n",
        "\n",
        "1. For this assignment, you will need to implement and fill in the missing code snippets for both the **Jupyter Notebook** `assignment1.ipynb` and the **`utils.py`** python file. In the `utils.py` file, you will add all the Dataset and Model classes you will implement according to the skeleton present in the file. In the notebook, you will add the data preprocessing pipeline for all the datasets, the training and testing pipelines of all implemented models and the report (See diagram below). \n",
        "    \n",
        "![assignment_1_arch.png](https://github.com/CS-552/a1-ReinBentdal/blob/main/docs/assignment_1_arch.png?raw=1)\n",
        "\n",
        "2. To implement your coding part, you can import the external libraries we provide in the `requirements.txt` file, however, you should not use any other package non included in these requirements. \n",
        "\n",
        "3. At the end of the notebook, you will need to fill in a **report** template, providing the results of your implementation. We provide you with the template for the report, therefore you need to fill in the missing Markdown cells with the requested information. \n",
        "\n",
        "4. Along with the `assignment1.ipynb` and the `utils.py` files, you need to additionally upload models' pickle files under the `models/` dir, regarding the following models:\n",
        "    - the three LSTM-variant models (PART 2)  \n",
        "    - the trained-from-scratch Transformer model (PART 2) \n",
        "    - the fine-tuned Encoder-Decoder model (PART 3) \n",
        "    - the fine-tuned pre-trained Transformer model (PART 3)\n",
        "    \n",
        "You will provide test results on all of the model variants according to the report template.\n",
        "    \n",
        "5. Finally, you will need to log your training pipelines using Tensorboard. Please follow the instructions in the `README.md` of the [tensorboard/](tensorboard/README.md) directory.\n",
        "\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GA3bGKeGUvcl"
      },
      "source": [
        "<div style=\"padding:15px 20px 20px 20px;border-left:3px solid green;background-color:#e4fae4;border-radius: 20px;\">\n",
        "\n",
        "## Assignment Description\n",
        "\n",
        "- In the first two parts of this assignment, you need to train and evaluate two different language models; an **LSTM-based model** and a **Transformer-based model**. You will first explore the distribution of the input data and perform data cleaning and pre-processing. Then you will build two language model training and testing pipelines implementing an LSTM and a Transformer language model. You will play around with different hyperparameters.\n",
        "- In the third part, you will build models on the downstream task of **Sentence Paraphrasing**. More specifically, you will fine-tune a sequence-2-sequence (**Encoder-Decoder**) architecture with attention and you will also fine-tune a **Transformer** model for this task.\n",
        "- Finally, you will fill out a report with the model results for the different parts. \n",
        "\n",
        "More specifically:\n",
        "\n",
        "- **[PART 1: Get to know your data](#1)**\n",
        "    - [1.1 Data Pre-processing](#11)\n",
        "    - [1.2 PyTorch Dataset creation](#12)\n",
        "- **[PART 2: Training Language Models](#2)**\n",
        "    - [2.1 LSTM-variants](#21)\n",
        "    - [2.2 Transformer-variants](#22)\n",
        "- **[PART 3: Fine-tune on the Text Paraphrasing task](#3)**\n",
        "    - [3.1 Train an Encoder-Decoder model on Text Paraphrasing](#31)\n",
        "    - [3.2 Run Transformer on Text Paraphrasing](#32)\n",
        "- **[PART 4: Write your report](#4)**\n",
        "    \n",
        "### Deliverables\n",
        "\n",
        "- ‚úÖ This Jupyter notebook\n",
        "- ‚úÖ `utils.py` file\n",
        "- ‚úÖ 3 pickle files with the three LSTM-variant language models (Part 2)\n",
        "- ‚úÖ Pickle file with the trained-from-scratch Transformer language model (Part 2)\n",
        "- ‚úÖ Pickle file with the Encoder-Decoder model (Part 3)\n",
        "- ‚úÖ Pickle file with the fine-tuned pre-trained Transformer model (Part 3)\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBdny5anryRz"
      },
      "source": [
        "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
        "\n",
        "üéØ Goal: **Add your SCIPER number below as a `str`!**\n",
        "     \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgVhlFcenHY0",
        "outputId": "12ca5dd3-39ad-49ad-a5b5-11cea22c2000"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "test"
      ],
      "metadata": {
        "id": "07RjWTzDZq4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "base_dir = '/content/drive/My Drive/Colab Notebooks/Modern natural language processing/a1-ReinBentdal'\n",
        "sys.path.append(f'{base_dir}')"
      ],
      "metadata": {
        "id": "r2345E-An6Z3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UEgVxw_sIrn"
      },
      "outputs": [],
      "source": [
        "import regex as re\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "SCIPER = \"366666\"\n",
        "\n",
        "try:\n",
        "    assert re.match(\"\\d{6}\", SCIPER)[0] == SCIPER, \"Invalid SCIPER given. please enter your correct 6-digit SCIPER number above!\"\n",
        "except:\n",
        "    print(\"Invalid SCIPER given. please enter your correct 6-digit SCIPER number above!\")\n",
        "\n",
        "student_seed = int(SCIPER[-4:])\n",
        "\n",
        "\n",
        "\"\"\"Set seed for reproducibility.\"\"\"\n",
        "random.seed(student_seed)\n",
        "np.random.seed(student_seed)\n",
        "torch.manual_seed(student_seed)\n",
        "torch.cuda.manual_seed_all(student_seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEXVikwvk-kp"
      },
      "source": [
        "### Packgage installation & importing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eiPc1rRNiS__"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" # limiting to one GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uexGOS0GiS__",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "978ca0e9-6744-45f8-d22f-7e4a1d4ff43d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.9/dist-packages (2.10.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (2023.3.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets) (1.4.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets) (23.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (2.25.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.13.2)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.9.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: apache_beam in /usr/local/lib/python3.9/dist-packages (2.46.0)\n",
            "Requirement already satisfied: regex>=2020.6.8 in /usr/local/lib/python3.9/dist-packages (from apache_beam) (2022.6.2)\n",
            "Requirement already satisfied: cloudpickle~=2.2.1 in /usr/local/lib/python3.9/dist-packages (from apache_beam) (2.2.1)\n",
            "Requirement already satisfied: numpy<1.25.0,>=1.14.3 in /usr/local/lib/python3.9/dist-packages (from apache_beam) (1.22.4)\n",
            "Requirement already satisfied: proto-plus<2,>=1.7.1 in /usr/local/lib/python3.9/dist-packages (from apache_beam) (1.22.2)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from apache_beam) (1.3.0)\n",
            "Requirement already satisfied: httplib2<0.22.0,>=0.8 in /usr/local/lib/python3.9/dist-packages (from apache_beam) (0.17.4)\n",
            "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /usr/local/lib/python3.9/dist-packages (from apache_beam) (3.13.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /usr/local/lib/python3.9/dist-packages (from apache_beam) (2.25.1)\n",
            "Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.9/dist-packages (from apache_beam) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.9/dist-packages (from apache_beam) (2.8.2)\n",
            "Collecting dill<0.3.2,>=0.3.1.1\n",
            "  Using cached dill-0.3.1.1-py3-none-any.whl\n",
            "Requirement already satisfied: orjson<4.0 in /usr/local/lib/python3.9/dist-packages (from apache_beam) (3.8.7)\n",
            "Requirement already satisfied: fasteners<1.0,>=0.3 in /usr/local/lib/python3.9/dist-packages (from apache_beam) (0.18)\n",
            "Requirement already satisfied: typing-extensions>=3.7.0 in /usr/local/lib/python3.9/dist-packages (from apache_beam) (4.5.0)\n",
            "Requirement already satisfied: fastavro<2,>=0.23.6 in /usr/local/lib/python3.9/dist-packages (from apache_beam) (1.7.3)\n",
            "Requirement already satisfied: grpcio!=1.48.0,<2,>=1.33.1 in /usr/local/lib/python3.9/dist-packages (from apache_beam) (1.51.3)\n",
            "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /usr/local/lib/python3.9/dist-packages (from apache_beam) (2.7.0)\n",
            "Requirement already satisfied: zstandard<1,>=0.18.0 in /usr/local/lib/python3.9/dist-packages (from apache_beam) (0.20.0)\n",
            "Requirement already satisfied: protobuf<4,>3.12.2 in /usr/local/lib/python3.9/dist-packages (from apache_beam) (3.19.6)\n",
            "Requirement already satisfied: objsize<0.7.0,>=0.6.1 in /usr/local/lib/python3.9/dist-packages (from apache_beam) (0.6.1)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.9/dist-packages (from apache_beam) (1.7)\n",
            "Requirement already satisfied: pyarrow<10.0.0,>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from apache_beam) (9.0.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from hdfs<3.0.0,>=2.1.0->apache_beam) (1.15.0)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.9/dist-packages (from hdfs<3.0.0,>=2.1.0->apache_beam) (0.6.2)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.9/dist-packages (from pydot<2,>=1.2.0->apache_beam) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.24.0->apache_beam) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.24.0->apache_beam) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.24.0->apache_beam) (1.26.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.24.0->apache_beam) (2.10)\n",
            "Installing collected packages: dill\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.6\n",
            "    Uninstalling dill-0.3.6:\n",
            "      Successfully uninstalled dill-0.3.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "multiprocess 0.70.14 requires dill>=0.3.6, but you have dill 0.3.1.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dill-0.3.1.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.9/dist-packages (0.11.4)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (23.0)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (1.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (4.5.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim==4.1.2 in /usr/local/lib/python3.9/dist-packages (4.1.2)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.9/dist-packages (from gensim==4.1.2) (1.10.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.9/dist-packages (from gensim==4.1.2) (1.22.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from gensim==4.1.2) (6.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.9/dist-packages (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from evaluate) (1.22.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from evaluate) (3.2.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (2023.3.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (2.25.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from evaluate) (23.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from evaluate) (4.65.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.3.1.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.70.14)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.13.2)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from evaluate) (1.4.4)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (2.10.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate) (3.8.4)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.9.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (2022.12.7)\n",
            "Collecting dill\n",
            "  Using cached dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->evaluate) (2022.7.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.2.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (3.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.15.0)\n",
            "Installing collected packages: dill\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.1.1\n",
            "    Uninstalling dill-0.3.1.1:\n",
            "      Successfully uninstalled dill-0.3.1.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dill-0.3.6\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "!pip install apache_beam\n",
        "!pip install torchmetrics\n",
        "!pip install gensim==4.1.2\n",
        "!pip install transformers\n",
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZ26xAfpk-kr"
      },
      "outputs": [],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "tb_writer = SummaryWriter(log_dir='tensorboard/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0bmJE-0iTAA"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "import gensim\n",
        "import torch\n",
        "import math\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchmetrics.classification import BinaryAccuracy\n",
        "from transformers import GPT2LMHeadModel, GPT2TokenizerFast, AutoConfig, AutoModelWithLMHead, AutoTokenizer, DataCollatorForLanguageModeling, Trainer, TrainingArguments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m06WLo6qUvcm"
      },
      "source": [
        "---\n",
        "\n",
        "<a name=\"1\"></a>\n",
        "# PART 1: Get to know your data üîé\n",
        "\n",
        "For the first two parts of this assignment, we will build our language models using the `wikitext-103` dataset.\n",
        "\n",
        "> The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified \n",
        "Good and Featured articles on Wikipedia. \n",
        "\n",
        "Bellow is an example from the dataset: \n",
        "<br>_(This example was too long and was cropped)_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdwMZ-OUiTAB"
      },
      "source": [
        "\n",
        "\n",
        "<div style=\"padding:8px 0 8px 15px;background-color:#F3F3F3;border-radius:20px;\">\n",
        "<code>{\n",
        "\"text\": \"\\\" The Sinclair Scientific Programmable was introduced in 1975 , with the same case as the Sinclair Oxford . It was larger than t...\"\n",
        "}\n",
        "</code>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRxt_WZBiTAB"
      },
      "source": [
        "üßê You can find more about this dataset [here](https://huggingface.co/datasets/wikitext)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOD9aSZIiTAC"
      },
      "source": [
        "<a name=\"11\"></a>\n",
        "## 1.1 Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7A0sYxb8klR"
      },
      "source": [
        "In this part, while you get to better understand the dataset sturcuture, you will also do several steps to clean the dataset before passing them to neural models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADe8a4xOUvcm",
        "outputId": "c88223b0-34a4-48d7-ef54-42ad1db5f2a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset wikitext (/root/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of the dataset is 1801350\n"
          ]
        }
      ],
      "source": [
        "# Loads the dataset\n",
        "wikitext_dataset = load_dataset(\"wikitext\", 'wikitext-103-v1', split=\"train\")\n",
        "\n",
        "print(f\"Size of the dataset is {len(wikitext_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfMJBW7OAnj-"
      },
      "source": [
        "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
        "\n",
        "üéØ Goal: **Filter out all empty sentences**.\n",
        "\n",
        "üíª API hint: Use `datasets.Dataset` utilities to manipulate the dataframe.\n",
        "     \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXz9W7tZiTAD",
        "outputId": "248d06f1-52aa-4268-a3e6-85ade74e784c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-49d0acf4a1230791.arrow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of the dataset is 1165029\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "wikitext_dataset = wikitext_dataset.filter(lambda x: x['text'] != '')\n",
        "\n",
        "print(f\"Size of the dataset is {len(wikitext_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdr5LIY5Dym3"
      },
      "source": [
        "Long sequences in the language model training can significantly slow down the training progress, both for RNN-based and transformer-based models.\n",
        "\n",
        "One of the tricks that is mentioned in [BERT's paper](https://arxiv.org/abs/1810.04805) is to perform pretraining with shorter sequences in the beginning. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sIemLH6iTAF"
      },
      "source": [
        "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
        "\n",
        "üéØ Goal: Following the same line of reasoning, **keep only samples that have at most 128 tokens**.\n",
        "    \n",
        "üíª API hint: Use `datasets.Dataset` utilities to manipulate the dataframe.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9_xgDXJiTAG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7243ea78-f063-4884-9dad-3e1d570572e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-17d3e269ad64ee62.arrow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of the dataset is 826663\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "# assuming word based tokens\n",
        "wikitext_dataset = wikitext_dataset.filter(lambda x: len(x['text'].split()) <= 128)\n",
        "\n",
        "print(f\"Size of the dataset is {len(wikitext_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9LcAg0EJzrz"
      },
      "source": [
        "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
        "\n",
        "üéØ Goal: Let's make the dataset samples **lower case** to decrease the vocabulary size.\n",
        "\n",
        "üíª API hint: Use `datasets.Dataset` utilities to manipulate the dataframe.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8-Y-7JQiTAH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efe68ef4-d79f-4aba-a359-9b05adb37b37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-cb093374093bd6c7.arrow\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "wikitext_dataset = wikitext_dataset.map(lambda x: {'text': x['text'].lower()})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0nfN9suk-ky",
        "outputId": "024420bf-b3ff-46b8-b714-bfd7435516b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': ' = valkyria chronicles iii = \\n'}\n"
          ]
        }
      ],
      "source": [
        "print(wikitext_dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUjCH6Z-NALB"
      },
      "source": [
        "If you take a look at the first few samples of the dataset, you will notice that they belong to [this](https://en.wikipedia.org/wiki/Valkyria_Chronicles_II) Wikipedia article.\n",
        "\n",
        "We notice that **the title of  sections/articles are also included** in the dataset (for instance the title itself, or the `gameplay` section in this article). These samples are not very useful for language modeling, due to not having a sentence structure.\n",
        "Given the pattern of these samples, we need to **filter them out** from the dataset.\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTuUlhFuiTAI"
      },
      "source": [
        "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
        "\n",
        "üéØ Goal: Filter out the samples with `= = <section> = = \\n` patterns.\n",
        "    \n",
        "üíª API hint: Use `datasets.Dataset` utilities to manipulate the dataframe.\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4snXCQBZiTAJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50f69d4b-02b7-4f6b-bebe-7afc2bc545ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-323a53e9e6e24c0a.arrow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of the dataset is 521520\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "pattern = r'=+ (.*?)=+'\n",
        "\n",
        "wikitext_dataset = wikitext_dataset.filter(lambda x: not bool(re.search(pattern, x['text'])))\n",
        "\n",
        "print(f\"Size of the dataset is {len(wikitext_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16AHClg64IE6"
      },
      "source": [
        "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
        "\n",
        "üéØ Goal:  **Normalize accented letters** (e.g., `cl√©ment` becomes `clement`) from text using `gensim.utils.deaccent` to further decrease vocabulary size.\n",
        "    \n",
        "üíª API hint: Use `datasets.Dataset` utilities to manipulate the dataframe. \n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_IHLtBriTAK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd594126-f606-40a2-ac76-0e3ef4d5d886"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-f149df46c29aff19.arrow\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "wikitext_dataset = wikitext_dataset.map(lambda x: {'text': gensim.utils.deaccent(x['text'])})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiD093zf5ie_"
      },
      "source": [
        "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
        "\n",
        "üéØ Goal: Remove all samples having **non-english characters**. \n",
        "   \n",
        "üíª API hint: Use `datasets.Dataset` utilities to manipulate the dataframe along with the provided function `isEnglish()`. \n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xf0eznSxiTAL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a48f1bb0-138b-4143-8dc6-957fde014b4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-ce691f83f1ede5d7.arrow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of the dataset is 432434\n"
          ]
        }
      ],
      "source": [
        "def isEnglish(s):\n",
        "    try:\n",
        "        s.encode(encoding='utf-8').decode('ascii')\n",
        "    except UnicodeDecodeError:\n",
        "        return False\n",
        "    else:\n",
        "        return True\n",
        "\n",
        "# YOUR CODE HERE\n",
        "wikitext_dataset = wikitext_dataset.filter(lambda x: isEnglish(x['text']))\n",
        "\n",
        "print(f\"Size of the dataset is {len(wikitext_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dboQyQoD9jVx"
      },
      "source": [
        "### Looking at the vocabulary\n",
        "\n",
        "Before we move into additional preprocessing (similarly with the dataset used in exercises for week 2), we will take a look at the vocabulary size of the dataset until this point. We will assume that tokens can be simply splitted by `\" \"`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TU6bUxgGiTAM"
      },
      "source": [
        "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
        "\n",
        "üéØ Goal:  **Compute the frequency of all tokens in the dataset.**\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1M0zpagYiTAM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51,
          "referenced_widgets": [
            "f6af81c6d3e64bec96b00107ed8907b1",
            "687c43a5a5ef4487a1d7f513b68426e1",
            "0681cf2e39274366bbea0c7f6321025d",
            "c75c377f24a04bfdabf87a7e2abf9aad",
            "89eb2c4a7f4443cb905834933285e8ae",
            "70eb72e4013c4161baf971ef72f5b5c0",
            "5207855594244b4f9ca9c6efee6b92e7",
            "f385584a70a345f48304492ae47d3721",
            "474e8a45002a4a7ba2c6fb7b7bf31db6",
            "7df46e35c8494bfb95768fe30ec99419",
            "10c65e32626f412e8722f991f9df83d5"
          ]
        },
        "outputId": "cf8f5f0a-7e12-4975-835d-dfe2121293df"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/432434 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f6af81c6d3e64bec96b00107ed8907b1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Vocabulary size of the dataset is 189365\n"
          ]
        }
      ],
      "source": [
        "def compute_token_frequency(dataset):\n",
        "  # YOUR CODE HERE\n",
        "    vocab_frequency = {}\n",
        "\n",
        "    def word_add(s):\n",
        "      s_list = s['text'].split()\n",
        "      for w in s_list:\n",
        "        vocab_frequency[w] = vocab_frequency.get(w, 0) + 1\n",
        "\n",
        "    dataset.map(word_add)\n",
        "\n",
        "    return vocab_frequency\n",
        "\n",
        "vocab_frequency = compute_token_frequency(wikitext_dataset)\n",
        "print(f\"\\nVocabulary size of the dataset is {len(vocab_frequency)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1XjEaDvWJtB"
      },
      "source": [
        "As discussed in the lectures, real text datasets have a relatively high fraction of rare tokens. For that reason, let's visualize the histogram of token frequencies to better see this effect."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dU97c1miTAN"
      },
      "source": [
        "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
        "\n",
        "üéØ Goal: Plot a **histogram** with the frequencies of the words of the vocabulary. \n",
        "   \n",
        "üíª API hint: You can use the `matplotlib.hist` function. \n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBKUzDEDiTAN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "c535fff6-860e-48be-890a-8ccec99d9efe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 30 artists>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "vocab_frequency_tuple = sorted(vocab_frequency.items(), key=lambda x: x[1], reverse=True)\n",
        "words = [x[0] for x in vocab_frequency_tuple]\n",
        "freqs = [x[1] for x in vocab_frequency_tuple]\n",
        "\n",
        "plt.bar(words[:30], freqs[:30])\n",
        "\n",
        "# i could have plotted by how likely a word is to only appear aswell, giving a massive height for the \"once\" tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-vDgr_5HyrE"
      },
      "source": [
        "As you saw in the cells above, the dataset vocabulary is quite huge. Let's consider every token that occurs less than (or equal to) 5 times as a rare token. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jf5G9QFyiTAN"
      },
      "source": [
        "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
        "\n",
        "üéØ Goal: Put these rare tokens in the **`rare_tokens` variable** and replace every rare token in the dataset with the `<unk>` token.\n",
        "    \n",
        "üíª API hint: Use `datasets.Dataset` utilities to manipulate the dataframe.\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRWJFfThiTAO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60bc78cd-d458-4bbe-b591-39d0081b3759"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-8eec49e7913870c4.arrow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "With threshold of 5, we have 0 rare tokens.\n",
            " The vocabulary size is now 189365\n"
          ]
        }
      ],
      "source": [
        "rare_threshold = 5\n",
        "rare_tokens = set()\n",
        "\n",
        "# YOUR CODE HERE\n",
        "def remove_rare_tokens(x):\n",
        "      t_list = x['text'].split()\n",
        "      for i, t in enumerate(t_list):\n",
        "            if vocab_frequency.get(t, 0) <= 5:\n",
        "                  t_list[i] = '<unk>'\n",
        "                  rare_tokens.add(t)\n",
        "      return {'text': ' '.join(t_list)}\n",
        "\n",
        "wikitext_dataset = wikitext_dataset.map(remove_rare_tokens)\n",
        "\n",
        "print(f\"With threshold of {rare_threshold}, we have {len(rare_tokens)} rare tokens.\\n\",\n",
        "      f\"The vocabulary size is now {len(vocab_frequency) - len(rare_tokens)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fu2W9EMiWrP"
      },
      "source": [
        "The dataset still includes many short samples which are not very useful for the language modeling task. We will filter out very short samples from the dataset. _(Note: Assume tokens can be achieved by simple `\" \"` splitting.)_\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruh0XqGKiTAO"
      },
      "source": [
        "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
        "\n",
        " üéØ Goal: Filter out every sample that has **less than (or equal to) 5 tokens**.\n",
        "    \n",
        "üíª API hint: Use `datasets.Dataset` utilities to manipulate the dataframe.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QMmBOwjiTAO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec96c07c-bc1f-491e-eedf-ce8c94880304"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-ae94ee309388d7bc.arrow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of the dataset is 401669\n"
          ]
        }
      ],
      "source": [
        "short_seq_threshold = 5\n",
        "\n",
        "# YOUR CODE HERE\n",
        "wikitext_dataset = wikitext_dataset.filter(lambda x: len(x['text'].split()) > 5)\n",
        "\n",
        "print(f\"Size of the dataset is {len(wikitext_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9wvH77WiTAP"
      },
      "source": [
        "After replacing rare tokens with <unk>, we could have sentences like `<unk> <unk> <unk> <unk>` which are again not very useful for language modeling. We will **filter out samples that more than 5% of its tokens are `<unk>`**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoC2HrwKlalG"
      },
      "source": [
        "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
        "\n",
        "üéØ Goal:  **Filter out samples that more than 5% of its tokens are `<unk>`**\n",
        "    \n",
        "üíª API hint: Use `datasets.Dataset` utilities to manipulate the dataframe.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdiXr9p0iTAP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c96df26-707c-4ed3-efec-a1bf62ea07ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-5fe1ff11e48112e8.arrow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of the dataset is 362607\n"
          ]
        }
      ],
      "source": [
        "unknown_token_threshold = 0.05  # every sample that more than 5% of its tokens are <unk> should be removed\n",
        "\n",
        "# YOUR CODE HERE\n",
        "wikitext_dataset = wikitext_dataset.filter(lambda x: x['text'].split().count('<unk>') / len(x['text'].split()) < unknown_token_threshold)\n",
        "\n",
        "print(f\"Size of the dataset is {len(wikitext_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGtIxEa07-8K"
      },
      "source": [
        "Let's recalculate the vocabulary for the resulting dataset, to see the vocabulary of the resulting dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71,
          "referenced_widgets": [
            "f488326046cb4b21af5b5442ee6dee2a",
            "65a3aa92603b4e5c8de4bc521d062da4",
            "d1e44425f4fe4d0dae39fe06b56bd084",
            "244464bc81d34b50b4c1990bde6189c7",
            "e5e8e9bf6ddd49649e614ac10a37bee2",
            "ca2d9b64f55a4271b6ac9372804bc4a7",
            "2204d598eced4892958d56533f7e323c",
            "358aea7c7dca490b9af79202fe330939",
            "982635f64cf4436893162236db8878e6",
            "a04be79799414c9c8b78e00d6f9ae8c6",
            "38a53dfbf1a24c63b76dd8f866640036"
          ]
        },
        "id": "q3-WohMHlXIo",
        "outputId": "8157f93e-a9a7-4f17-f28d-c34e0a6a139b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/362607 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f488326046cb4b21af5b5442ee6dee2a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "vocabulary size of the dataset is 83392\n"
          ]
        }
      ],
      "source": [
        "vocab_frequency = compute_token_frequency(wikitext_dataset)\n",
        "\n",
        "print(f\"\\nvocabulary size of the dataset is {len(vocab_frequency)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhT1WIUYk-k9",
        "outputId": "2124a691-28aa-418b-8594-10beb71551cb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': \"it met with positive sales in japan , and was praised by both japanese and western critics . after release , it received downloadable content , along with an expanded edition in november of that year . it was also adapted into manga and an original video animation series . due to low sales of valkyria chronicles ii , valkyria chronicles iii was not localized , but a fan translation compatible with the game 's expanded edition was released in 2014 . <unk> would return to the franchise with the development of valkyria : azure revolution for the playstation 4 .\"}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "wikitext_dataset[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07Q5by1kiTAQ"
      },
      "source": [
        "---\n",
        "\n",
        "<a name=\"12\"></a>\n",
        "## 1.2 PyTorch Dataset  creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXgKUIGbSdAu"
      },
      "source": [
        "After the pre-processing of the dataset, we will now create a `torch.Dataset` class for the wiki-text dataset. \n",
        "We need to do so, in order to transform the dataset to the right format for the language modeling task. \n",
        "The following steps should be implemented:\n",
        "\n",
        "- Add `<start>` and `<stop>` tokens at the beginning and end of a sentence respectively.\n",
        "- Add padding tokens ( `<pad>` ) at the end of the sentences.\n",
        "- Create a fallback to <unk> token if an unseen word is encoded.\n",
        "- Define dictionaries that map tokens to their respective index in the embedding matrix and vice versa.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtKKcjq4iTAR"
      },
      "source": [
        "### Create the RNN Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYcATZ2XiTAR"
      },
      "source": [
        "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
        "\n",
        "üéØ Goal:  **Go to the `utils.py` file, and fill in the `RNNDataset` class with your implemenation.**\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmSdlO_BBbFO"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import datasets\n",
        "import importlib\n",
        "import src.utils\n",
        "importlib.reload(src.utils)\n",
        "from src.utils import RNNDataset\n",
        "\n",
        "MAX_SEQ_LENGTH = 128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NG5flxTZiTAR"
      },
      "source": [
        "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
        "\n",
        "üéØ Goal:  **Instantiate** the implemented RNNDataset.\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvovsTi0iTAS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "e47f84316c8245bf803b8ebf96059ea0",
            "209712e84d1447aebdbe7419316ab5e9",
            "ca437d85f8b640768c4781804a418040",
            "0e11e2d4ab964764beb3c69cae414d5e",
            "1c4c00e452f9426491fa9f22fc20e37d",
            "be9f8ea727cf411480d1c602175947b7",
            "724986e44a2442e499e11e4fc3bec667",
            "ac41c7c0beff4770bedd083c705fcc77",
            "2cf7c4bd748c4ff6a812d91363cc89f0",
            "600db3a945f741f2b175ad81d07525f5",
            "44c3c527aad14caab979d432335900a1"
          ]
        },
        "outputId": "d7414bc1-13f7-49ea-9a44-2f2914ae71c7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/362607 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e47f84316c8245bf803b8ebf96059ea0"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "rnn_dataset = RNNDataset(wikitext_dataset, MAX_SEQ_LENGTH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HaOZ5zFk-lA",
        "outputId": "48e8e057-0101-420d-e394-c41594c63f9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([13924, 24707, 57115,  4604, 60716, 44743, 21674, 32090, 31405, 81833,\n",
            "        56330, 31143, 78078, 11265, 24707, 41163, 22719, 62774, 42555, 15506,\n",
            "        57747, 34377, 13924, 21513,  8509, 52406, 24707, 48551, 75214, 11265,\n",
            "        24707, 45585, 32090,  8509, 57358, 62518, 54268, 27209, 32090, 68099,\n",
            "        29255, 18328, 24707, 57115, 69817, 34156, 79888, 45585, 12370, 34377,\n",
            "        13924, 36664,  4937, 55818, 55818, 46170, 64412, 12418, 22957, 33433,\n",
            "        40324, 72172, 74816, 18430, 32090, 55262, 51267, 42555, 15506, 57747,\n",
            "        48296, 12604, 54388, 34377, 13924, 56330, 31143, 11403, 11265, 18477,\n",
            "        17362, 24707, 68953, 34377, 13924, 24707, 57115, 42948,  1694, 58163,\n",
            "        75204, 38398, 19304,  2648, 39284, 21993, 54035, 11447, 11447, 11447,\n",
            "        11447, 11447, 11447, 11447, 11447, 11447, 11447, 11447, 11447, 11447,\n",
            "        11447, 11447, 11447, 11447, 11447, 11447, 11447, 11447, 11447, 11447,\n",
            "        11447, 11447, 11447, 11447, 11447, 11447, 11447, 11447])\n",
            "tensor([24707, 57115,  4604, 60716, 44743, 21674, 32090, 31405, 81833, 56330,\n",
            "        31143, 78078, 11265, 24707, 41163, 22719, 62774, 42555, 15506, 57747,\n",
            "        34377, 13924, 21513,  8509, 52406, 24707, 48551, 75214, 11265, 24707,\n",
            "        45585, 32090,  8509, 57358, 62518, 54268, 27209, 32090, 68099, 29255,\n",
            "        18328, 24707, 57115, 69817, 34156, 79888, 45585, 12370, 34377, 13924,\n",
            "        36664,  4937, 55818, 55818, 46170, 64412, 12418, 22957, 33433, 40324,\n",
            "        72172, 74816, 18430, 32090, 55262, 51267, 42555, 15506, 57747, 48296,\n",
            "        12604, 54388, 34377, 13924, 56330, 31143, 11403, 11265, 18477, 17362,\n",
            "        24707, 68953, 34377, 13924, 24707, 57115, 42948,  1694, 58163, 75204,\n",
            "        38398, 19304,  2648, 39284, 21993, 54035, 11447, 11447, 11447, 11447,\n",
            "        11447, 11447, 11447, 11447, 11447, 11447, 11447, 11447, 11447, 11447,\n",
            "        11447, 11447, 11447, 11447, 11447, 11447, 11447, 11447, 11447, 11447,\n",
            "        11447, 11447, 11447, 11447, 11447, 11447, 11447, 11447])\n"
          ]
        }
      ],
      "source": [
        "inp, lab = rnn_dataset[0]\n",
        "print(inp)\n",
        "print(lab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_woATTryiTAS"
      },
      "source": [
        "### Split data into train and test\n",
        "\n",
        "Once we have created the dataset ready for the model training pipeline, we will split it into train and test datasets. Then we will pass them to a `DataLoader` class, following the same method we saw in the exercises session. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFevD8HUiTAS"
      },
      "source": [
        "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
        "\n",
        "üéØ Goal:  **Split** the implemented RNNDataset into train and test subsets.\n",
        "    \n",
        "    \n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTS-9IfliTAT"
      },
      "outputs": [],
      "source": [
        "TRAIN_RATIO = 0.9\n",
        "\n",
        "dataset_length = len(wikitext_dataset)\n",
        "train_length = math.floor(dataset_length * TRAIN_RATIO)\n",
        "test_length = dataset_length - train_length\n",
        "\n",
        "rnn_train_dataset, rnn_test_dataset = torch.utils.data.random_split(rnn_dataset,\n",
        "                                                               [train_length, test_length],\n",
        "                                                               generator=torch.Generator().manual_seed(student_seed))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fi1TfgHbiTAT"
      },
      "source": [
        "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
        "    \n",
        "üéØ Goal:  Create `DataLoader` objects using `batch_size = 8` for the train and test subsets.\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEab9WiQiTAU"
      },
      "outputs": [],
      "source": [
        "batch_size = 8\n",
        "\n",
        "train_dataloader = DataLoader(rnn_train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(rnn_test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZpVeuyWiTAU"
      },
      "source": [
        "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #8e7cc3;background-color:#e4e1eb;border-radius: 15px;\">\n",
        "\n",
        "üéâ Excellent work! By this point, you will have made all the needed steps to make your data ready for training. \n",
        "\n",
        "#### Part 1 - Checklist\n",
        "Here are the core building blocks you created and that you will need for Part 2:\n",
        "   \n",
        "- [X] `rnn_dataset`: A Dataset obj with the data, the vocabulary, the pad index, the max sequence length, and maps of idx to word type and vice versa. \n",
        "- [X] `train_dataloader`: A DataLoader obj with your training data\n",
        "- [X] `test_dataloader`: A DataLoader obj with your testing data\n",
        "\n",
        "\n",
        "_Tip: Try to familiarize yourself with these objects and what functionalities and attributes they provide._\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPDNtdxFUvcn"
      },
      "source": [
        "---\n",
        "\n",
        "<a name=\"2\"></a>\n",
        "# PART 2:  Training Language Models ü§ó\n",
        "\n",
        "#### Language Model: a probabilistic model of a sequence of tokens.\n",
        "\n",
        "üîµ **What?**\n",
        "\n",
        "Language modeling (LM) is the use of various statistical and probabilistic techniques to determine the probability of a given sequence of words occurring in a sentence. Language models analyze bodies of text data to provide a basis for their word predictions. They are used in natural language processing (NLP) applications, particularly ones that generate text as an output. Some of these applications include, machine translation and question-answering.\n",
        "\n",
        "üü° **How?**\n",
        "\n",
        "There are several different probabilistic approaches to modeling language, which vary depending on the purpose of the language model. From a technical perspective, the various types differ by the amount of text data they analyze and the math they use to analyze it (architecture). Some LMs we've already seen and will learn about during lectures are n-gram / count-based models, Recurrent Neural Networks (RNNs), and Transformer models. \n",
        "\n",
        "üü£ **Why?**\n",
        "\n",
        "Language modeling is crucial in modern NLP applications. It is the reason that machines can understand qualitative information. Each language model type, in one way or another, turns qualitative information into quantitative information. This allows people to communicate with machines as they do with each other to a limited extent. It is used directly in a variety of industries including tech, finance, healthcare, transportation, legal, military and government. Additionally, it's likely most people reading this have interacted with a language model in some way at some point in the day, whether it be through Google search, an autocomplete text function or engaging with a voice assistant.\n",
        "\n",
        "‚ÑπÔ∏è Source: [Original article](https://www.techtarget.com/searchenterpriseai/definition/language-modeling#:~:text=Language%20models%20determine%20word%20probability,predict%20or%20produce%20new%20sentences.)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_CK0Zl7iTAU"
      },
      "source": [
        "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid gray;background-color:#F3F3F3;border-radius: 15px;\">\n",
        "\n",
        "In this part, you will train your own language models using the dataset created in Part 1.\n",
        "\n",
        "More specifically, you need to implement **5 different model variants**, train and test them to compute their perplexity.\n",
        "    \n",
        "| Model | Variant | Description |\n",
        "|:---- |:----- | :----- |\n",
        "| | Token embeddings trained from scratch | An LSTM model with a trainable token Embedding layer <br>that will be initialized randomly and trained from scratch along with the LM. |\n",
        "| **LSTM** | Pre-trained token embeddings & frozen | An LSTM model with pre-trained GloVe embeddings as input <br>that will be frozen while the LM is training. |\n",
        "|  | Pre-trained token embeddings & trainable | An LSTM model with pre-trained GloVe embeddings as input <br>that will be further trained along with the LM. |\n",
        "||||\n",
        "| **Transformer** | Trained from scratch | A Transformer based model that follows the architecture of [DistilGPT2](https://huggingface.co/distilgpt2). |\n",
        "|  | Pre-trained DistilGPT2 | A pre-trained Transformer based model called [DistilGPT2](https://huggingface.co/distilgpt2) <br>and will be used only for testing (not training). |\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "po9S7IYGxb1G",
        "outputId": "d3e435b8-4c87-4aa2-ef4e-46a1262fc802"
      },
      "source": [
        "---\n",
        "<a name=\"21\"></a>\n",
        "## 2.1 LSTM-variants\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Smi1xBm3iTAV"
      },
      "source": [
        "### 2.1.1 Implementing all LSTM variants in one Model class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7AkS2_0iTAV"
      },
      "source": [
        "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
        "\n",
        "üéØ Goal:  **Go to the `utils.py` file, and fill in the `VanillaLSTM` class with your implemenation.**\n",
        "    \n",
        "üíª Implementation hint: You will create one model class for all variants. Try to incorporate all the different cases into one Model class.\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIdJ9c9-Uvco"
      },
      "outputs": [],
      "source": [
        "import src.utils\n",
        "importlib.reload(src.utils)\n",
        "from src.utils import VanillaLSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pfHmcVCiTAW"
      },
      "source": [
        "### 2.1.2 Building training and testing pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3i0pjS2iTAW"
      },
      "source": [
        "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
        "\n",
        "üéØ Goal:  Implement training and testing pipelines.\n",
        "  \n",
        "üíª Implementation hint: Check the pipelines we created in the exercises sessions.\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyo5izV8iTAW"
      },
      "outputs": [],
      "source": [
        "def train(model: nn.Module, train_loader: DataLoader, optimizer: torch.optim.Optimizer, criterion: nn.Module, device):\n",
        "    \"\"\"\n",
        "    Main training pipeline. Implement the following:\n",
        "    - pass inputs to the model\n",
        "    - compute loss\n",
        "    - perform backward pass and update weights\n",
        "\n",
        "    :param model: \n",
        "    :param train_loader:\n",
        "    :param optimizer:\n",
        "    :param criterion: \n",
        "    return: \n",
        "    \"\"\"\n",
        "    \n",
        "    # YOUR CODE HERE\n",
        "    \n",
        "    epochs = 10\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    # Training loop\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        for i, (inputs, labels) in enumerate(iter(train_loader)):\n",
        "            \n",
        "            # move data to correct RAM\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # reset gradients from last iteration\n",
        "            model.zero_grad()\n",
        "            \n",
        "            # predict output\n",
        "            output = model(inputs)\n",
        "            \n",
        "            # calculate loss with respect to loss function and output and labels\n",
        "            loss = criterion(output.view(-1, output.shape[-1]), labels.view(-1))\n",
        "            \n",
        "            # calculate new gradients\n",
        "            loss.backward()\n",
        "            \n",
        "            # update weights with respect to the gradients calculated\n",
        "            optimizer.step()\n",
        "            \n",
        "            epoch_loss += loss\n",
        "            \n",
        "            print(f'e {epoch}/{epochs} i {i}/{len(train_loader)}: loss {loss}')\n",
        "\n",
        "        print(f'{epoch}: loss {epoch_loss / len(train_loader)}')\n",
        "    \n",
        "    return epoch_loss / len(train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3MaSi_TiTAX"
      },
      "outputs": [],
      "source": [
        "def test(model: nn.Module, test_loader: DataLoader, criterion: nn.Module):\n",
        "    \"\"\"\n",
        "    Main testing pipeline. Implement the following:\n",
        "    - pass inputs to the model\n",
        "    - compute loss\n",
        "    - compute perplexity\n",
        "\n",
        "    :param model: \n",
        "    :param test_loader:\n",
        "    :param criterion: \n",
        "    return: \n",
        "    \"\"\"\n",
        "    \n",
        "    # YOUR CODE HERE\n",
        "    \n",
        "    # Testing loop\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    for i, (inputs, labels) in tqdm(enumerate(test_loader)):\n",
        "        outputs = model(inputs)\n",
        "        \n",
        "        pass\n",
        "    model.train()\n",
        "    test_loss /= len(test_loader)\n",
        "    perplexity = 0\n",
        "    \n",
        "    print(f'Test loss: {test_loss:.3f}')\n",
        "    print(f'Test Perplexity: {perplexity:.3f}')\n",
        "    return test_loss, perplexity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PyjPXYsnNI6"
      },
      "source": [
        "### 2.1.3 Train and test LSTM variants\n",
        "\n",
        "For **all the LSTM variants** you will perform the following steps:\n",
        "\n",
        "1. Set hypeparameters\n",
        "2. Load embeddings if needed\n",
        "3. Instantiate the model and set training configurations\n",
        "4. Run training pipeline (from 2.1.2)\n",
        "5. Save the model\n",
        "6. Run testing pipeline and compute perplexity (from 2.1.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pro1_GX0iTAX"
      },
      "source": [
        "#### LSTM Variant A: Embeddings trained from scratch\n",
        "\n",
        "An LSTM model with a trainable Embedding layer that will be initialized randomly and trained from scratch along with the LM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYml9WJiiTAX"
      },
      "source": [
        "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
        "\n",
        "üéØ Goal: Set hyperparameters according to the objective of the model.\n",
        "      \n",
        "üíª Implementation hint: You can play arround with different values for `dropout_rate`, `lr` and `num_layers`.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npU3rud1iTAY"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "vocab_size = len(vocab_frequency)\n",
        "embedding_dim = 100\n",
        "hidden_dim = 100\n",
        "num_layers = 3\n",
        "dropout_rate = 0.4\n",
        "lr = 0.001  # learning rate\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqARabVmiTAY"
      },
      "source": [
        "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
        "\n",
        "üéØ Goal: Instantiate the **model, optimizer and loss**.\n",
        "      \n",
        "üíª Implementation hint: Choose your training settings according to the task you need to do.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Md5wrZODiTAY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24abf28e-b712-47f4-dfe2-dbf540d46ccf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 17,004,192 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "model = VanillaLSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate)\n",
        "model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=rnn_dataset.pad_index())\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'The model has {num_params:,} trainable parameters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqbaWk-niTAZ"
      },
      "source": [
        "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
        "\n",
        "üéØ Goal: Run **training and testing pipelines** on **10% data** (train and test) and compute perplexity.\n",
        "      \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pktk0srSeH4J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34a63d6e-837b-4467-f2aa-80f4880219e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "e 1/10 i 9988/40794: loss 6.055098533630371\n",
            "e 1/10 i 9989/40794: loss 5.870339870452881\n",
            "e 1/10 i 9990/40794: loss 5.987513065338135\n",
            "e 1/10 i 9991/40794: loss 6.226763725280762\n",
            "e 1/10 i 9992/40794: loss 6.127248287200928\n",
            "e 1/10 i 9993/40794: loss 5.868135452270508\n",
            "e 1/10 i 9994/40794: loss 5.762372970581055\n",
            "e 1/10 i 9995/40794: loss 6.169251918792725\n",
            "e 1/10 i 9996/40794: loss 6.118994235992432\n",
            "e 1/10 i 9997/40794: loss 6.117870330810547\n",
            "e 1/10 i 9998/40794: loss 6.1202569007873535\n",
            "e 1/10 i 9999/40794: loss 5.856876850128174\n",
            "e 1/10 i 10000/40794: loss 5.822028160095215\n",
            "e 1/10 i 10001/40794: loss 5.594770431518555\n",
            "e 1/10 i 10002/40794: loss 6.15214729309082\n",
            "e 1/10 i 10003/40794: loss 5.883103847503662\n",
            "e 1/10 i 10004/40794: loss 5.7709856033325195\n",
            "e 1/10 i 10005/40794: loss 5.924291610717773\n",
            "e 1/10 i 10006/40794: loss 5.920834064483643\n",
            "e 1/10 i 10007/40794: loss 6.327523708343506\n",
            "e 1/10 i 10008/40794: loss 6.056015491485596\n",
            "e 1/10 i 10009/40794: loss 5.589653968811035\n",
            "e 1/10 i 10010/40794: loss 5.730043888092041\n",
            "e 1/10 i 10011/40794: loss 5.625333786010742\n",
            "e 1/10 i 10012/40794: loss 6.112443447113037\n",
            "e 1/10 i 10013/40794: loss 5.884042739868164\n",
            "e 1/10 i 10014/40794: loss 5.551696300506592\n",
            "e 1/10 i 10015/40794: loss 6.031404972076416\n",
            "e 1/10 i 10016/40794: loss 6.191157817840576\n",
            "e 1/10 i 10017/40794: loss 6.297280311584473\n",
            "e 1/10 i 10018/40794: loss 5.749331474304199\n",
            "e 1/10 i 10019/40794: loss 6.3169145584106445\n",
            "e 1/10 i 10020/40794: loss 6.014194488525391\n",
            "e 1/10 i 10021/40794: loss 5.92758321762085\n",
            "e 1/10 i 10022/40794: loss 5.827342987060547\n",
            "e 1/10 i 10023/40794: loss 6.306915760040283\n",
            "e 1/10 i 10024/40794: loss 5.655018329620361\n",
            "e 1/10 i 10025/40794: loss 5.774974822998047\n",
            "e 1/10 i 10026/40794: loss 6.045607566833496\n",
            "e 1/10 i 10027/40794: loss 5.885106086730957\n",
            "e 1/10 i 10028/40794: loss 5.711679935455322\n",
            "e 1/10 i 10029/40794: loss 6.005999565124512\n",
            "e 1/10 i 10030/40794: loss 5.810566425323486\n",
            "e 1/10 i 10031/40794: loss 6.101196765899658\n",
            "e 1/10 i 10032/40794: loss 5.70159387588501\n",
            "e 1/10 i 10033/40794: loss 5.814387798309326\n",
            "e 1/10 i 10034/40794: loss 6.196067810058594\n",
            "e 1/10 i 10035/40794: loss 5.913367748260498\n",
            "e 1/10 i 10036/40794: loss 5.850870609283447\n",
            "e 1/10 i 10037/40794: loss 6.1485161781311035\n",
            "e 1/10 i 10038/40794: loss 5.992112636566162\n",
            "e 1/10 i 10039/40794: loss 5.871495723724365\n",
            "e 1/10 i 10040/40794: loss 5.894400596618652\n",
            "e 1/10 i 10041/40794: loss 5.881877899169922\n",
            "e 1/10 i 10042/40794: loss 5.927075386047363\n",
            "e 1/10 i 10043/40794: loss 5.64370584487915\n",
            "e 1/10 i 10044/40794: loss 5.917170524597168\n",
            "e 1/10 i 10045/40794: loss 5.603850841522217\n",
            "e 1/10 i 10046/40794: loss 5.692128658294678\n",
            "e 1/10 i 10047/40794: loss 5.816518306732178\n",
            "e 1/10 i 10048/40794: loss 6.174817085266113\n",
            "e 1/10 i 10049/40794: loss 5.912321090698242\n",
            "e 1/10 i 10050/40794: loss 6.056750774383545\n",
            "e 1/10 i 10051/40794: loss 5.8855133056640625\n",
            "e 1/10 i 10052/40794: loss 5.881464004516602\n",
            "e 1/10 i 10053/40794: loss 5.889494895935059\n",
            "e 1/10 i 10054/40794: loss 5.729152202606201\n",
            "e 1/10 i 10055/40794: loss 5.787390232086182\n",
            "e 1/10 i 10056/40794: loss 5.932842254638672\n",
            "e 1/10 i 10057/40794: loss 6.183187484741211\n",
            "e 1/10 i 10058/40794: loss 5.900888442993164\n",
            "e 1/10 i 10059/40794: loss 6.068120956420898\n",
            "e 1/10 i 10060/40794: loss 5.815324306488037\n",
            "e 1/10 i 10061/40794: loss 6.2202277183532715\n",
            "e 1/10 i 10062/40794: loss 5.814951419830322\n",
            "e 1/10 i 10063/40794: loss 5.921228885650635\n",
            "e 1/10 i 10064/40794: loss 5.680037021636963\n",
            "e 1/10 i 10065/40794: loss 6.201286315917969\n",
            "e 1/10 i 10066/40794: loss 5.99208927154541\n",
            "e 1/10 i 10067/40794: loss 6.059431076049805\n",
            "e 1/10 i 10068/40794: loss 5.950140953063965\n",
            "e 1/10 i 10069/40794: loss 6.1671929359436035\n",
            "e 1/10 i 10070/40794: loss 5.961462020874023\n",
            "e 1/10 i 10071/40794: loss 6.049546241760254\n",
            "e 1/10 i 10072/40794: loss 5.99907112121582\n",
            "e 1/10 i 10073/40794: loss 5.778854846954346\n",
            "e 1/10 i 10074/40794: loss 5.848137378692627\n",
            "e 1/10 i 10075/40794: loss 5.739935398101807\n",
            "e 1/10 i 10076/40794: loss 5.676784515380859\n",
            "e 1/10 i 10077/40794: loss 5.977260589599609\n",
            "e 1/10 i 10078/40794: loss 6.021651744842529\n",
            "e 1/10 i 10079/40794: loss 5.474016189575195\n",
            "e 1/10 i 10080/40794: loss 5.561796188354492\n",
            "e 1/10 i 10081/40794: loss 5.801485061645508\n",
            "e 1/10 i 10082/40794: loss 5.986713886260986\n",
            "e 1/10 i 10083/40794: loss 6.203776836395264\n",
            "e 1/10 i 10084/40794: loss 5.983895301818848\n",
            "e 1/10 i 10085/40794: loss 6.042779922485352\n",
            "e 1/10 i 10086/40794: loss 5.852632999420166\n",
            "e 1/10 i 10087/40794: loss 5.883440017700195\n",
            "e 1/10 i 10088/40794: loss 5.759815216064453\n",
            "e 1/10 i 10089/40794: loss 5.722936630249023\n",
            "e 1/10 i 10090/40794: loss 5.862199306488037\n",
            "e 1/10 i 10091/40794: loss 5.752016544342041\n",
            "e 1/10 i 10092/40794: loss 5.716721057891846\n",
            "e 1/10 i 10093/40794: loss 6.348424434661865\n",
            "e 1/10 i 10094/40794: loss 5.969122886657715\n",
            "e 1/10 i 10095/40794: loss 6.248631954193115\n",
            "e 1/10 i 10096/40794: loss 5.973001003265381\n",
            "e 1/10 i 10097/40794: loss 5.9542927742004395\n",
            "e 1/10 i 10098/40794: loss 5.743741989135742\n",
            "e 1/10 i 10099/40794: loss 5.952022552490234\n",
            "e 1/10 i 10100/40794: loss 6.19678258895874\n",
            "e 1/10 i 10101/40794: loss 5.841066360473633\n",
            "e 1/10 i 10102/40794: loss 5.695851802825928\n",
            "e 1/10 i 10103/40794: loss 5.853918075561523\n",
            "e 1/10 i 10104/40794: loss 6.033111572265625\n",
            "e 1/10 i 10105/40794: loss 6.141602993011475\n",
            "e 1/10 i 10106/40794: loss 6.049420356750488\n",
            "e 1/10 i 10107/40794: loss 6.228613376617432\n",
            "e 1/10 i 10108/40794: loss 6.110437870025635\n",
            "e 1/10 i 10109/40794: loss 5.6194024085998535\n",
            "e 1/10 i 10110/40794: loss 6.041894435882568\n",
            "e 1/10 i 10111/40794: loss 5.995476245880127\n",
            "e 1/10 i 10112/40794: loss 5.594342231750488\n",
            "e 1/10 i 10113/40794: loss 5.879291534423828\n",
            "e 1/10 i 10114/40794: loss 6.122198104858398\n",
            "e 1/10 i 10115/40794: loss 5.8285956382751465\n",
            "e 1/10 i 10116/40794: loss 5.95127010345459\n",
            "e 1/10 i 10117/40794: loss 6.105213642120361\n",
            "e 1/10 i 10118/40794: loss 6.237858772277832\n",
            "e 1/10 i 10119/40794: loss 6.234201908111572\n",
            "e 1/10 i 10120/40794: loss 5.935822010040283\n",
            "e 1/10 i 10121/40794: loss 6.361776351928711\n",
            "e 1/10 i 10122/40794: loss 5.96543025970459\n",
            "e 1/10 i 10123/40794: loss 5.859696388244629\n",
            "e 1/10 i 10124/40794: loss 5.67667818069458\n",
            "e 1/10 i 10125/40794: loss 5.904482364654541\n",
            "e 1/10 i 10126/40794: loss 5.886088848114014\n",
            "e 1/10 i 10127/40794: loss 5.479165554046631\n",
            "e 1/10 i 10128/40794: loss 5.864885330200195\n",
            "e 1/10 i 10129/40794: loss 6.046026229858398\n",
            "e 1/10 i 10130/40794: loss 5.876034259796143\n",
            "e 1/10 i 10131/40794: loss 6.106942176818848\n",
            "e 1/10 i 10132/40794: loss 5.775221824645996\n",
            "e 1/10 i 10133/40794: loss 5.715269565582275\n",
            "e 1/10 i 10134/40794: loss 5.791231632232666\n",
            "e 1/10 i 10135/40794: loss 6.229516983032227\n",
            "e 1/10 i 10136/40794: loss 6.410545825958252\n",
            "e 1/10 i 10137/40794: loss 5.935574531555176\n",
            "e 1/10 i 10138/40794: loss 5.822688579559326\n",
            "e 1/10 i 10139/40794: loss 5.936442852020264\n",
            "e 1/10 i 10140/40794: loss 5.682066917419434\n",
            "e 1/10 i 10141/40794: loss 5.73417329788208\n",
            "e 1/10 i 10142/40794: loss 5.712413787841797\n",
            "e 1/10 i 10143/40794: loss 6.002298355102539\n",
            "e 1/10 i 10144/40794: loss 6.082470893859863\n",
            "e 1/10 i 10145/40794: loss 6.122188568115234\n",
            "e 1/10 i 10146/40794: loss 5.856907367706299\n",
            "e 1/10 i 10147/40794: loss 5.865402698516846\n",
            "e 1/10 i 10148/40794: loss 5.920773029327393\n",
            "e 1/10 i 10149/40794: loss 6.0059638023376465\n",
            "e 1/10 i 10150/40794: loss 5.597231864929199\n",
            "e 1/10 i 10151/40794: loss 5.696404933929443\n",
            "e 1/10 i 10152/40794: loss 5.801916599273682\n",
            "e 1/10 i 10153/40794: loss 5.922268390655518\n",
            "e 1/10 i 10154/40794: loss 6.000406742095947\n",
            "e 1/10 i 10155/40794: loss 5.975632190704346\n",
            "e 1/10 i 10156/40794: loss 5.918997287750244\n",
            "e 1/10 i 10157/40794: loss 6.075531482696533\n",
            "e 1/10 i 10158/40794: loss 5.7664971351623535\n",
            "e 1/10 i 10159/40794: loss 6.010857582092285\n",
            "e 1/10 i 10160/40794: loss 5.873427867889404\n",
            "e 1/10 i 10161/40794: loss 5.867092132568359\n",
            "e 1/10 i 10162/40794: loss 6.214919090270996\n",
            "e 1/10 i 10163/40794: loss 6.189956188201904\n",
            "e 1/10 i 10164/40794: loss 5.833995819091797\n",
            "e 1/10 i 10165/40794: loss 6.0771989822387695\n",
            "e 1/10 i 10166/40794: loss 6.213217735290527\n",
            "e 1/10 i 10167/40794: loss 5.815922260284424\n",
            "e 1/10 i 10168/40794: loss 5.9142656326293945\n",
            "e 1/10 i 10169/40794: loss 5.8062663078308105\n",
            "e 1/10 i 10170/40794: loss 5.29055643081665\n",
            "e 1/10 i 10171/40794: loss 5.900772571563721\n",
            "e 1/10 i 10172/40794: loss 5.877525329589844\n",
            "e 1/10 i 10173/40794: loss 6.304972171783447\n",
            "e 1/10 i 10174/40794: loss 5.885979652404785\n",
            "e 1/10 i 10175/40794: loss 5.814706325531006\n",
            "e 1/10 i 10176/40794: loss 5.938248634338379\n",
            "e 1/10 i 10177/40794: loss 5.873449802398682\n",
            "e 1/10 i 10178/40794: loss 6.1608734130859375\n",
            "e 1/10 i 10179/40794: loss 6.21392822265625\n",
            "e 1/10 i 10180/40794: loss 6.206392288208008\n",
            "e 1/10 i 10181/40794: loss 5.973916530609131\n",
            "e 1/10 i 10182/40794: loss 5.802163124084473\n",
            "e 1/10 i 10183/40794: loss 6.1279215812683105\n",
            "e 1/10 i 10184/40794: loss 5.962216854095459\n",
            "e 1/10 i 10185/40794: loss 6.021304130554199\n",
            "e 1/10 i 10186/40794: loss 5.979213714599609\n",
            "e 1/10 i 10187/40794: loss 6.110413551330566\n",
            "e 1/10 i 10188/40794: loss 5.926226615905762\n",
            "e 1/10 i 10189/40794: loss 5.936880588531494\n",
            "e 1/10 i 10190/40794: loss 5.792823791503906\n",
            "e 1/10 i 10191/40794: loss 5.852776050567627\n",
            "e 1/10 i 10192/40794: loss 6.087807655334473\n",
            "e 1/10 i 10193/40794: loss 5.820297718048096\n",
            "e 1/10 i 10194/40794: loss 5.9247283935546875\n",
            "e 1/10 i 10195/40794: loss 5.819144248962402\n",
            "e 1/10 i 10196/40794: loss 5.655879497528076\n",
            "e 1/10 i 10197/40794: loss 6.221731185913086\n",
            "e 1/10 i 10198/40794: loss 6.211852550506592\n",
            "e 1/10 i 10199/40794: loss 5.867931365966797\n",
            "e 1/10 i 10200/40794: loss 6.060257911682129\n",
            "e 1/10 i 10201/40794: loss 6.3409552574157715\n",
            "e 1/10 i 10202/40794: loss 5.6503801345825195\n",
            "e 1/10 i 10203/40794: loss 6.109951019287109\n",
            "e 1/10 i 10204/40794: loss 6.142415523529053\n",
            "e 1/10 i 10205/40794: loss 6.254348278045654\n",
            "e 1/10 i 10206/40794: loss 5.787217617034912\n",
            "e 1/10 i 10207/40794: loss 5.962503910064697\n",
            "e 1/10 i 10208/40794: loss 6.1385297775268555\n",
            "e 1/10 i 10209/40794: loss 5.880181789398193\n",
            "e 1/10 i 10210/40794: loss 5.957402229309082\n",
            "e 1/10 i 10211/40794: loss 5.810428619384766\n",
            "e 1/10 i 10212/40794: loss 5.925594329833984\n",
            "e 1/10 i 10213/40794: loss 6.0490899085998535\n",
            "e 1/10 i 10214/40794: loss 5.646938800811768\n",
            "e 1/10 i 10215/40794: loss 5.789460182189941\n",
            "e 1/10 i 10216/40794: loss 6.036146640777588\n",
            "e 1/10 i 10217/40794: loss 5.741941452026367\n",
            "e 1/10 i 10218/40794: loss 5.785118103027344\n",
            "e 1/10 i 10219/40794: loss 5.97304105758667\n",
            "e 1/10 i 10220/40794: loss 5.789308071136475\n",
            "e 1/10 i 10221/40794: loss 5.675659656524658\n",
            "e 1/10 i 10222/40794: loss 5.648582935333252\n",
            "e 1/10 i 10223/40794: loss 5.593620300292969\n",
            "e 1/10 i 10224/40794: loss 5.811914443969727\n",
            "e 1/10 i 10225/40794: loss 5.6315836906433105\n",
            "e 1/10 i 10226/40794: loss 5.781103610992432\n",
            "e 1/10 i 10227/40794: loss 5.987889289855957\n",
            "e 1/10 i 10228/40794: loss 5.637950420379639\n",
            "e 1/10 i 10229/40794: loss 5.883660316467285\n",
            "e 1/10 i 10230/40794: loss 5.805084228515625\n",
            "e 1/10 i 10231/40794: loss 5.911514759063721\n",
            "e 1/10 i 10232/40794: loss 6.01286506652832\n",
            "e 1/10 i 10233/40794: loss 5.932260990142822\n",
            "e 1/10 i 10234/40794: loss 6.124973297119141\n",
            "e 1/10 i 10235/40794: loss 5.890439033508301\n",
            "e 1/10 i 10236/40794: loss 5.722799301147461\n",
            "e 1/10 i 10237/40794: loss 5.876396656036377\n",
            "e 1/10 i 10238/40794: loss 6.031980514526367\n",
            "e 1/10 i 10239/40794: loss 6.052060604095459\n",
            "e 1/10 i 10240/40794: loss 5.97353458404541\n",
            "e 1/10 i 10241/40794: loss 6.055146217346191\n",
            "e 1/10 i 10242/40794: loss 6.059934616088867\n",
            "e 1/10 i 10243/40794: loss 6.01579475402832\n",
            "e 1/10 i 10244/40794: loss 5.789398670196533\n",
            "e 1/10 i 10245/40794: loss 5.850594520568848\n",
            "e 1/10 i 10246/40794: loss 5.850198268890381\n",
            "e 1/10 i 10247/40794: loss 6.173648357391357\n",
            "e 1/10 i 10248/40794: loss 5.712290287017822\n",
            "e 1/10 i 10249/40794: loss 6.284817218780518\n",
            "e 1/10 i 10250/40794: loss 5.949962139129639\n",
            "e 1/10 i 10251/40794: loss 6.028624534606934\n",
            "e 1/10 i 10252/40794: loss 6.108782768249512\n",
            "e 1/10 i 10253/40794: loss 5.613434791564941\n",
            "e 1/10 i 10254/40794: loss 6.288385391235352\n",
            "e 1/10 i 10255/40794: loss 6.131653308868408\n",
            "e 1/10 i 10256/40794: loss 6.135724067687988\n",
            "e 1/10 i 10257/40794: loss 5.867883205413818\n",
            "e 1/10 i 10258/40794: loss 5.94596004486084\n",
            "e 1/10 i 10259/40794: loss 5.660999774932861\n",
            "e 1/10 i 10260/40794: loss 5.678987979888916\n",
            "e 1/10 i 10261/40794: loss 6.083047866821289\n",
            "e 1/10 i 10262/40794: loss 5.818291664123535\n",
            "e 1/10 i 10263/40794: loss 6.16175651550293\n",
            "e 1/10 i 10264/40794: loss 5.702333450317383\n",
            "e 1/10 i 10265/40794: loss 5.985572814941406\n",
            "e 1/10 i 10266/40794: loss 5.812590599060059\n",
            "e 1/10 i 10267/40794: loss 6.021411895751953\n",
            "e 1/10 i 10268/40794: loss 6.057446002960205\n",
            "e 1/10 i 10269/40794: loss 5.785114288330078\n",
            "e 1/10 i 10270/40794: loss 5.841947555541992\n",
            "e 1/10 i 10271/40794: loss 5.895412445068359\n",
            "e 1/10 i 10272/40794: loss 5.918755054473877\n",
            "e 1/10 i 10273/40794: loss 5.710183143615723\n",
            "e 1/10 i 10274/40794: loss 6.007918834686279\n",
            "e 1/10 i 10275/40794: loss 5.973599910736084\n",
            "e 1/10 i 10276/40794: loss 5.876760959625244\n",
            "e 1/10 i 10277/40794: loss 6.014709949493408\n",
            "e 1/10 i 10278/40794: loss 5.460747718811035\n",
            "e 1/10 i 10279/40794: loss 6.116239070892334\n",
            "e 1/10 i 10280/40794: loss 6.023715019226074\n",
            "e 1/10 i 10281/40794: loss 5.861006736755371\n",
            "e 1/10 i 10282/40794: loss 5.818300247192383\n",
            "e 1/10 i 10283/40794: loss 5.983329772949219\n",
            "e 1/10 i 10284/40794: loss 6.046437740325928\n",
            "e 1/10 i 10285/40794: loss 6.031971454620361\n",
            "e 1/10 i 10286/40794: loss 5.962989807128906\n",
            "e 1/10 i 10287/40794: loss 5.7824225425720215\n",
            "e 1/10 i 10288/40794: loss 5.960864543914795\n",
            "e 1/10 i 10289/40794: loss 6.075606346130371\n",
            "e 1/10 i 10290/40794: loss 6.070073127746582\n",
            "e 1/10 i 10291/40794: loss 6.319033622741699\n",
            "e 1/10 i 10292/40794: loss 6.212048530578613\n",
            "e 1/10 i 10293/40794: loss 6.0867791175842285\n",
            "e 1/10 i 10294/40794: loss 6.113309860229492\n",
            "e 1/10 i 10295/40794: loss 5.671442985534668\n",
            "e 1/10 i 10296/40794: loss 5.730199337005615\n",
            "e 1/10 i 10297/40794: loss 6.288877964019775\n",
            "e 1/10 i 10298/40794: loss 5.933513641357422\n",
            "e 1/10 i 10299/40794: loss 6.215031147003174\n",
            "e 1/10 i 10300/40794: loss 5.7749857902526855\n",
            "e 1/10 i 10301/40794: loss 5.936028957366943\n",
            "e 1/10 i 10302/40794: loss 6.277354717254639\n",
            "e 1/10 i 10303/40794: loss 5.798274993896484\n",
            "e 1/10 i 10304/40794: loss 6.0942463874816895\n",
            "e 1/10 i 10305/40794: loss 5.8284525871276855\n",
            "e 1/10 i 10306/40794: loss 5.841079235076904\n",
            "e 1/10 i 10307/40794: loss 6.0780792236328125\n",
            "e 1/10 i 10308/40794: loss 5.807828903198242\n",
            "e 1/10 i 10309/40794: loss 5.696564674377441\n",
            "e 1/10 i 10310/40794: loss 6.016176223754883\n",
            "e 1/10 i 10311/40794: loss 5.986031532287598\n",
            "e 1/10 i 10312/40794: loss 5.71696662902832\n",
            "e 1/10 i 10313/40794: loss 5.967193126678467\n",
            "e 1/10 i 10314/40794: loss 5.927502155303955\n",
            "e 1/10 i 10315/40794: loss 5.6735639572143555\n",
            "e 1/10 i 10316/40794: loss 5.665062427520752\n",
            "e 1/10 i 10317/40794: loss 5.876446723937988\n",
            "e 1/10 i 10318/40794: loss 6.167664051055908\n",
            "e 1/10 i 10319/40794: loss 5.857969284057617\n",
            "e 1/10 i 10320/40794: loss 6.247146129608154\n",
            "e 1/10 i 10321/40794: loss 5.959865570068359\n",
            "e 1/10 i 10322/40794: loss 5.761442184448242\n",
            "e 1/10 i 10323/40794: loss 5.722636699676514\n",
            "e 1/10 i 10324/40794: loss 6.031901836395264\n",
            "e 1/10 i 10325/40794: loss 5.862544536590576\n",
            "e 1/10 i 10326/40794: loss 5.657646656036377\n",
            "e 1/10 i 10327/40794: loss 5.700430870056152\n",
            "e 1/10 i 10328/40794: loss 6.114831924438477\n",
            "e 1/10 i 10329/40794: loss 5.917954444885254\n",
            "e 1/10 i 10330/40794: loss 5.965578079223633\n",
            "e 1/10 i 10331/40794: loss 5.846975803375244\n",
            "e 1/10 i 10332/40794: loss 5.933252334594727\n",
            "e 1/10 i 10333/40794: loss 5.491647720336914\n",
            "e 1/10 i 10334/40794: loss 6.208590507507324\n",
            "e 1/10 i 10335/40794: loss 5.808294296264648\n",
            "e 1/10 i 10336/40794: loss 5.889139652252197\n",
            "e 1/10 i 10337/40794: loss 5.809327602386475\n",
            "e 1/10 i 10338/40794: loss 5.987436294555664\n",
            "e 1/10 i 10339/40794: loss 6.189537048339844\n",
            "e 1/10 i 10340/40794: loss 5.826570987701416\n",
            "e 1/10 i 10341/40794: loss 5.909653186798096\n",
            "e 1/10 i 10342/40794: loss 5.692427158355713\n",
            "e 1/10 i 10343/40794: loss 5.745254993438721\n",
            "e 1/10 i 10344/40794: loss 5.3784589767456055\n",
            "e 1/10 i 10345/40794: loss 5.843989372253418\n",
            "e 1/10 i 10346/40794: loss 5.652735710144043\n",
            "e 1/10 i 10347/40794: loss 5.947338104248047\n",
            "e 1/10 i 10348/40794: loss 6.159246444702148\n",
            "e 1/10 i 10349/40794: loss 6.014547824859619\n",
            "e 1/10 i 10350/40794: loss 5.51959228515625\n",
            "e 1/10 i 10351/40794: loss 6.122154235839844\n",
            "e 1/10 i 10352/40794: loss 5.783878803253174\n",
            "e 1/10 i 10353/40794: loss 5.911685943603516\n",
            "e 1/10 i 10354/40794: loss 5.9135870933532715\n",
            "e 1/10 i 10355/40794: loss 5.906196594238281\n",
            "e 1/10 i 10356/40794: loss 5.982271671295166\n",
            "e 1/10 i 10357/40794: loss 5.990182399749756\n",
            "e 1/10 i 10358/40794: loss 5.997319221496582\n",
            "e 1/10 i 10359/40794: loss 5.827281951904297\n",
            "e 1/10 i 10360/40794: loss 5.882941722869873\n",
            "e 1/10 i 10361/40794: loss 5.783950328826904\n",
            "e 1/10 i 10362/40794: loss 6.014322757720947\n",
            "e 1/10 i 10363/40794: loss 6.190072536468506\n",
            "e 1/10 i 10364/40794: loss 6.026760578155518\n",
            "e 1/10 i 10365/40794: loss 6.009385108947754\n",
            "e 1/10 i 10366/40794: loss 5.741217136383057\n",
            "e 1/10 i 10367/40794: loss 5.876525402069092\n",
            "e 1/10 i 10368/40794: loss 5.994091033935547\n",
            "e 1/10 i 10369/40794: loss 5.704835891723633\n",
            "e 1/10 i 10370/40794: loss 6.109058856964111\n",
            "e 1/10 i 10371/40794: loss 5.678114891052246\n",
            "e 1/10 i 10372/40794: loss 5.755126476287842\n",
            "e 1/10 i 10373/40794: loss 6.056132793426514\n",
            "e 1/10 i 10374/40794: loss 5.856215953826904\n",
            "e 1/10 i 10375/40794: loss 5.969188690185547\n",
            "e 1/10 i 10376/40794: loss 6.001104831695557\n",
            "e 1/10 i 10377/40794: loss 5.774849891662598\n",
            "e 1/10 i 10378/40794: loss 5.879716396331787\n",
            "e 1/10 i 10379/40794: loss 5.795838832855225\n",
            "e 1/10 i 10380/40794: loss 5.889880180358887\n",
            "e 1/10 i 10381/40794: loss 5.964935302734375\n",
            "e 1/10 i 10382/40794: loss 5.881156921386719\n",
            "e 1/10 i 10383/40794: loss 5.770925045013428\n",
            "e 1/10 i 10384/40794: loss 5.848603248596191\n",
            "e 1/10 i 10385/40794: loss 6.090622425079346\n",
            "e 1/10 i 10386/40794: loss 6.064889430999756\n",
            "e 1/10 i 10387/40794: loss 5.7875447273254395\n",
            "e 1/10 i 10388/40794: loss 5.863081932067871\n",
            "e 1/10 i 10389/40794: loss 5.883005142211914\n",
            "e 1/10 i 10390/40794: loss 5.913824081420898\n",
            "e 1/10 i 10391/40794: loss 5.903105735778809\n",
            "e 1/10 i 10392/40794: loss 6.014520168304443\n",
            "e 1/10 i 10393/40794: loss 5.562407493591309\n",
            "e 1/10 i 10394/40794: loss 5.8909173011779785\n",
            "e 1/10 i 10395/40794: loss 6.0712409019470215\n",
            "e 1/10 i 10396/40794: loss 6.121267318725586\n",
            "e 1/10 i 10397/40794: loss 5.8481974601745605\n",
            "e 1/10 i 10398/40794: loss 5.917062282562256\n",
            "e 1/10 i 10399/40794: loss 5.829440593719482\n",
            "e 1/10 i 10400/40794: loss 6.074619293212891\n",
            "e 1/10 i 10401/40794: loss 5.950345039367676\n",
            "e 1/10 i 10402/40794: loss 6.109849452972412\n",
            "e 1/10 i 10403/40794: loss 6.13839054107666\n",
            "e 1/10 i 10404/40794: loss 5.800579071044922\n",
            "e 1/10 i 10405/40794: loss 5.834656238555908\n",
            "e 1/10 i 10406/40794: loss 5.813787460327148\n",
            "e 1/10 i 10407/40794: loss 6.3482208251953125\n",
            "e 1/10 i 10408/40794: loss 5.953573226928711\n",
            "e 1/10 i 10409/40794: loss 6.002297401428223\n",
            "e 1/10 i 10410/40794: loss 5.7390971183776855\n",
            "e 1/10 i 10411/40794: loss 5.792899131774902\n",
            "e 1/10 i 10412/40794: loss 6.0017290115356445\n",
            "e 1/10 i 10413/40794: loss 6.334235668182373\n",
            "e 1/10 i 10414/40794: loss 5.8285017013549805\n",
            "e 1/10 i 10415/40794: loss 5.817021369934082\n",
            "e 1/10 i 10416/40794: loss 5.756197452545166\n",
            "e 1/10 i 10417/40794: loss 5.885397434234619\n",
            "e 1/10 i 10418/40794: loss 6.1744256019592285\n",
            "e 1/10 i 10419/40794: loss 6.165176868438721\n",
            "e 1/10 i 10420/40794: loss 6.467820644378662\n",
            "e 1/10 i 10421/40794: loss 5.804788589477539\n",
            "e 1/10 i 10422/40794: loss 5.8358001708984375\n",
            "e 1/10 i 10423/40794: loss 5.850765228271484\n",
            "e 1/10 i 10424/40794: loss 5.667177677154541\n",
            "e 1/10 i 10425/40794: loss 5.939525127410889\n",
            "e 1/10 i 10426/40794: loss 5.893459320068359\n",
            "e 1/10 i 10427/40794: loss 5.872988224029541\n",
            "e 1/10 i 10428/40794: loss 5.937875747680664\n",
            "e 1/10 i 10429/40794: loss 5.948885917663574\n",
            "e 1/10 i 10430/40794: loss 5.709827423095703\n",
            "e 1/10 i 10431/40794: loss 5.780402183532715\n",
            "e 1/10 i 10432/40794: loss 5.867031097412109\n",
            "e 1/10 i 10433/40794: loss 6.062254428863525\n",
            "e 1/10 i 10434/40794: loss 5.768758296966553\n",
            "e 1/10 i 10435/40794: loss 5.860620498657227\n",
            "e 1/10 i 10436/40794: loss 6.100002765655518\n",
            "e 1/10 i 10437/40794: loss 6.006256580352783\n",
            "e 1/10 i 10438/40794: loss 6.0534586906433105\n",
            "e 1/10 i 10439/40794: loss 5.805793762207031\n",
            "e 1/10 i 10440/40794: loss 5.982904434204102\n",
            "e 1/10 i 10441/40794: loss 5.727664947509766\n",
            "e 1/10 i 10442/40794: loss 5.834039688110352\n",
            "e 1/10 i 10443/40794: loss 5.926140308380127\n",
            "e 1/10 i 10444/40794: loss 6.1036376953125\n",
            "e 1/10 i 10445/40794: loss 5.615845680236816\n",
            "e 1/10 i 10446/40794: loss 6.307289123535156\n",
            "e 1/10 i 10447/40794: loss 6.242791175842285\n",
            "e 1/10 i 10448/40794: loss 6.033918857574463\n",
            "e 1/10 i 10449/40794: loss 5.8139424324035645\n",
            "e 1/10 i 10450/40794: loss 5.965561866760254\n",
            "e 1/10 i 10451/40794: loss 5.658668041229248\n",
            "e 1/10 i 10452/40794: loss 6.577106475830078\n",
            "e 1/10 i 10453/40794: loss 5.833219528198242\n",
            "e 1/10 i 10454/40794: loss 5.9597673416137695\n",
            "e 1/10 i 10455/40794: loss 6.022706508636475\n",
            "e 1/10 i 10456/40794: loss 5.966426849365234\n",
            "e 1/10 i 10457/40794: loss 5.991502285003662\n",
            "e 1/10 i 10458/40794: loss 6.059952735900879\n",
            "e 1/10 i 10459/40794: loss 6.177891731262207\n",
            "e 1/10 i 10460/40794: loss 5.919654369354248\n",
            "e 1/10 i 10461/40794: loss 5.790603160858154\n",
            "e 1/10 i 10462/40794: loss 5.728541374206543\n",
            "e 1/10 i 10463/40794: loss 5.867488384246826\n",
            "e 1/10 i 10464/40794: loss 5.79110050201416\n",
            "e 1/10 i 10465/40794: loss 5.814358234405518\n",
            "e 1/10 i 10466/40794: loss 5.885351657867432\n",
            "e 1/10 i 10467/40794: loss 5.795285701751709\n",
            "e 1/10 i 10468/40794: loss 5.952493667602539\n",
            "e 1/10 i 10469/40794: loss 6.124238014221191\n",
            "e 1/10 i 10470/40794: loss 5.748826503753662\n",
            "e 1/10 i 10471/40794: loss 5.964871883392334\n",
            "e 1/10 i 10472/40794: loss 5.955480098724365\n",
            "e 1/10 i 10473/40794: loss 5.975688457489014\n",
            "e 1/10 i 10474/40794: loss 5.67221212387085\n",
            "e 1/10 i 10475/40794: loss 5.741379261016846\n",
            "e 1/10 i 10476/40794: loss 6.435894966125488\n",
            "e 1/10 i 10477/40794: loss 6.055086135864258\n",
            "e 1/10 i 10478/40794: loss 5.811653137207031\n",
            "e 1/10 i 10479/40794: loss 6.003845691680908\n",
            "e 1/10 i 10480/40794: loss 5.592747211456299\n",
            "e 1/10 i 10481/40794: loss 5.799023151397705\n",
            "e 1/10 i 10482/40794: loss 5.9231367111206055\n",
            "e 1/10 i 10483/40794: loss 5.668648719787598\n",
            "e 1/10 i 10484/40794: loss 5.961520195007324\n",
            "e 1/10 i 10485/40794: loss 5.966094493865967\n",
            "e 1/10 i 10486/40794: loss 5.864342212677002\n",
            "e 1/10 i 10487/40794: loss 5.949859142303467\n",
            "e 1/10 i 10488/40794: loss 5.798703193664551\n",
            "e 1/10 i 10489/40794: loss 5.948122501373291\n",
            "e 1/10 i 10490/40794: loss 5.917805194854736\n",
            "e 1/10 i 10491/40794: loss 6.103570938110352\n",
            "e 1/10 i 10492/40794: loss 5.910677433013916\n",
            "e 1/10 i 10493/40794: loss 6.024127960205078\n",
            "e 1/10 i 10494/40794: loss 5.921988010406494\n",
            "e 1/10 i 10495/40794: loss 5.6831231117248535\n",
            "e 1/10 i 10496/40794: loss 5.990106582641602\n",
            "e 1/10 i 10497/40794: loss 6.141575336456299\n",
            "e 1/10 i 10498/40794: loss 5.9885149002075195\n",
            "e 1/10 i 10499/40794: loss 5.7305426597595215\n",
            "e 1/10 i 10500/40794: loss 6.057947158813477\n",
            "e 1/10 i 10501/40794: loss 5.91209602355957\n",
            "e 1/10 i 10502/40794: loss 6.33324670791626\n",
            "e 1/10 i 10503/40794: loss 6.120263576507568\n",
            "e 1/10 i 10504/40794: loss 5.690372467041016\n",
            "e 1/10 i 10505/40794: loss 6.078347682952881\n",
            "e 1/10 i 10506/40794: loss 6.117975234985352\n",
            "e 1/10 i 10507/40794: loss 5.698104381561279\n",
            "e 1/10 i 10508/40794: loss 6.0840744972229\n",
            "e 1/10 i 10509/40794: loss 5.911709308624268\n",
            "e 1/10 i 10510/40794: loss 5.970190048217773\n",
            "e 1/10 i 10511/40794: loss 5.790346622467041\n",
            "e 1/10 i 10512/40794: loss 5.87229585647583\n",
            "e 1/10 i 10513/40794: loss 5.835630893707275\n",
            "e 1/10 i 10514/40794: loss 5.813794136047363\n",
            "e 1/10 i 10515/40794: loss 6.018346309661865\n",
            "e 1/10 i 10516/40794: loss 6.0016679763793945\n",
            "e 1/10 i 10517/40794: loss 5.554356575012207\n",
            "e 1/10 i 10518/40794: loss 5.999658584594727\n",
            "e 1/10 i 10519/40794: loss 5.901919364929199\n",
            "e 1/10 i 10520/40794: loss 6.039298057556152\n",
            "e 1/10 i 10521/40794: loss 6.044122695922852\n",
            "e 1/10 i 10522/40794: loss 5.8101654052734375\n",
            "e 1/10 i 10523/40794: loss 6.053496837615967\n",
            "e 1/10 i 10524/40794: loss 5.706185817718506\n",
            "e 1/10 i 10525/40794: loss 6.104523181915283\n",
            "e 1/10 i 10526/40794: loss 5.724681854248047\n",
            "e 1/10 i 10527/40794: loss 5.975330352783203\n",
            "e 1/10 i 10528/40794: loss 5.495159149169922\n",
            "e 1/10 i 10529/40794: loss 6.035898208618164\n",
            "e 1/10 i 10530/40794: loss 5.82088041305542\n",
            "e 1/10 i 10531/40794: loss 5.792929649353027\n",
            "e 1/10 i 10532/40794: loss 6.138809680938721\n",
            "e 1/10 i 10533/40794: loss 5.877182960510254\n",
            "e 1/10 i 10534/40794: loss 6.124236583709717\n",
            "e 1/10 i 10535/40794: loss 5.853466987609863\n",
            "e 1/10 i 10536/40794: loss 5.87106466293335\n",
            "e 1/10 i 10537/40794: loss 5.658279895782471\n",
            "e 1/10 i 10538/40794: loss 5.730245113372803\n",
            "e 1/10 i 10539/40794: loss 5.840998649597168\n",
            "e 1/10 i 10540/40794: loss 5.816455364227295\n",
            "e 1/10 i 10541/40794: loss 5.778326034545898\n",
            "e 1/10 i 10542/40794: loss 6.021071434020996\n",
            "e 1/10 i 10543/40794: loss 6.137580871582031\n",
            "e 1/10 i 10544/40794: loss 5.985569953918457\n",
            "e 1/10 i 10545/40794: loss 5.717307090759277\n",
            "e 1/10 i 10546/40794: loss 6.062339782714844\n",
            "e 1/10 i 10547/40794: loss 6.32042121887207\n",
            "e 1/10 i 10548/40794: loss 6.173014163970947\n",
            "e 1/10 i 10549/40794: loss 5.837802410125732\n",
            "e 1/10 i 10550/40794: loss 5.8466362953186035\n",
            "e 1/10 i 10551/40794: loss 5.6686811447143555\n",
            "e 1/10 i 10552/40794: loss 5.930719375610352\n",
            "e 1/10 i 10553/40794: loss 5.992462635040283\n",
            "e 1/10 i 10554/40794: loss 5.592389106750488\n",
            "e 1/10 i 10555/40794: loss 5.929612159729004\n",
            "e 1/10 i 10556/40794: loss 6.092077732086182\n",
            "e 1/10 i 10557/40794: loss 5.814549922943115\n",
            "e 1/10 i 10558/40794: loss 5.805883884429932\n",
            "e 1/10 i 10559/40794: loss 5.927978038787842\n",
            "e 1/10 i 10560/40794: loss 5.934169292449951\n",
            "e 1/10 i 10561/40794: loss 6.104373455047607\n",
            "e 1/10 i 10562/40794: loss 6.021917819976807\n",
            "e 1/10 i 10563/40794: loss 5.892476558685303\n",
            "e 1/10 i 10564/40794: loss 6.072161674499512\n",
            "e 1/10 i 10565/40794: loss 5.730860710144043\n",
            "e 1/10 i 10566/40794: loss 6.1031036376953125\n",
            "e 1/10 i 10567/40794: loss 5.536829471588135\n",
            "e 1/10 i 10568/40794: loss 6.1548871994018555\n",
            "e 1/10 i 10569/40794: loss 5.813511371612549\n",
            "e 1/10 i 10570/40794: loss 5.716279029846191\n",
            "e 1/10 i 10571/40794: loss 5.843165397644043\n",
            "e 1/10 i 10572/40794: loss 6.173325061798096\n",
            "e 1/10 i 10573/40794: loss 5.950905799865723\n",
            "e 1/10 i 10574/40794: loss 6.224804401397705\n",
            "e 1/10 i 10575/40794: loss 5.929264545440674\n",
            "e 1/10 i 10576/40794: loss 5.923348426818848\n",
            "e 1/10 i 10577/40794: loss 5.756381988525391\n",
            "e 1/10 i 10578/40794: loss 6.124232292175293\n",
            "e 1/10 i 10579/40794: loss 6.012972831726074\n",
            "e 1/10 i 10580/40794: loss 5.963567733764648\n",
            "e 1/10 i 10581/40794: loss 5.777267932891846\n",
            "e 1/10 i 10582/40794: loss 5.574000835418701\n",
            "e 1/10 i 10583/40794: loss 5.779787540435791\n",
            "e 1/10 i 10584/40794: loss 5.816381931304932\n",
            "e 1/10 i 10585/40794: loss 5.9703192710876465\n",
            "e 1/10 i 10586/40794: loss 5.846622467041016\n",
            "e 1/10 i 10587/40794: loss 6.073115348815918\n",
            "e 1/10 i 10588/40794: loss 5.78908634185791\n",
            "e 1/10 i 10589/40794: loss 5.763501167297363\n",
            "e 1/10 i 10590/40794: loss 6.040767669677734\n",
            "e 1/10 i 10591/40794: loss 6.005470275878906\n",
            "e 1/10 i 10592/40794: loss 5.738354682922363\n",
            "e 1/10 i 10593/40794: loss 6.292666912078857\n",
            "e 1/10 i 10594/40794: loss 6.102342128753662\n",
            "e 1/10 i 10595/40794: loss 6.008097171783447\n",
            "e 1/10 i 10596/40794: loss 5.964720249176025\n",
            "e 1/10 i 10597/40794: loss 5.828246116638184\n",
            "e 1/10 i 10598/40794: loss 5.963798522949219\n",
            "e 1/10 i 10599/40794: loss 5.726863861083984\n",
            "e 1/10 i 10600/40794: loss 6.110832214355469\n",
            "e 1/10 i 10601/40794: loss 6.225327968597412\n",
            "e 1/10 i 10602/40794: loss 5.78349494934082\n",
            "e 1/10 i 10603/40794: loss 5.853055477142334\n",
            "e 1/10 i 10604/40794: loss 6.171903133392334\n",
            "e 1/10 i 10605/40794: loss 6.010012626647949\n",
            "e 1/10 i 10606/40794: loss 5.9827752113342285\n",
            "e 1/10 i 10607/40794: loss 5.819608688354492\n",
            "e 1/10 i 10608/40794: loss 6.01775598526001\n",
            "e 1/10 i 10609/40794: loss 6.127468109130859\n",
            "e 1/10 i 10610/40794: loss 6.304150581359863\n",
            "e 1/10 i 10611/40794: loss 5.999495506286621\n",
            "e 1/10 i 10612/40794: loss 5.766952037811279\n",
            "e 1/10 i 10613/40794: loss 5.779620170593262\n",
            "e 1/10 i 10614/40794: loss 6.733935356140137\n",
            "e 1/10 i 10615/40794: loss 5.848578453063965\n",
            "e 1/10 i 10616/40794: loss 6.046612739562988\n",
            "e 1/10 i 10617/40794: loss 5.709561347961426\n",
            "e 1/10 i 10618/40794: loss 5.713396072387695\n",
            "e 1/10 i 10619/40794: loss 6.059825420379639\n",
            "e 1/10 i 10620/40794: loss 5.855439186096191\n",
            "e 1/10 i 10621/40794: loss 5.8229594230651855\n",
            "e 1/10 i 10622/40794: loss 5.719707012176514\n",
            "e 1/10 i 10623/40794: loss 5.929191589355469\n",
            "e 1/10 i 10624/40794: loss 6.2453484535217285\n",
            "e 1/10 i 10625/40794: loss 6.016557216644287\n",
            "e 1/10 i 10626/40794: loss 5.594095706939697\n",
            "e 1/10 i 10627/40794: loss 6.045418739318848\n",
            "e 1/10 i 10628/40794: loss 5.989480495452881\n",
            "e 1/10 i 10629/40794: loss 5.857230186462402\n",
            "e 1/10 i 10630/40794: loss 6.172720432281494\n",
            "e 1/10 i 10631/40794: loss 5.537106513977051\n",
            "e 1/10 i 10632/40794: loss 5.889304161071777\n",
            "e 1/10 i 10633/40794: loss 5.882133960723877\n",
            "e 1/10 i 10634/40794: loss 6.0275163650512695\n",
            "e 1/10 i 10635/40794: loss 5.730406761169434\n",
            "e 1/10 i 10636/40794: loss 5.99565315246582\n",
            "e 1/10 i 10637/40794: loss 5.892271995544434\n",
            "e 1/10 i 10638/40794: loss 6.057723522186279\n",
            "e 1/10 i 10639/40794: loss 5.994076251983643\n",
            "e 1/10 i 10640/40794: loss 5.924924850463867\n",
            "e 1/10 i 10641/40794: loss 6.066992282867432\n",
            "e 1/10 i 10642/40794: loss 5.944093704223633\n",
            "e 1/10 i 10643/40794: loss 6.155785083770752\n",
            "e 1/10 i 10644/40794: loss 5.881319999694824\n",
            "e 1/10 i 10645/40794: loss 5.913829803466797\n",
            "e 1/10 i 10646/40794: loss 5.694336891174316\n",
            "e 1/10 i 10647/40794: loss 5.900233268737793\n",
            "e 1/10 i 10648/40794: loss 6.236556529998779\n",
            "e 1/10 i 10649/40794: loss 5.233519554138184\n",
            "e 1/10 i 10650/40794: loss 5.796388149261475\n",
            "e 1/10 i 10651/40794: loss 5.834718227386475\n",
            "e 1/10 i 10652/40794: loss 5.801327228546143\n",
            "e 1/10 i 10653/40794: loss 6.105966091156006\n",
            "e 1/10 i 10654/40794: loss 6.165923595428467\n",
            "e 1/10 i 10655/40794: loss 5.81317663192749\n",
            "e 1/10 i 10656/40794: loss 6.046295642852783\n",
            "e 1/10 i 10657/40794: loss 5.787200450897217\n",
            "e 1/10 i 10658/40794: loss 5.880527973175049\n",
            "e 1/10 i 10659/40794: loss 6.021501064300537\n",
            "e 1/10 i 10660/40794: loss 6.036531448364258\n",
            "e 1/10 i 10661/40794: loss 6.37677001953125\n",
            "e 1/10 i 10662/40794: loss 5.831966876983643\n",
            "e 1/10 i 10663/40794: loss 6.024087905883789\n",
            "e 1/10 i 10664/40794: loss 6.212744235992432\n",
            "e 1/10 i 10665/40794: loss 5.96883487701416\n",
            "e 1/10 i 10666/40794: loss 5.614731788635254\n",
            "e 1/10 i 10667/40794: loss 5.972042560577393\n",
            "e 1/10 i 10668/40794: loss 6.166861534118652\n",
            "e 1/10 i 10669/40794: loss 5.6260528564453125\n",
            "e 1/10 i 10670/40794: loss 6.038549423217773\n",
            "e 1/10 i 10671/40794: loss 6.051510810852051\n",
            "e 1/10 i 10672/40794: loss 5.675661563873291\n",
            "e 1/10 i 10673/40794: loss 6.069496154785156\n",
            "e 1/10 i 10674/40794: loss 5.729996204376221\n",
            "e 1/10 i 10675/40794: loss 6.274195671081543\n",
            "e 1/10 i 10676/40794: loss 6.129793643951416\n",
            "e 1/10 i 10677/40794: loss 6.07227087020874\n",
            "e 1/10 i 10678/40794: loss 5.6677727699279785\n",
            "e 1/10 i 10679/40794: loss 6.005983352661133\n",
            "e 1/10 i 10680/40794: loss 5.898158550262451\n",
            "e 1/10 i 10681/40794: loss 5.9202189445495605\n",
            "e 1/10 i 10682/40794: loss 6.002795219421387\n",
            "e 1/10 i 10683/40794: loss 5.77004861831665\n",
            "e 1/10 i 10684/40794: loss 5.730835437774658\n",
            "e 1/10 i 10685/40794: loss 5.845936298370361\n",
            "e 1/10 i 10686/40794: loss 5.865118980407715\n",
            "e 1/10 i 10687/40794: loss 5.608946800231934\n",
            "e 1/10 i 10688/40794: loss 6.060133457183838\n",
            "e 1/10 i 10689/40794: loss 5.619933605194092\n",
            "e 1/10 i 10690/40794: loss 6.060284614562988\n",
            "e 1/10 i 10691/40794: loss 5.920626163482666\n",
            "e 1/10 i 10692/40794: loss 5.693142890930176\n",
            "e 1/10 i 10693/40794: loss 5.679270267486572\n",
            "e 1/10 i 10694/40794: loss 5.989773273468018\n",
            "e 1/10 i 10695/40794: loss 5.884641170501709\n",
            "e 1/10 i 10696/40794: loss 6.164060115814209\n",
            "e 1/10 i 10697/40794: loss 6.135308742523193\n",
            "e 1/10 i 10698/40794: loss 5.512675762176514\n",
            "e 1/10 i 10699/40794: loss 5.948779106140137\n",
            "e 1/10 i 10700/40794: loss 5.876789093017578\n",
            "e 1/10 i 10701/40794: loss 6.196288585662842\n",
            "e 1/10 i 10702/40794: loss 5.865333557128906\n",
            "e 1/10 i 10703/40794: loss 5.958508014678955\n",
            "e 1/10 i 10704/40794: loss 5.5011305809021\n",
            "e 1/10 i 10705/40794: loss 5.810067176818848\n",
            "e 1/10 i 10706/40794: loss 5.808197975158691\n",
            "e 1/10 i 10707/40794: loss 6.015748500823975\n",
            "e 1/10 i 10708/40794: loss 5.777225971221924\n",
            "e 1/10 i 10709/40794: loss 5.756469249725342\n",
            "e 1/10 i 10710/40794: loss 6.0955095291137695\n",
            "e 1/10 i 10711/40794: loss 6.015827178955078\n",
            "e 1/10 i 10712/40794: loss 5.982451915740967\n",
            "e 1/10 i 10713/40794: loss 5.749083042144775\n",
            "e 1/10 i 10714/40794: loss 6.17537260055542\n",
            "e 1/10 i 10715/40794: loss 5.973026752471924\n",
            "e 1/10 i 10716/40794: loss 5.955221176147461\n",
            "e 1/10 i 10717/40794: loss 6.036342620849609\n",
            "e 1/10 i 10718/40794: loss 5.87497091293335\n",
            "e 1/10 i 10719/40794: loss 5.6442365646362305\n",
            "e 1/10 i 10720/40794: loss 5.9483747482299805\n",
            "e 1/10 i 10721/40794: loss 6.2010931968688965\n",
            "e 1/10 i 10722/40794: loss 5.965038776397705\n",
            "e 1/10 i 10723/40794: loss 5.867146015167236\n",
            "e 1/10 i 10724/40794: loss 5.791133403778076\n",
            "e 1/10 i 10725/40794: loss 5.838895320892334\n",
            "e 1/10 i 10726/40794: loss 6.136726379394531\n",
            "e 1/10 i 10727/40794: loss 5.958693504333496\n",
            "e 1/10 i 10728/40794: loss 5.892907619476318\n",
            "e 1/10 i 10729/40794: loss 6.144543647766113\n",
            "e 1/10 i 10730/40794: loss 6.024274826049805\n",
            "e 1/10 i 10731/40794: loss 5.9006500244140625\n",
            "e 1/10 i 10732/40794: loss 6.053264141082764\n",
            "e 1/10 i 10733/40794: loss 5.45480489730835\n",
            "e 1/10 i 10734/40794: loss 5.767181873321533\n",
            "e 1/10 i 10735/40794: loss 5.96308708190918\n",
            "e 1/10 i 10736/40794: loss 6.240071773529053\n",
            "e 1/10 i 10737/40794: loss 6.226171493530273\n",
            "e 1/10 i 10738/40794: loss 6.37217903137207\n",
            "e 1/10 i 10739/40794: loss 5.859194278717041\n",
            "e 1/10 i 10740/40794: loss 5.940576076507568\n",
            "e 1/10 i 10741/40794: loss 6.223628044128418\n",
            "e 1/10 i 10742/40794: loss 5.962684631347656\n",
            "e 1/10 i 10743/40794: loss 5.913416385650635\n",
            "e 1/10 i 10744/40794: loss 6.128539562225342\n",
            "e 1/10 i 10745/40794: loss 6.126424789428711\n",
            "e 1/10 i 10746/40794: loss 5.712070465087891\n",
            "e 1/10 i 10747/40794: loss 5.77094030380249\n",
            "e 1/10 i 10748/40794: loss 5.762453079223633\n",
            "e 1/10 i 10749/40794: loss 5.915217876434326\n",
            "e 1/10 i 10750/40794: loss 6.344692707061768\n",
            "e 1/10 i 10751/40794: loss 5.843319892883301\n",
            "e 1/10 i 10752/40794: loss 5.7788405418396\n",
            "e 1/10 i 10753/40794: loss 5.939060211181641\n",
            "e 1/10 i 10754/40794: loss 6.054889678955078\n",
            "e 1/10 i 10755/40794: loss 6.045617580413818\n",
            "e 1/10 i 10756/40794: loss 5.6087775230407715\n",
            "e 1/10 i 10757/40794: loss 5.758774757385254\n",
            "e 1/10 i 10758/40794: loss 5.975132465362549\n",
            "e 1/10 i 10759/40794: loss 6.144116401672363\n",
            "e 1/10 i 10760/40794: loss 5.958108425140381\n",
            "e 1/10 i 10761/40794: loss 6.050705909729004\n",
            "e 1/10 i 10762/40794: loss 6.003788948059082\n",
            "e 1/10 i 10763/40794: loss 5.906428337097168\n",
            "e 1/10 i 10764/40794: loss 5.956727504730225\n",
            "e 1/10 i 10765/40794: loss 5.789337158203125\n",
            "e 1/10 i 10766/40794: loss 5.880346298217773\n",
            "e 1/10 i 10767/40794: loss 5.978488445281982\n",
            "e 1/10 i 10768/40794: loss 5.963029861450195\n",
            "e 1/10 i 10769/40794: loss 6.1349711418151855\n",
            "e 1/10 i 10770/40794: loss 6.112598896026611\n",
            "e 1/10 i 10771/40794: loss 5.545100212097168\n",
            "e 1/10 i 10772/40794: loss 5.670862674713135\n",
            "e 1/10 i 10773/40794: loss 5.9102983474731445\n",
            "e 1/10 i 10774/40794: loss 6.0280961990356445\n",
            "e 1/10 i 10775/40794: loss 5.810327053070068\n",
            "e 1/10 i 10776/40794: loss 5.954782962799072\n",
            "e 1/10 i 10777/40794: loss 5.966547966003418\n",
            "e 1/10 i 10778/40794: loss 6.260583400726318\n",
            "e 1/10 i 10779/40794: loss 6.147822856903076\n",
            "e 1/10 i 10780/40794: loss 5.783748626708984\n",
            "e 1/10 i 10781/40794: loss 6.034176349639893\n",
            "e 1/10 i 10782/40794: loss 5.896446704864502\n",
            "e 1/10 i 10783/40794: loss 6.075462341308594\n",
            "e 1/10 i 10784/40794: loss 6.175662994384766\n",
            "e 1/10 i 10785/40794: loss 6.08510684967041\n",
            "e 1/10 i 10786/40794: loss 6.299386501312256\n",
            "e 1/10 i 10787/40794: loss 6.028383731842041\n",
            "e 1/10 i 10788/40794: loss 5.852559566497803\n",
            "e 1/10 i 10789/40794: loss 6.1476826667785645\n",
            "e 1/10 i 10790/40794: loss 5.971953392028809\n",
            "e 1/10 i 10791/40794: loss 5.939971446990967\n",
            "e 1/10 i 10792/40794: loss 5.692665100097656\n",
            "e 1/10 i 10793/40794: loss 5.980742454528809\n",
            "e 1/10 i 10794/40794: loss 6.254109859466553\n",
            "e 1/10 i 10795/40794: loss 5.75516414642334\n",
            "e 1/10 i 10796/40794: loss 6.191775798797607\n",
            "e 1/10 i 10797/40794: loss 5.841588973999023\n",
            "e 1/10 i 10798/40794: loss 6.040048599243164\n",
            "e 1/10 i 10799/40794: loss 5.830517768859863\n",
            "e 1/10 i 10800/40794: loss 6.0619707107543945\n",
            "e 1/10 i 10801/40794: loss 5.8799896240234375\n",
            "e 1/10 i 10802/40794: loss 5.988952159881592\n",
            "e 1/10 i 10803/40794: loss 5.867922782897949\n",
            "e 1/10 i 10804/40794: loss 5.991331100463867\n",
            "e 1/10 i 10805/40794: loss 5.869076728820801\n",
            "e 1/10 i 10806/40794: loss 5.855059623718262\n",
            "e 1/10 i 10807/40794: loss 5.957952976226807\n",
            "e 1/10 i 10808/40794: loss 6.07568883895874\n",
            "e 1/10 i 10809/40794: loss 5.881798267364502\n",
            "e 1/10 i 10810/40794: loss 5.837594032287598\n",
            "e 1/10 i 10811/40794: loss 6.219732761383057\n",
            "e 1/10 i 10812/40794: loss 5.939172267913818\n",
            "e 1/10 i 10813/40794: loss 5.929739952087402\n",
            "e 1/10 i 10814/40794: loss 5.735589981079102\n",
            "e 1/10 i 10815/40794: loss 5.830157279968262\n",
            "e 1/10 i 10816/40794: loss 5.894540309906006\n",
            "e 1/10 i 10817/40794: loss 5.869690895080566\n",
            "e 1/10 i 10818/40794: loss 6.01786470413208\n",
            "e 1/10 i 10819/40794: loss 5.900276184082031\n",
            "e 1/10 i 10820/40794: loss 5.898866653442383\n",
            "e 1/10 i 10821/40794: loss 5.391235828399658\n",
            "e 1/10 i 10822/40794: loss 5.920132160186768\n",
            "e 1/10 i 10823/40794: loss 6.424274921417236\n",
            "e 1/10 i 10824/40794: loss 5.97811222076416\n",
            "e 1/10 i 10825/40794: loss 6.323930263519287\n",
            "e 1/10 i 10826/40794: loss 6.004497051239014\n",
            "e 1/10 i 10827/40794: loss 5.85244607925415\n",
            "e 1/10 i 10828/40794: loss 5.77517557144165\n",
            "e 1/10 i 10829/40794: loss 5.5570173263549805\n",
            "e 1/10 i 10830/40794: loss 5.688809394836426\n",
            "e 1/10 i 10831/40794: loss 5.7783203125\n",
            "e 1/10 i 10832/40794: loss 5.715420246124268\n",
            "e 1/10 i 10833/40794: loss 5.86430025100708\n",
            "e 1/10 i 10834/40794: loss 5.697093486785889\n",
            "e 1/10 i 10835/40794: loss 6.209188461303711\n",
            "e 1/10 i 10836/40794: loss 5.892712593078613\n",
            "e 1/10 i 10837/40794: loss 5.842127799987793\n",
            "e 1/10 i 10838/40794: loss 6.007277011871338\n",
            "e 1/10 i 10839/40794: loss 6.218795299530029\n",
            "e 1/10 i 10840/40794: loss 6.005449295043945\n",
            "e 1/10 i 10841/40794: loss 6.1377458572387695\n",
            "e 1/10 i 10842/40794: loss 5.813190460205078\n",
            "e 1/10 i 10843/40794: loss 5.932002067565918\n",
            "e 1/10 i 10844/40794: loss 6.149590492248535\n",
            "e 1/10 i 10845/40794: loss 5.790038108825684\n",
            "e 1/10 i 10846/40794: loss 6.092569828033447\n",
            "e 1/10 i 10847/40794: loss 5.902645587921143\n",
            "e 1/10 i 10848/40794: loss 6.327399253845215\n",
            "e 1/10 i 10849/40794: loss 6.01882791519165\n",
            "e 1/10 i 10850/40794: loss 6.264883995056152\n",
            "e 1/10 i 10851/40794: loss 5.704185962677002\n",
            "e 1/10 i 10852/40794: loss 5.696944713592529\n",
            "e 1/10 i 10853/40794: loss 5.795825958251953\n",
            "e 1/10 i 10854/40794: loss 6.121214389801025\n",
            "e 1/10 i 10855/40794: loss 5.906186580657959\n",
            "e 1/10 i 10856/40794: loss 5.984934329986572\n",
            "e 1/10 i 10857/40794: loss 5.8418869972229\n",
            "e 1/10 i 10858/40794: loss 6.062849044799805\n",
            "e 1/10 i 10859/40794: loss 5.782792568206787\n",
            "e 1/10 i 10860/40794: loss 6.163878917694092\n",
            "e 1/10 i 10861/40794: loss 5.746293067932129\n",
            "e 1/10 i 10862/40794: loss 5.781533241271973\n",
            "e 1/10 i 10863/40794: loss 5.701558589935303\n",
            "e 1/10 i 10864/40794: loss 5.887850761413574\n",
            "e 1/10 i 10865/40794: loss 5.866075038909912\n",
            "e 1/10 i 10866/40794: loss 6.041578769683838\n",
            "e 1/10 i 10867/40794: loss 6.202606201171875\n",
            "e 1/10 i 10868/40794: loss 5.784530162811279\n",
            "e 1/10 i 10869/40794: loss 5.981614589691162\n",
            "e 1/10 i 10870/40794: loss 6.029143333435059\n",
            "e 1/10 i 10871/40794: loss 5.763586044311523\n",
            "e 1/10 i 10872/40794: loss 5.917192459106445\n",
            "e 1/10 i 10873/40794: loss 5.8761467933654785\n",
            "e 1/10 i 10874/40794: loss 5.9359331130981445\n",
            "e 1/10 i 10875/40794: loss 5.905428409576416\n",
            "e 1/10 i 10876/40794: loss 5.448956489562988\n",
            "e 1/10 i 10877/40794: loss 5.963894367218018\n",
            "e 1/10 i 10878/40794: loss 5.744638919830322\n",
            "e 1/10 i 10879/40794: loss 6.009924411773682\n",
            "e 1/10 i 10880/40794: loss 5.887020111083984\n",
            "e 1/10 i 10881/40794: loss 6.1417155265808105\n",
            "e 1/10 i 10882/40794: loss 6.3378190994262695\n",
            "e 1/10 i 10883/40794: loss 5.546801567077637\n",
            "e 1/10 i 10884/40794: loss 5.812747955322266\n",
            "e 1/10 i 10885/40794: loss 5.810304164886475\n",
            "e 1/10 i 10886/40794: loss 5.968503952026367\n",
            "e 1/10 i 10887/40794: loss 5.881016254425049\n",
            "e 1/10 i 10888/40794: loss 5.6418890953063965\n",
            "e 1/10 i 10889/40794: loss 5.925526142120361\n",
            "e 1/10 i 10890/40794: loss 6.078897476196289\n",
            "e 1/10 i 10891/40794: loss 6.010552883148193\n",
            "e 1/10 i 10892/40794: loss 5.9724249839782715\n",
            "e 1/10 i 10893/40794: loss 5.838837146759033\n",
            "e 1/10 i 10894/40794: loss 6.086603164672852\n",
            "e 1/10 i 10895/40794: loss 5.775620460510254\n",
            "e 1/10 i 10896/40794: loss 6.077389717102051\n",
            "e 1/10 i 10897/40794: loss 6.033060550689697\n",
            "e 1/10 i 10898/40794: loss 5.834387302398682\n",
            "e 1/10 i 10899/40794: loss 6.011329650878906\n",
            "e 1/10 i 10900/40794: loss 6.0119404792785645\n",
            "e 1/10 i 10901/40794: loss 5.828084468841553\n",
            "e 1/10 i 10902/40794: loss 6.04233980178833\n",
            "e 1/10 i 10903/40794: loss 6.037103176116943\n",
            "e 1/10 i 10904/40794: loss 6.114936351776123\n",
            "e 1/10 i 10905/40794: loss 5.87367057800293\n",
            "e 1/10 i 10906/40794: loss 5.9322190284729\n",
            "e 1/10 i 10907/40794: loss 5.98341178894043\n",
            "e 1/10 i 10908/40794: loss 6.072020053863525\n",
            "e 1/10 i 10909/40794: loss 6.049110412597656\n",
            "e 1/10 i 10910/40794: loss 5.791099548339844\n",
            "e 1/10 i 10911/40794: loss 6.066033840179443\n",
            "e 1/10 i 10912/40794: loss 5.950590133666992\n",
            "e 1/10 i 10913/40794: loss 6.025762557983398\n",
            "e 1/10 i 10914/40794: loss 6.168098449707031\n",
            "e 1/10 i 10915/40794: loss 5.785042762756348\n",
            "e 1/10 i 10916/40794: loss 5.567976951599121\n",
            "e 1/10 i 10917/40794: loss 5.96772575378418\n",
            "e 1/10 i 10918/40794: loss 6.037069797515869\n",
            "e 1/10 i 10919/40794: loss 6.1109299659729\n",
            "e 1/10 i 10920/40794: loss 6.176968574523926\n",
            "e 1/10 i 10921/40794: loss 6.281538009643555\n",
            "e 1/10 i 10922/40794: loss 6.225077152252197\n",
            "e 1/10 i 10923/40794: loss 6.166665554046631\n",
            "e 1/10 i 10924/40794: loss 5.828250408172607\n",
            "e 1/10 i 10925/40794: loss 6.065723896026611\n",
            "e 1/10 i 10926/40794: loss 5.673286437988281\n",
            "e 1/10 i 10927/40794: loss 6.00439453125\n",
            "e 1/10 i 10928/40794: loss 6.155450344085693\n",
            "e 1/10 i 10929/40794: loss 6.018650531768799\n",
            "e 1/10 i 10930/40794: loss 6.029106616973877\n",
            "e 1/10 i 10931/40794: loss 5.952155113220215\n",
            "e 1/10 i 10932/40794: loss 5.811328411102295\n",
            "e 1/10 i 10933/40794: loss 6.524117946624756\n",
            "e 1/10 i 10934/40794: loss 6.041219711303711\n",
            "e 1/10 i 10935/40794: loss 6.0097975730896\n",
            "e 1/10 i 10936/40794: loss 6.1259965896606445\n",
            "e 1/10 i 10937/40794: loss 5.878110408782959\n",
            "e 1/10 i 10938/40794: loss 5.875041961669922\n",
            "e 1/10 i 10939/40794: loss 5.722137928009033\n",
            "e 1/10 i 10940/40794: loss 6.2546305656433105\n",
            "e 1/10 i 10941/40794: loss 5.976186752319336\n",
            "e 1/10 i 10942/40794: loss 6.049574851989746\n",
            "e 1/10 i 10943/40794: loss 6.276596546173096\n",
            "e 1/10 i 10944/40794: loss 6.1735944747924805\n",
            "e 1/10 i 10945/40794: loss 6.03445291519165\n",
            "e 1/10 i 10946/40794: loss 5.878445148468018\n",
            "e 1/10 i 10947/40794: loss 6.047985076904297\n",
            "e 1/10 i 10948/40794: loss 5.9346723556518555\n",
            "e 1/10 i 10949/40794: loss 5.954535007476807\n",
            "e 1/10 i 10950/40794: loss 6.072861194610596\n",
            "e 1/10 i 10951/40794: loss 5.926035404205322\n",
            "e 1/10 i 10952/40794: loss 6.184647083282471\n",
            "e 1/10 i 10953/40794: loss 5.876572132110596\n",
            "e 1/10 i 10954/40794: loss 5.85918664932251\n",
            "e 1/10 i 10955/40794: loss 6.079668045043945\n",
            "e 1/10 i 10956/40794: loss 6.120323181152344\n",
            "e 1/10 i 10957/40794: loss 5.855740547180176\n",
            "e 1/10 i 10958/40794: loss 5.924200057983398\n",
            "e 1/10 i 10959/40794: loss 5.824914455413818\n",
            "e 1/10 i 10960/40794: loss 6.040303707122803\n",
            "e 1/10 i 10961/40794: loss 5.8239054679870605\n",
            "e 1/10 i 10962/40794: loss 5.910848617553711\n",
            "e 1/10 i 10963/40794: loss 6.0081095695495605\n",
            "e 1/10 i 10964/40794: loss 5.552277565002441\n",
            "e 1/10 i 10965/40794: loss 5.9300923347473145\n",
            "e 1/10 i 10966/40794: loss 5.567497253417969\n",
            "e 1/10 i 10967/40794: loss 5.662580490112305\n",
            "e 1/10 i 10968/40794: loss 5.707063674926758\n",
            "e 1/10 i 10969/40794: loss 5.757976055145264\n",
            "e 1/10 i 10970/40794: loss 5.955510139465332\n",
            "e 1/10 i 10971/40794: loss 6.051385879516602\n",
            "e 1/10 i 10972/40794: loss 6.040276050567627\n",
            "e 1/10 i 10973/40794: loss 6.073899269104004\n",
            "e 1/10 i 10974/40794: loss 5.938770771026611\n",
            "e 1/10 i 10975/40794: loss 5.916632175445557\n",
            "e 1/10 i 10976/40794: loss 5.998600482940674\n",
            "e 1/10 i 10977/40794: loss 5.880681991577148\n",
            "e 1/10 i 10978/40794: loss 6.086740970611572\n",
            "e 1/10 i 10979/40794: loss 5.626197814941406\n",
            "e 1/10 i 10980/40794: loss 5.746799945831299\n",
            "e 1/10 i 10981/40794: loss 5.867858409881592\n",
            "e 1/10 i 10982/40794: loss 5.8568010330200195\n",
            "e 1/10 i 10983/40794: loss 5.679037094116211\n",
            "e 1/10 i 10984/40794: loss 6.064914703369141\n",
            "e 1/10 i 10985/40794: loss 5.9943952560424805\n",
            "e 1/10 i 10986/40794: loss 5.954432010650635\n",
            "e 1/10 i 10987/40794: loss 5.9795989990234375\n",
            "e 1/10 i 10988/40794: loss 6.082334518432617\n",
            "e 1/10 i 10989/40794: loss 5.921440601348877\n",
            "e 1/10 i 10990/40794: loss 5.977993488311768\n",
            "e 1/10 i 10991/40794: loss 5.558770656585693\n",
            "e 1/10 i 10992/40794: loss 6.000065803527832\n",
            "e 1/10 i 10993/40794: loss 5.992417812347412\n",
            "e 1/10 i 10994/40794: loss 5.813335418701172\n",
            "e 1/10 i 10995/40794: loss 6.233048915863037\n",
            "e 1/10 i 10996/40794: loss 5.898688316345215\n",
            "e 1/10 i 10997/40794: loss 5.953566551208496\n",
            "e 1/10 i 10998/40794: loss 5.86627721786499\n",
            "e 1/10 i 10999/40794: loss 6.00551700592041\n",
            "e 1/10 i 11000/40794: loss 5.833740711212158\n",
            "e 1/10 i 11001/40794: loss 5.822754383087158\n",
            "e 1/10 i 11002/40794: loss 6.101902961730957\n",
            "e 1/10 i 11003/40794: loss 5.905724048614502\n",
            "e 1/10 i 11004/40794: loss 5.817389011383057\n",
            "e 1/10 i 11005/40794: loss 6.22554349899292\n",
            "e 1/10 i 11006/40794: loss 5.547901153564453\n",
            "e 1/10 i 11007/40794: loss 6.009791851043701\n",
            "e 1/10 i 11008/40794: loss 6.1082072257995605\n",
            "e 1/10 i 11009/40794: loss 5.802092552185059\n",
            "e 1/10 i 11010/40794: loss 5.643616676330566\n",
            "e 1/10 i 11011/40794: loss 5.644014835357666\n",
            "e 1/10 i 11012/40794: loss 5.89612340927124\n",
            "e 1/10 i 11013/40794: loss 6.040062427520752\n",
            "e 1/10 i 11014/40794: loss 6.250969409942627\n",
            "e 1/10 i 11015/40794: loss 5.675851345062256\n",
            "e 1/10 i 11016/40794: loss 6.354068279266357\n",
            "e 1/10 i 11017/40794: loss 5.755726337432861\n",
            "e 1/10 i 11018/40794: loss 5.789964199066162\n",
            "e 1/10 i 11019/40794: loss 6.004251480102539\n",
            "e 1/10 i 11020/40794: loss 6.00197696685791\n",
            "e 1/10 i 11021/40794: loss 5.832970142364502\n",
            "e 1/10 i 11022/40794: loss 6.19951057434082\n",
            "e 1/10 i 11023/40794: loss 5.864378929138184\n",
            "e 1/10 i 11024/40794: loss 5.931631088256836\n",
            "e 1/10 i 11025/40794: loss 5.812697410583496\n",
            "e 1/10 i 11026/40794: loss 6.006363868713379\n",
            "e 1/10 i 11027/40794: loss 5.919417381286621\n",
            "e 1/10 i 11028/40794: loss 5.7535624504089355\n",
            "e 1/10 i 11029/40794: loss 5.962141513824463\n",
            "e 1/10 i 11030/40794: loss 5.4216628074646\n",
            "e 1/10 i 11031/40794: loss 6.054455757141113\n",
            "e 1/10 i 11032/40794: loss 5.8345947265625\n",
            "e 1/10 i 11033/40794: loss 6.124034404754639\n",
            "e 1/10 i 11034/40794: loss 6.200616359710693\n",
            "e 1/10 i 11035/40794: loss 6.224613189697266\n",
            "e 1/10 i 11036/40794: loss 6.117591857910156\n",
            "e 1/10 i 11037/40794: loss 5.859489917755127\n",
            "e 1/10 i 11038/40794: loss 5.923446178436279\n",
            "e 1/10 i 11039/40794: loss 5.9710588455200195\n",
            "e 1/10 i 11040/40794: loss 5.847512722015381\n",
            "e 1/10 i 11041/40794: loss 6.051701068878174\n",
            "e 1/10 i 11042/40794: loss 5.964906692504883\n",
            "e 1/10 i 11043/40794: loss 6.082849502563477\n",
            "e 1/10 i 11044/40794: loss 5.845475673675537\n",
            "e 1/10 i 11045/40794: loss 5.788599014282227\n",
            "e 1/10 i 11046/40794: loss 5.939321994781494\n",
            "e 1/10 i 11047/40794: loss 5.889888763427734\n",
            "e 1/10 i 11048/40794: loss 5.87765645980835\n",
            "e 1/10 i 11049/40794: loss 5.795356750488281\n",
            "e 1/10 i 11050/40794: loss 5.635344505310059\n",
            "e 1/10 i 11051/40794: loss 6.011641025543213\n",
            "e 1/10 i 11052/40794: loss 6.1037492752075195\n",
            "e 1/10 i 11053/40794: loss 5.787667274475098\n",
            "e 1/10 i 11054/40794: loss 5.875801086425781\n",
            "e 1/10 i 11055/40794: loss 5.78584098815918\n",
            "e 1/10 i 11056/40794: loss 5.745551109313965\n",
            "e 1/10 i 11057/40794: loss 6.215832710266113\n",
            "e 1/10 i 11058/40794: loss 5.354665756225586\n",
            "e 1/10 i 11059/40794: loss 5.521572113037109\n",
            "e 1/10 i 11060/40794: loss 5.828370094299316\n",
            "e 1/10 i 11061/40794: loss 5.793130874633789\n",
            "e 1/10 i 11062/40794: loss 5.999229907989502\n",
            "e 1/10 i 11063/40794: loss 5.944747447967529\n",
            "e 1/10 i 11064/40794: loss 6.148926734924316\n",
            "e 1/10 i 11065/40794: loss 5.151283264160156\n",
            "e 1/10 i 11066/40794: loss 6.109165191650391\n",
            "e 1/10 i 11067/40794: loss 5.790896892547607\n",
            "e 1/10 i 11068/40794: loss 5.698754787445068\n",
            "e 1/10 i 11069/40794: loss 5.912949562072754\n",
            "e 1/10 i 11070/40794: loss 5.931723594665527\n",
            "e 1/10 i 11071/40794: loss 5.8997063636779785\n",
            "e 1/10 i 11072/40794: loss 6.017231464385986\n",
            "e 1/10 i 11073/40794: loss 6.209136009216309\n",
            "e 1/10 i 11074/40794: loss 6.2298970222473145\n",
            "e 1/10 i 11075/40794: loss 5.851611137390137\n",
            "e 1/10 i 11076/40794: loss 6.005360126495361\n",
            "e 1/10 i 11077/40794: loss 5.736125469207764\n",
            "e 1/10 i 11078/40794: loss 5.832072734832764\n",
            "e 1/10 i 11079/40794: loss 5.903036594390869\n",
            "e 1/10 i 11080/40794: loss 5.936972618103027\n",
            "e 1/10 i 11081/40794: loss 5.98588752746582\n",
            "e 1/10 i 11082/40794: loss 6.285128593444824\n",
            "e 1/10 i 11083/40794: loss 6.013504981994629\n",
            "e 1/10 i 11084/40794: loss 6.060741901397705\n",
            "e 1/10 i 11085/40794: loss 5.9106926918029785\n",
            "e 1/10 i 11086/40794: loss 5.626364707946777\n",
            "e 1/10 i 11087/40794: loss 5.293639183044434\n",
            "e 1/10 i 11088/40794: loss 5.8633809089660645\n",
            "e 1/10 i 11089/40794: loss 6.250696659088135\n",
            "e 1/10 i 11090/40794: loss 6.050467491149902\n",
            "e 1/10 i 11091/40794: loss 6.370504856109619\n",
            "e 1/10 i 11092/40794: loss 5.900760650634766\n",
            "e 1/10 i 11093/40794: loss 5.731029987335205\n",
            "e 1/10 i 11094/40794: loss 6.26893424987793\n",
            "e 1/10 i 11095/40794: loss 5.840106010437012\n",
            "e 1/10 i 11096/40794: loss 5.957993984222412\n",
            "e 1/10 i 11097/40794: loss 6.114543914794922\n",
            "e 1/10 i 11098/40794: loss 5.8737382888793945\n",
            "e 1/10 i 11099/40794: loss 5.983370304107666\n",
            "e 1/10 i 11100/40794: loss 5.796751976013184\n",
            "e 1/10 i 11101/40794: loss 6.002022743225098\n",
            "e 1/10 i 11102/40794: loss 5.745542526245117\n",
            "e 1/10 i 11103/40794: loss 5.719954490661621\n",
            "e 1/10 i 11104/40794: loss 6.319451332092285\n",
            "e 1/10 i 11105/40794: loss 5.787795543670654\n",
            "e 1/10 i 11106/40794: loss 5.875048637390137\n",
            "e 1/10 i 11107/40794: loss 5.971315383911133\n",
            "e 1/10 i 11108/40794: loss 6.301187992095947\n",
            "e 1/10 i 11109/40794: loss 5.612242221832275\n",
            "e 1/10 i 11110/40794: loss 5.59103536605835\n",
            "e 1/10 i 11111/40794: loss 5.960143566131592\n",
            "e 1/10 i 11112/40794: loss 5.839576244354248\n",
            "e 1/10 i 11113/40794: loss 6.184209823608398\n",
            "e 1/10 i 11114/40794: loss 6.267031192779541\n",
            "e 1/10 i 11115/40794: loss 5.777083873748779\n",
            "e 1/10 i 11116/40794: loss 5.521948337554932\n",
            "e 1/10 i 11117/40794: loss 5.9110107421875\n",
            "e 1/10 i 11118/40794: loss 5.981462478637695\n",
            "e 1/10 i 11119/40794: loss 5.9255266189575195\n",
            "e 1/10 i 11120/40794: loss 6.252638816833496\n",
            "e 1/10 i 11121/40794: loss 6.049156188964844\n",
            "e 1/10 i 11122/40794: loss 5.8483171463012695\n",
            "e 1/10 i 11123/40794: loss 5.698050498962402\n",
            "e 1/10 i 11124/40794: loss 5.841557025909424\n",
            "e 1/10 i 11125/40794: loss 5.633987903594971\n",
            "e 1/10 i 11126/40794: loss 5.998400688171387\n",
            "e 1/10 i 11127/40794: loss 5.866565704345703\n",
            "e 1/10 i 11128/40794: loss 5.924457550048828\n",
            "e 1/10 i 11129/40794: loss 5.7522053718566895\n",
            "e 1/10 i 11130/40794: loss 6.081700325012207\n",
            "e 1/10 i 11131/40794: loss 5.993393898010254\n",
            "e 1/10 i 11132/40794: loss 6.09972620010376\n",
            "e 1/10 i 11133/40794: loss 6.051259994506836\n",
            "e 1/10 i 11134/40794: loss 6.170539855957031\n",
            "e 1/10 i 11135/40794: loss 6.074617862701416\n",
            "e 1/10 i 11136/40794: loss 5.583963394165039\n",
            "e 1/10 i 11137/40794: loss 5.639191627502441\n",
            "e 1/10 i 11138/40794: loss 6.028513431549072\n",
            "e 1/10 i 11139/40794: loss 6.0187225341796875\n",
            "e 1/10 i 11140/40794: loss 6.192054748535156\n",
            "e 1/10 i 11141/40794: loss 5.802903175354004\n",
            "e 1/10 i 11142/40794: loss 5.976733684539795\n",
            "e 1/10 i 11143/40794: loss 5.908374309539795\n",
            "e 1/10 i 11144/40794: loss 6.106453895568848\n",
            "e 1/10 i 11145/40794: loss 6.050513744354248\n",
            "e 1/10 i 11146/40794: loss 5.967471122741699\n",
            "e 1/10 i 11147/40794: loss 6.0245361328125\n",
            "e 1/10 i 11148/40794: loss 5.733129024505615\n",
            "e 1/10 i 11149/40794: loss 5.978167533874512\n",
            "e 1/10 i 11150/40794: loss 5.7447285652160645\n",
            "e 1/10 i 11151/40794: loss 6.250320911407471\n",
            "e 1/10 i 11152/40794: loss 5.853584289550781\n",
            "e 1/10 i 11153/40794: loss 6.210026264190674\n",
            "e 1/10 i 11154/40794: loss 6.169325828552246\n",
            "e 1/10 i 11155/40794: loss 6.058928489685059\n",
            "e 1/10 i 11156/40794: loss 5.955216407775879\n",
            "e 1/10 i 11157/40794: loss 5.941044330596924\n",
            "e 1/10 i 11158/40794: loss 5.976675510406494\n",
            "e 1/10 i 11159/40794: loss 6.07684326171875\n",
            "e 1/10 i 11160/40794: loss 5.744985103607178\n",
            "e 1/10 i 11161/40794: loss 6.130906581878662\n",
            "e 1/10 i 11162/40794: loss 5.818108081817627\n",
            "e 1/10 i 11163/40794: loss 5.855857849121094\n",
            "e 1/10 i 11164/40794: loss 6.186291694641113\n",
            "e 1/10 i 11165/40794: loss 5.969745635986328\n",
            "e 1/10 i 11166/40794: loss 5.87843132019043\n",
            "e 1/10 i 11167/40794: loss 6.0785698890686035\n",
            "e 1/10 i 11168/40794: loss 6.001510143280029\n",
            "e 1/10 i 11169/40794: loss 5.826810359954834\n",
            "e 1/10 i 11170/40794: loss 5.750153541564941\n",
            "e 1/10 i 11171/40794: loss 5.9374213218688965\n",
            "e 1/10 i 11172/40794: loss 5.653575420379639\n",
            "e 1/10 i 11173/40794: loss 6.073782444000244\n",
            "e 1/10 i 11174/40794: loss 5.725498676300049\n",
            "e 1/10 i 11175/40794: loss 6.246792316436768\n",
            "e 1/10 i 11176/40794: loss 5.740904808044434\n",
            "e 1/10 i 11177/40794: loss 6.171988487243652\n",
            "e 1/10 i 11178/40794: loss 5.63681697845459\n",
            "e 1/10 i 11179/40794: loss 5.805455684661865\n",
            "e 1/10 i 11180/40794: loss 5.72236442565918\n",
            "e 1/10 i 11181/40794: loss 6.015561103820801\n",
            "e 1/10 i 11182/40794: loss 5.848062038421631\n",
            "e 1/10 i 11183/40794: loss 5.908021450042725\n",
            "e 1/10 i 11184/40794: loss 5.825241565704346\n",
            "e 1/10 i 11185/40794: loss 5.9558539390563965\n",
            "e 1/10 i 11186/40794: loss 6.3704609870910645\n",
            "e 1/10 i 11187/40794: loss 6.186306953430176\n",
            "e 1/10 i 11188/40794: loss 5.941067695617676\n",
            "e 1/10 i 11189/40794: loss 5.891536712646484\n",
            "e 1/10 i 11190/40794: loss 6.138806343078613\n",
            "e 1/10 i 11191/40794: loss 5.453577518463135\n",
            "e 1/10 i 11192/40794: loss 6.1226067543029785\n",
            "e 1/10 i 11193/40794: loss 6.043448448181152\n",
            "e 1/10 i 11194/40794: loss 6.108623027801514\n",
            "e 1/10 i 11195/40794: loss 5.8787994384765625\n",
            "e 1/10 i 11196/40794: loss 5.724399566650391\n",
            "e 1/10 i 11197/40794: loss 5.825303077697754\n",
            "e 1/10 i 11198/40794: loss 6.069095611572266\n",
            "e 1/10 i 11199/40794: loss 6.035165309906006\n",
            "e 1/10 i 11200/40794: loss 5.6903395652771\n",
            "e 1/10 i 11201/40794: loss 5.980225086212158\n",
            "e 1/10 i 11202/40794: loss 5.701959609985352\n",
            "e 1/10 i 11203/40794: loss 6.132144927978516\n",
            "e 1/10 i 11204/40794: loss 5.962111949920654\n",
            "e 1/10 i 11205/40794: loss 5.87185525894165\n",
            "e 1/10 i 11206/40794: loss 6.028305530548096\n",
            "e 1/10 i 11207/40794: loss 5.528232097625732\n",
            "e 1/10 i 11208/40794: loss 5.9705634117126465\n",
            "e 1/10 i 11209/40794: loss 5.996606826782227\n",
            "e 1/10 i 11210/40794: loss 5.770041465759277\n",
            "e 1/10 i 11211/40794: loss 5.756016254425049\n",
            "e 1/10 i 11212/40794: loss 5.956456661224365\n",
            "e 1/10 i 11213/40794: loss 6.119822025299072\n",
            "e 1/10 i 11214/40794: loss 5.77293586730957\n",
            "e 1/10 i 11215/40794: loss 5.794566631317139\n",
            "e 1/10 i 11216/40794: loss 5.897789478302002\n",
            "e 1/10 i 11217/40794: loss 5.593247890472412\n",
            "e 1/10 i 11218/40794: loss 5.94901180267334\n",
            "e 1/10 i 11219/40794: loss 6.101234436035156\n",
            "e 1/10 i 11220/40794: loss 5.85468053817749\n",
            "e 1/10 i 11221/40794: loss 5.737403869628906\n",
            "e 1/10 i 11222/40794: loss 5.89235258102417\n",
            "e 1/10 i 11223/40794: loss 6.302122116088867\n",
            "e 1/10 i 11224/40794: loss 6.03519344329834\n",
            "e 1/10 i 11225/40794: loss 6.007678985595703\n",
            "e 1/10 i 11226/40794: loss 5.710965633392334\n",
            "e 1/10 i 11227/40794: loss 5.5821123123168945\n",
            "e 1/10 i 11228/40794: loss 5.72323751449585\n",
            "e 1/10 i 11229/40794: loss 6.042543411254883\n",
            "e 1/10 i 11230/40794: loss 5.651944637298584\n",
            "e 1/10 i 11231/40794: loss 5.857729434967041\n",
            "e 1/10 i 11232/40794: loss 6.185202121734619\n",
            "e 1/10 i 11233/40794: loss 6.0740814208984375\n",
            "e 1/10 i 11234/40794: loss 5.833337783813477\n",
            "e 1/10 i 11235/40794: loss 5.778613090515137\n",
            "e 1/10 i 11236/40794: loss 5.895565986633301\n",
            "e 1/10 i 11237/40794: loss 5.960473537445068\n",
            "e 1/10 i 11238/40794: loss 5.6703386306762695\n",
            "e 1/10 i 11239/40794: loss 5.99990701675415\n",
            "e 1/10 i 11240/40794: loss 6.0352044105529785\n",
            "e 1/10 i 11241/40794: loss 6.153970718383789\n",
            "e 1/10 i 11242/40794: loss 5.901852607727051\n",
            "e 1/10 i 11243/40794: loss 5.900484085083008\n",
            "e 1/10 i 11244/40794: loss 5.859490871429443\n",
            "e 1/10 i 11245/40794: loss 6.09313440322876\n",
            "e 1/10 i 11246/40794: loss 6.093207836151123\n",
            "e 1/10 i 11247/40794: loss 5.8924479484558105\n",
            "e 1/10 i 11248/40794: loss 5.909605979919434\n",
            "e 1/10 i 11249/40794: loss 6.349304676055908\n",
            "e 1/10 i 11250/40794: loss 5.998721599578857\n",
            "e 1/10 i 11251/40794: loss 5.9093546867370605\n",
            "e 1/10 i 11252/40794: loss 5.9252190589904785\n",
            "e 1/10 i 11253/40794: loss 5.923858642578125\n",
            "e 1/10 i 11254/40794: loss 5.900362491607666\n",
            "e 1/10 i 11255/40794: loss 5.88030481338501\n",
            "e 1/10 i 11256/40794: loss 5.602309226989746\n",
            "e 1/10 i 11257/40794: loss 6.154270648956299\n",
            "e 1/10 i 11258/40794: loss 6.154188632965088\n",
            "e 1/10 i 11259/40794: loss 5.834022521972656\n",
            "e 1/10 i 11260/40794: loss 5.8491973876953125\n",
            "e 1/10 i 11261/40794: loss 6.320408821105957\n",
            "e 1/10 i 11262/40794: loss 6.171167850494385\n",
            "e 1/10 i 11263/40794: loss 5.96178674697876\n",
            "e 1/10 i 11264/40794: loss 5.905221939086914\n",
            "e 1/10 i 11265/40794: loss 5.910182476043701\n",
            "e 1/10 i 11266/40794: loss 5.823802947998047\n",
            "e 1/10 i 11267/40794: loss 5.9407501220703125\n",
            "e 1/10 i 11268/40794: loss 5.933190822601318\n",
            "e 1/10 i 11269/40794: loss 5.688691139221191\n",
            "e 1/10 i 11270/40794: loss 6.102336883544922\n",
            "e 1/10 i 11271/40794: loss 5.938782215118408\n",
            "e 1/10 i 11272/40794: loss 6.092752456665039\n",
            "e 1/10 i 11273/40794: loss 5.893380165100098\n",
            "e 1/10 i 11274/40794: loss 6.028862476348877\n",
            "e 1/10 i 11275/40794: loss 5.880526542663574\n",
            "e 1/10 i 11276/40794: loss 5.729915142059326\n",
            "e 1/10 i 11277/40794: loss 5.716742515563965\n",
            "e 1/10 i 11278/40794: loss 6.093142032623291\n",
            "e 1/10 i 11279/40794: loss 5.964742660522461\n",
            "e 1/10 i 11280/40794: loss 6.056276798248291\n",
            "e 1/10 i 11281/40794: loss 5.842390060424805\n",
            "e 1/10 i 11282/40794: loss 5.8122968673706055\n",
            "e 1/10 i 11283/40794: loss 5.633656024932861\n",
            "e 1/10 i 11284/40794: loss 5.9780755043029785\n",
            "e 1/10 i 11285/40794: loss 6.022175312042236\n",
            "e 1/10 i 11286/40794: loss 5.764062881469727\n",
            "e 1/10 i 11287/40794: loss 6.479924201965332\n",
            "e 1/10 i 11288/40794: loss 5.7176055908203125\n",
            "e 1/10 i 11289/40794: loss 6.0485148429870605\n",
            "e 1/10 i 11290/40794: loss 5.812991142272949\n",
            "e 1/10 i 11291/40794: loss 6.005690574645996\n",
            "e 1/10 i 11292/40794: loss 5.836478233337402\n",
            "e 1/10 i 11293/40794: loss 5.651039123535156\n",
            "e 1/10 i 11294/40794: loss 5.798596382141113\n",
            "e 1/10 i 11295/40794: loss 5.8708319664001465\n",
            "e 1/10 i 11296/40794: loss 5.9372076988220215\n",
            "e 1/10 i 11297/40794: loss 6.079430103302002\n",
            "e 1/10 i 11298/40794: loss 6.186669826507568\n",
            "e 1/10 i 11299/40794: loss 6.067561626434326\n",
            "e 1/10 i 11300/40794: loss 5.881521701812744\n",
            "e 1/10 i 11301/40794: loss 5.8716139793396\n",
            "e 1/10 i 11302/40794: loss 6.042358875274658\n",
            "e 1/10 i 11303/40794: loss 5.829338073730469\n",
            "e 1/10 i 11304/40794: loss 5.813796520233154\n",
            "e 1/10 i 11305/40794: loss 6.121177673339844\n",
            "e 1/10 i 11306/40794: loss 5.912424087524414\n",
            "e 1/10 i 11307/40794: loss 5.935280799865723\n",
            "e 1/10 i 11308/40794: loss 5.6441240310668945\n",
            "e 1/10 i 11309/40794: loss 5.800048828125\n",
            "e 1/10 i 11310/40794: loss 5.928306579589844\n",
            "e 1/10 i 11311/40794: loss 6.274875164031982\n",
            "e 1/10 i 11312/40794: loss 5.7821455001831055\n",
            "e 1/10 i 11313/40794: loss 6.245850563049316\n",
            "e 1/10 i 11314/40794: loss 6.003716468811035\n",
            "e 1/10 i 11315/40794: loss 5.843067169189453\n",
            "e 1/10 i 11316/40794: loss 5.820483207702637\n",
            "e 1/10 i 11317/40794: loss 6.026623725891113\n",
            "e 1/10 i 11318/40794: loss 6.179045677185059\n",
            "e 1/10 i 11319/40794: loss 6.079379558563232\n",
            "e 1/10 i 11320/40794: loss 5.503814697265625\n",
            "e 1/10 i 11321/40794: loss 5.694759368896484\n",
            "e 1/10 i 11322/40794: loss 6.278510570526123\n",
            "e 1/10 i 11323/40794: loss 5.891485214233398\n",
            "e 1/10 i 11324/40794: loss 5.86016845703125\n",
            "e 1/10 i 11325/40794: loss 5.955805778503418\n",
            "e 1/10 i 11326/40794: loss 6.0917277336120605\n",
            "e 1/10 i 11327/40794: loss 6.078014850616455\n",
            "e 1/10 i 11328/40794: loss 6.005588531494141\n",
            "e 1/10 i 11329/40794: loss 6.1155571937561035\n",
            "e 1/10 i 11330/40794: loss 6.1487884521484375\n",
            "e 1/10 i 11331/40794: loss 6.062470436096191\n",
            "e 1/10 i 11332/40794: loss 5.762041091918945\n",
            "e 1/10 i 11333/40794: loss 5.902891635894775\n",
            "e 1/10 i 11334/40794: loss 6.137624263763428\n",
            "e 1/10 i 11335/40794: loss 6.032548904418945\n",
            "e 1/10 i 11336/40794: loss 5.90640115737915\n",
            "e 1/10 i 11337/40794: loss 5.5842061042785645\n",
            "e 1/10 i 11338/40794: loss 5.950793743133545\n",
            "e 1/10 i 11339/40794: loss 6.148026943206787\n",
            "e 1/10 i 11340/40794: loss 6.1671223640441895\n",
            "e 1/10 i 11341/40794: loss 6.250496864318848\n",
            "e 1/10 i 11342/40794: loss 5.8816728591918945\n",
            "e 1/10 i 11343/40794: loss 5.828218460083008\n",
            "e 1/10 i 11344/40794: loss 6.092874050140381\n",
            "e 1/10 i 11345/40794: loss 6.103277683258057\n",
            "e 1/10 i 11346/40794: loss 5.93651819229126\n",
            "e 1/10 i 11347/40794: loss 5.700897216796875\n",
            "e 1/10 i 11348/40794: loss 6.11044454574585\n",
            "e 1/10 i 11349/40794: loss 6.3801798820495605\n",
            "e 1/10 i 11350/40794: loss 5.653179168701172\n",
            "e 1/10 i 11351/40794: loss 5.864927291870117\n",
            "e 1/10 i 11352/40794: loss 5.794344902038574\n",
            "e 1/10 i 11353/40794: loss 5.784657955169678\n",
            "e 1/10 i 11354/40794: loss 5.979511737823486\n",
            "e 1/10 i 11355/40794: loss 5.937394142150879\n",
            "e 1/10 i 11356/40794: loss 5.8882904052734375\n",
            "e 1/10 i 11357/40794: loss 5.869607925415039\n",
            "e 1/10 i 11358/40794: loss 6.18833589553833\n",
            "e 1/10 i 11359/40794: loss 6.073395729064941\n",
            "e 1/10 i 11360/40794: loss 5.815343379974365\n",
            "e 1/10 i 11361/40794: loss 5.951871871948242\n",
            "e 1/10 i 11362/40794: loss 5.927145004272461\n",
            "e 1/10 i 11363/40794: loss 5.865232467651367\n",
            "e 1/10 i 11364/40794: loss 5.999574661254883\n",
            "e 1/10 i 11365/40794: loss 5.936711311340332\n",
            "e 1/10 i 11366/40794: loss 6.188431262969971\n",
            "e 1/10 i 11367/40794: loss 5.9662628173828125\n",
            "e 1/10 i 11368/40794: loss 5.888885974884033\n",
            "e 1/10 i 11369/40794: loss 6.0179572105407715\n",
            "e 1/10 i 11370/40794: loss 5.935827255249023\n",
            "e 1/10 i 11371/40794: loss 5.916301250457764\n",
            "e 1/10 i 11372/40794: loss 5.918910503387451\n",
            "e 1/10 i 11373/40794: loss 6.12497615814209\n",
            "e 1/10 i 11374/40794: loss 5.821364879608154\n",
            "e 1/10 i 11375/40794: loss 5.87276029586792\n",
            "e 1/10 i 11376/40794: loss 5.5046796798706055\n",
            "e 1/10 i 11377/40794: loss 5.80633020401001\n",
            "e 1/10 i 11378/40794: loss 5.823363780975342\n",
            "e 1/10 i 11379/40794: loss 6.061139106750488\n",
            "e 1/10 i 11380/40794: loss 6.316842555999756\n",
            "e 1/10 i 11381/40794: loss 5.903848648071289\n",
            "e 1/10 i 11382/40794: loss 5.827359676361084\n",
            "e 1/10 i 11383/40794: loss 6.212526321411133\n",
            "e 1/10 i 11384/40794: loss 5.8496413230896\n",
            "e 1/10 i 11385/40794: loss 6.008054256439209\n",
            "e 1/10 i 11386/40794: loss 5.871021747589111\n",
            "e 1/10 i 11387/40794: loss 5.937824726104736\n",
            "e 1/10 i 11388/40794: loss 5.790446758270264\n",
            "e 1/10 i 11389/40794: loss 5.75205135345459\n",
            "e 1/10 i 11390/40794: loss 5.954801082611084\n",
            "e 1/10 i 11391/40794: loss 6.222836017608643\n",
            "e 1/10 i 11392/40794: loss 5.743236541748047\n",
            "e 1/10 i 11393/40794: loss 6.06912899017334\n",
            "e 1/10 i 11394/40794: loss 5.473260879516602\n",
            "e 1/10 i 11395/40794: loss 6.0381646156311035\n",
            "e 1/10 i 11396/40794: loss 5.953629016876221\n",
            "e 1/10 i 11397/40794: loss 6.1197662353515625\n",
            "e 1/10 i 11398/40794: loss 5.877573490142822\n",
            "e 1/10 i 11399/40794: loss 5.490089416503906\n",
            "e 1/10 i 11400/40794: loss 6.1229400634765625\n",
            "e 1/10 i 11401/40794: loss 6.4021196365356445\n",
            "e 1/10 i 11402/40794: loss 5.989748954772949\n",
            "e 1/10 i 11403/40794: loss 5.876768112182617\n",
            "e 1/10 i 11404/40794: loss 5.993003845214844\n",
            "e 1/10 i 11405/40794: loss 5.990085124969482\n",
            "e 1/10 i 11406/40794: loss 5.927425861358643\n",
            "e 1/10 i 11407/40794: loss 6.103202819824219\n",
            "e 1/10 i 11408/40794: loss 5.843737602233887\n",
            "e 1/10 i 11409/40794: loss 5.484401702880859\n",
            "e 1/10 i 11410/40794: loss 6.3302459716796875\n",
            "e 1/10 i 11411/40794: loss 6.024609565734863\n",
            "e 1/10 i 11412/40794: loss 5.9550461769104\n",
            "e 1/10 i 11413/40794: loss 6.155147552490234\n",
            "e 1/10 i 11414/40794: loss 5.984449863433838\n",
            "e 1/10 i 11415/40794: loss 6.181976318359375\n",
            "e 1/10 i 11416/40794: loss 5.920423984527588\n",
            "e 1/10 i 11417/40794: loss 5.802465915679932\n",
            "e 1/10 i 11418/40794: loss 6.081555366516113\n",
            "e 1/10 i 11419/40794: loss 5.934336185455322\n",
            "e 1/10 i 11420/40794: loss 5.474518775939941\n",
            "e 1/10 i 11421/40794: loss 5.9629387855529785\n",
            "e 1/10 i 11422/40794: loss 5.926914691925049\n",
            "e 1/10 i 11423/40794: loss 5.912225723266602\n",
            "e 1/10 i 11424/40794: loss 6.241837024688721\n",
            "e 1/10 i 11425/40794: loss 5.868521213531494\n",
            "e 1/10 i 11426/40794: loss 5.9721479415893555\n",
            "e 1/10 i 11427/40794: loss 5.826817035675049\n",
            "e 1/10 i 11428/40794: loss 5.9822587966918945\n",
            "e 1/10 i 11429/40794: loss 6.143216133117676\n",
            "e 1/10 i 11430/40794: loss 5.951804161071777\n",
            "e 1/10 i 11431/40794: loss 5.920013904571533\n",
            "e 1/10 i 11432/40794: loss 5.454855442047119\n",
            "e 1/10 i 11433/40794: loss 5.801644802093506\n",
            "e 1/10 i 11434/40794: loss 5.762296199798584\n",
            "e 1/10 i 11435/40794: loss 5.786292552947998\n",
            "e 1/10 i 11436/40794: loss 5.783973217010498\n",
            "e 1/10 i 11437/40794: loss 6.288888454437256\n",
            "e 1/10 i 11438/40794: loss 5.785918712615967\n",
            "e 1/10 i 11439/40794: loss 5.919516086578369\n",
            "e 1/10 i 11440/40794: loss 5.787520408630371\n",
            "e 1/10 i 11441/40794: loss 5.819736957550049\n",
            "e 1/10 i 11442/40794: loss 5.960367679595947\n",
            "e 1/10 i 11443/40794: loss 5.992154598236084\n",
            "e 1/10 i 11444/40794: loss 6.080658435821533\n",
            "e 1/10 i 11445/40794: loss 5.823542594909668\n",
            "e 1/10 i 11446/40794: loss 5.850409030914307\n",
            "e 1/10 i 11447/40794: loss 5.709886074066162\n",
            "e 1/10 i 11448/40794: loss 6.115538597106934\n",
            "e 1/10 i 11449/40794: loss 5.821920394897461\n",
            "e 1/10 i 11450/40794: loss 5.914926052093506\n",
            "e 1/10 i 11451/40794: loss 6.022762775421143\n",
            "e 1/10 i 11452/40794: loss 5.87934684753418\n",
            "e 1/10 i 11453/40794: loss 5.768454551696777\n",
            "e 1/10 i 11454/40794: loss 6.048742294311523\n",
            "e 1/10 i 11455/40794: loss 6.088820457458496\n",
            "e 1/10 i 11456/40794: loss 6.275144577026367\n",
            "e 1/10 i 11457/40794: loss 5.796872615814209\n",
            "e 1/10 i 11458/40794: loss 5.558457374572754\n",
            "e 1/10 i 11459/40794: loss 5.899319171905518\n",
            "e 1/10 i 11460/40794: loss 5.928552150726318\n",
            "e 1/10 i 11461/40794: loss 6.068999767303467\n",
            "e 1/10 i 11462/40794: loss 5.778609752655029\n",
            "e 1/10 i 11463/40794: loss 6.016668319702148\n",
            "e 1/10 i 11464/40794: loss 6.2606730461120605\n",
            "e 1/10 i 11465/40794: loss 5.72808313369751\n",
            "e 1/10 i 11466/40794: loss 6.17650032043457\n",
            "e 1/10 i 11467/40794: loss 5.547373294830322\n",
            "e 1/10 i 11468/40794: loss 5.7569684982299805\n",
            "e 1/10 i 11469/40794: loss 6.095026969909668\n",
            "e 1/10 i 11470/40794: loss 5.87750244140625\n",
            "e 1/10 i 11471/40794: loss 5.887853145599365\n",
            "e 1/10 i 11472/40794: loss 5.829038619995117\n",
            "e 1/10 i 11473/40794: loss 5.79983377456665\n",
            "e 1/10 i 11474/40794: loss 5.843322277069092\n",
            "e 1/10 i 11475/40794: loss 5.937527179718018\n",
            "e 1/10 i 11476/40794: loss 6.091495037078857\n",
            "e 1/10 i 11477/40794: loss 5.823774814605713\n",
            "e 1/10 i 11478/40794: loss 5.785438060760498\n",
            "e 1/10 i 11479/40794: loss 5.802642345428467\n",
            "e 1/10 i 11480/40794: loss 5.805564880371094\n",
            "e 1/10 i 11481/40794: loss 5.8520588874816895\n",
            "e 1/10 i 11482/40794: loss 5.745049953460693\n",
            "e 1/10 i 11483/40794: loss 5.990212440490723\n",
            "e 1/10 i 11484/40794: loss 6.157015800476074\n",
            "e 1/10 i 11485/40794: loss 6.075455665588379\n",
            "e 1/10 i 11486/40794: loss 5.959131240844727\n",
            "e 1/10 i 11487/40794: loss 5.69821310043335\n",
            "e 1/10 i 11488/40794: loss 5.884247303009033\n",
            "e 1/10 i 11489/40794: loss 5.952620983123779\n",
            "e 1/10 i 11490/40794: loss 5.726536273956299\n",
            "e 1/10 i 11491/40794: loss 6.018101692199707\n",
            "e 1/10 i 11492/40794: loss 5.964500904083252\n",
            "e 1/10 i 11493/40794: loss 5.872848033905029\n",
            "e 1/10 i 11494/40794: loss 6.112225532531738\n",
            "e 1/10 i 11495/40794: loss 6.270252704620361\n",
            "e 1/10 i 11496/40794: loss 5.663634777069092\n",
            "e 1/10 i 11497/40794: loss 6.096217155456543\n",
            "e 1/10 i 11498/40794: loss 6.064074993133545\n",
            "e 1/10 i 11499/40794: loss 5.789271354675293\n",
            "e 1/10 i 11500/40794: loss 5.732965469360352\n",
            "e 1/10 i 11501/40794: loss 5.756978511810303\n",
            "e 1/10 i 11502/40794: loss 5.8867621421813965\n",
            "e 1/10 i 11503/40794: loss 6.101871967315674\n",
            "e 1/10 i 11504/40794: loss 6.0855584144592285\n",
            "e 1/10 i 11505/40794: loss 5.74994421005249\n",
            "e 1/10 i 11506/40794: loss 6.040389060974121\n",
            "e 1/10 i 11507/40794: loss 6.075904369354248\n",
            "e 1/10 i 11508/40794: loss 6.220801830291748\n",
            "e 1/10 i 11509/40794: loss 5.995514392852783\n",
            "e 1/10 i 11510/40794: loss 6.330088138580322\n",
            "e 1/10 i 11511/40794: loss 5.99650764465332\n",
            "e 1/10 i 11512/40794: loss 6.238799095153809\n",
            "e 1/10 i 11513/40794: loss 5.8151140213012695\n",
            "e 1/10 i 11514/40794: loss 6.145168304443359\n",
            "e 1/10 i 11515/40794: loss 6.012094497680664\n",
            "e 1/10 i 11516/40794: loss 5.822596073150635\n",
            "e 1/10 i 11517/40794: loss 5.891017436981201\n",
            "e 1/10 i 11518/40794: loss 5.488778591156006\n",
            "e 1/10 i 11519/40794: loss 5.893232345581055\n",
            "e 1/10 i 11520/40794: loss 5.880861759185791\n",
            "e 1/10 i 11521/40794: loss 5.794521808624268\n",
            "e 1/10 i 11522/40794: loss 6.0774431228637695\n",
            "e 1/10 i 11523/40794: loss 5.936334609985352\n",
            "e 1/10 i 11524/40794: loss 5.695191860198975\n",
            "e 1/10 i 11525/40794: loss 5.529512882232666\n",
            "e 1/10 i 11526/40794: loss 5.990960121154785\n",
            "e 1/10 i 11527/40794: loss 5.789820671081543\n",
            "e 1/10 i 11528/40794: loss 5.699881076812744\n",
            "e 1/10 i 11529/40794: loss 6.295324802398682\n",
            "e 1/10 i 11530/40794: loss 6.002826690673828\n",
            "e 1/10 i 11531/40794: loss 5.748898029327393\n",
            "e 1/10 i 11532/40794: loss 6.300792217254639\n",
            "e 1/10 i 11533/40794: loss 6.019436359405518\n",
            "e 1/10 i 11534/40794: loss 5.87587308883667\n",
            "e 1/10 i 11535/40794: loss 5.864907264709473\n",
            "e 1/10 i 11536/40794: loss 5.995926380157471\n",
            "e 1/10 i 11537/40794: loss 6.067429542541504\n",
            "e 1/10 i 11538/40794: loss 5.893041133880615\n",
            "e 1/10 i 11539/40794: loss 6.021207332611084\n",
            "e 1/10 i 11540/40794: loss 6.218260288238525\n",
            "e 1/10 i 11541/40794: loss 5.749959468841553\n",
            "e 1/10 i 11542/40794: loss 5.886902809143066\n",
            "e 1/10 i 11543/40794: loss 5.77610969543457\n",
            "e 1/10 i 11544/40794: loss 5.756105899810791\n",
            "e 1/10 i 11545/40794: loss 6.085907936096191\n",
            "e 1/10 i 11546/40794: loss 5.684171676635742\n",
            "e 1/10 i 11547/40794: loss 5.6900434494018555\n",
            "e 1/10 i 11548/40794: loss 5.987477779388428\n",
            "e 1/10 i 11549/40794: loss 5.815537452697754\n",
            "e 1/10 i 11550/40794: loss 5.787128448486328\n",
            "e 1/10 i 11551/40794: loss 5.834691047668457\n",
            "e 1/10 i 11552/40794: loss 5.914547920227051\n",
            "e 1/10 i 11553/40794: loss 5.872995376586914\n",
            "e 1/10 i 11554/40794: loss 5.9552507400512695\n",
            "e 1/10 i 11555/40794: loss 6.135226249694824\n",
            "e 1/10 i 11556/40794: loss 6.2058844566345215\n",
            "e 1/10 i 11557/40794: loss 6.06935453414917\n",
            "e 1/10 i 11558/40794: loss 5.771917819976807\n",
            "e 1/10 i 11559/40794: loss 6.130975723266602\n",
            "e 1/10 i 11560/40794: loss 6.041438102722168\n",
            "e 1/10 i 11561/40794: loss 6.059239864349365\n",
            "e 1/10 i 11562/40794: loss 6.309139728546143\n",
            "e 1/10 i 11563/40794: loss 5.477565765380859\n",
            "e 1/10 i 11564/40794: loss 6.0134477615356445\n",
            "e 1/10 i 11565/40794: loss 6.058034896850586\n",
            "e 1/10 i 11566/40794: loss 5.878302097320557\n",
            "e 1/10 i 11567/40794: loss 6.056127548217773\n",
            "e 1/10 i 11568/40794: loss 5.737236499786377\n",
            "e 1/10 i 11569/40794: loss 5.8147873878479\n",
            "e 1/10 i 11570/40794: loss 6.03358268737793\n",
            "e 1/10 i 11571/40794: loss 5.894877910614014\n",
            "e 1/10 i 11572/40794: loss 5.856579780578613\n",
            "e 1/10 i 11573/40794: loss 6.066585063934326\n",
            "e 1/10 i 11574/40794: loss 5.8674445152282715\n",
            "e 1/10 i 11575/40794: loss 5.982784748077393\n",
            "e 1/10 i 11576/40794: loss 5.766562461853027\n",
            "e 1/10 i 11577/40794: loss 5.836332321166992\n",
            "e 1/10 i 11578/40794: loss 5.828036785125732\n",
            "e 1/10 i 11579/40794: loss 5.749725818634033\n",
            "e 1/10 i 11580/40794: loss 5.806323051452637\n",
            "e 1/10 i 11581/40794: loss 5.620824337005615\n",
            "e 1/10 i 11582/40794: loss 5.859091281890869\n",
            "e 1/10 i 11583/40794: loss 5.978181838989258\n",
            "e 1/10 i 11584/40794: loss 5.851024627685547\n",
            "e 1/10 i 11585/40794: loss 5.571541786193848\n",
            "e 1/10 i 11586/40794: loss 6.116637706756592\n",
            "e 1/10 i 11587/40794: loss 6.0795159339904785\n",
            "e 1/10 i 11588/40794: loss 5.676339149475098\n",
            "e 1/10 i 11589/40794: loss 6.117982387542725\n",
            "e 1/10 i 11590/40794: loss 6.120416641235352\n",
            "e 1/10 i 11591/40794: loss 6.022831916809082\n",
            "e 1/10 i 11592/40794: loss 5.669183731079102\n",
            "e 1/10 i 11593/40794: loss 5.875112056732178\n",
            "e 1/10 i 11594/40794: loss 5.638363361358643\n",
            "e 1/10 i 11595/40794: loss 6.053042411804199\n",
            "e 1/10 i 11596/40794: loss 5.868449687957764\n",
            "e 1/10 i 11597/40794: loss 6.015222072601318\n",
            "e 1/10 i 11598/40794: loss 6.066389083862305\n",
            "e 1/10 i 11599/40794: loss 5.664985179901123\n",
            "e 1/10 i 11600/40794: loss 5.743417263031006\n",
            "e 1/10 i 11601/40794: loss 6.2492451667785645\n",
            "e 1/10 i 11602/40794: loss 5.550253868103027\n",
            "e 1/10 i 11603/40794: loss 5.929653167724609\n",
            "e 1/10 i 11604/40794: loss 5.904458999633789\n",
            "e 1/10 i 11605/40794: loss 5.594052314758301\n",
            "e 1/10 i 11606/40794: loss 6.169189453125\n",
            "e 1/10 i 11607/40794: loss 5.724918365478516\n",
            "e 1/10 i 11608/40794: loss 5.973048210144043\n",
            "e 1/10 i 11609/40794: loss 6.142773628234863\n",
            "e 1/10 i 11610/40794: loss 5.913791179656982\n",
            "e 1/10 i 11611/40794: loss 5.74661922454834\n",
            "e 1/10 i 11612/40794: loss 5.876773834228516\n",
            "e 1/10 i 11613/40794: loss 5.832944869995117\n",
            "e 1/10 i 11614/40794: loss 6.132387161254883\n",
            "e 1/10 i 11615/40794: loss 6.28432035446167\n",
            "e 1/10 i 11616/40794: loss 6.018072605133057\n",
            "e 1/10 i 11617/40794: loss 5.827081680297852\n",
            "e 1/10 i 11618/40794: loss 5.714200019836426\n",
            "e 1/10 i 11619/40794: loss 6.276477336883545\n",
            "e 1/10 i 11620/40794: loss 5.9146647453308105\n",
            "e 1/10 i 11621/40794: loss 5.836801528930664\n",
            "e 1/10 i 11622/40794: loss 5.4790425300598145\n",
            "e 1/10 i 11623/40794: loss 5.888067722320557\n",
            "e 1/10 i 11624/40794: loss 5.814711570739746\n",
            "e 1/10 i 11625/40794: loss 5.881396293640137\n",
            "e 1/10 i 11626/40794: loss 5.9290571212768555\n",
            "e 1/10 i 11627/40794: loss 5.785914421081543\n",
            "e 1/10 i 11628/40794: loss 5.687498569488525\n",
            "e 1/10 i 11629/40794: loss 5.924615859985352\n",
            "e 1/10 i 11630/40794: loss 6.043705940246582\n",
            "e 1/10 i 11631/40794: loss 5.996658802032471\n",
            "e 1/10 i 11632/40794: loss 5.901864528656006\n",
            "e 1/10 i 11633/40794: loss 5.924045085906982\n",
            "e 1/10 i 11634/40794: loss 6.030338287353516\n",
            "e 1/10 i 11635/40794: loss 5.74398136138916\n",
            "e 1/10 i 11636/40794: loss 5.889287948608398\n",
            "e 1/10 i 11637/40794: loss 5.679872512817383\n",
            "e 1/10 i 11638/40794: loss 6.078750133514404\n",
            "e 1/10 i 11639/40794: loss 5.951342582702637\n",
            "e 1/10 i 11640/40794: loss 5.778713703155518\n",
            "e 1/10 i 11641/40794: loss 5.752397060394287\n",
            "e 1/10 i 11642/40794: loss 5.953091621398926\n",
            "e 1/10 i 11643/40794: loss 5.829033851623535\n",
            "e 1/10 i 11644/40794: loss 5.755692958831787\n",
            "e 1/10 i 11645/40794: loss 6.243680477142334\n",
            "e 1/10 i 11646/40794: loss 6.140920639038086\n",
            "e 1/10 i 11647/40794: loss 6.226512908935547\n",
            "e 1/10 i 11648/40794: loss 6.105579376220703\n",
            "e 1/10 i 11649/40794: loss 5.774633884429932\n",
            "e 1/10 i 11650/40794: loss 5.768822193145752\n",
            "e 1/10 i 11651/40794: loss 6.118814945220947\n",
            "e 1/10 i 11652/40794: loss 5.7400689125061035\n",
            "e 1/10 i 11653/40794: loss 5.818435192108154\n",
            "e 1/10 i 11654/40794: loss 6.077049255371094\n",
            "e 1/10 i 11655/40794: loss 6.122163772583008\n",
            "e 1/10 i 11656/40794: loss 5.953047752380371\n",
            "e 1/10 i 11657/40794: loss 5.912337303161621\n",
            "e 1/10 i 11658/40794: loss 5.882368087768555\n",
            "e 1/10 i 11659/40794: loss 5.9956889152526855\n",
            "e 1/10 i 11660/40794: loss 5.778565883636475\n",
            "e 1/10 i 11661/40794: loss 6.086193084716797\n",
            "e 1/10 i 11662/40794: loss 6.135343551635742\n",
            "e 1/10 i 11663/40794: loss 6.186619758605957\n",
            "e 1/10 i 11664/40794: loss 5.816936016082764\n",
            "e 1/10 i 11665/40794: loss 5.871832847595215\n",
            "e 1/10 i 11666/40794: loss 6.203906059265137\n",
            "e 1/10 i 11667/40794: loss 5.936476707458496\n",
            "e 1/10 i 11668/40794: loss 6.313474655151367\n",
            "e 1/10 i 11669/40794: loss 6.18598747253418\n",
            "e 1/10 i 11670/40794: loss 6.134604454040527\n",
            "e 1/10 i 11671/40794: loss 5.98200798034668\n",
            "e 1/10 i 11672/40794: loss 6.238439559936523\n",
            "e 1/10 i 11673/40794: loss 6.1527180671691895\n",
            "e 1/10 i 11674/40794: loss 5.792629241943359\n",
            "e 1/10 i 11675/40794: loss 5.982824325561523\n",
            "e 1/10 i 11676/40794: loss 6.108108997344971\n",
            "e 1/10 i 11677/40794: loss 5.8310394287109375\n",
            "e 1/10 i 11678/40794: loss 5.861599445343018\n",
            "e 1/10 i 11679/40794: loss 5.83761739730835\n",
            "e 1/10 i 11680/40794: loss 5.690517425537109\n",
            "e 1/10 i 11681/40794: loss 5.889523029327393\n",
            "e 1/10 i 11682/40794: loss 5.527180194854736\n",
            "e 1/10 i 11683/40794: loss 6.051302909851074\n",
            "e 1/10 i 11684/40794: loss 5.997766017913818\n",
            "e 1/10 i 11685/40794: loss 5.7161641120910645\n",
            "e 1/10 i 11686/40794: loss 5.885967254638672\n",
            "e 1/10 i 11687/40794: loss 5.87291145324707\n",
            "e 1/10 i 11688/40794: loss 5.764388084411621\n",
            "e 1/10 i 11689/40794: loss 5.845010757446289\n",
            "e 1/10 i 11690/40794: loss 5.856774806976318\n",
            "e 1/10 i 11691/40794: loss 5.953086853027344\n",
            "e 1/10 i 11692/40794: loss 5.8650665283203125\n",
            "e 1/10 i 11693/40794: loss 6.053240776062012\n",
            "e 1/10 i 11694/40794: loss 5.886817455291748\n",
            "e 1/10 i 11695/40794: loss 5.758211135864258\n",
            "e 1/10 i 11696/40794: loss 6.114663600921631\n",
            "e 1/10 i 11697/40794: loss 5.995162010192871\n",
            "e 1/10 i 11698/40794: loss 5.798817157745361\n",
            "e 1/10 i 11699/40794: loss 5.827295780181885\n",
            "e 1/10 i 11700/40794: loss 5.916845798492432\n",
            "e 1/10 i 11701/40794: loss 6.0971999168396\n",
            "e 1/10 i 11702/40794: loss 6.005171775817871\n",
            "e 1/10 i 11703/40794: loss 6.133246421813965\n",
            "e 1/10 i 11704/40794: loss 5.669651985168457\n",
            "e 1/10 i 11705/40794: loss 5.9295244216918945\n",
            "e 1/10 i 11706/40794: loss 5.564525127410889\n",
            "e 1/10 i 11707/40794: loss 6.054017543792725\n",
            "e 1/10 i 11708/40794: loss 5.9026103019714355\n",
            "e 1/10 i 11709/40794: loss 5.868957996368408\n",
            "e 1/10 i 11710/40794: loss 6.204981803894043\n",
            "e 1/10 i 11711/40794: loss 5.582888126373291\n",
            "e 1/10 i 11712/40794: loss 6.082385540008545\n",
            "e 1/10 i 11713/40794: loss 6.3893022537231445\n",
            "e 1/10 i 11714/40794: loss 5.911995887756348\n",
            "e 1/10 i 11715/40794: loss 5.946142673492432\n",
            "e 1/10 i 11716/40794: loss 5.521912574768066\n",
            "e 1/10 i 11717/40794: loss 5.718819618225098\n",
            "e 1/10 i 11718/40794: loss 6.004493236541748\n",
            "e 1/10 i 11719/40794: loss 6.020997047424316\n",
            "e 1/10 i 11720/40794: loss 5.865406513214111\n",
            "e 1/10 i 11721/40794: loss 5.912109851837158\n",
            "e 1/10 i 11722/40794: loss 5.72745943069458\n",
            "e 1/10 i 11723/40794: loss 5.887511730194092\n",
            "e 1/10 i 11724/40794: loss 5.986005783081055\n",
            "e 1/10 i 11725/40794: loss 5.849691867828369\n",
            "e 1/10 i 11726/40794: loss 5.555429458618164\n",
            "e 1/10 i 11727/40794: loss 6.11173152923584\n",
            "e 1/10 i 11728/40794: loss 5.919206619262695\n",
            "e 1/10 i 11729/40794: loss 5.64250373840332\n",
            "e 1/10 i 11730/40794: loss 5.6610331535339355\n",
            "e 1/10 i 11731/40794: loss 6.097397327423096\n",
            "e 1/10 i 11732/40794: loss 5.652254104614258\n",
            "e 1/10 i 11733/40794: loss 6.010543346405029\n",
            "e 1/10 i 11734/40794: loss 5.841490268707275\n",
            "e 1/10 i 11735/40794: loss 5.8202714920043945\n",
            "e 1/10 i 11736/40794: loss 5.605811595916748\n",
            "e 1/10 i 11737/40794: loss 5.7778120040893555\n",
            "e 1/10 i 11738/40794: loss 5.983238220214844\n",
            "e 1/10 i 11739/40794: loss 5.930786609649658\n",
            "e 1/10 i 11740/40794: loss 5.996241092681885\n",
            "e 1/10 i 11741/40794: loss 5.953386306762695\n",
            "e 1/10 i 11742/40794: loss 5.799859523773193\n",
            "e 1/10 i 11743/40794: loss 5.9979753494262695\n",
            "e 1/10 i 11744/40794: loss 6.090987682342529\n",
            "e 1/10 i 11745/40794: loss 5.956499099731445\n",
            "e 1/10 i 11746/40794: loss 5.972193717956543\n",
            "e 1/10 i 11747/40794: loss 5.839914321899414\n",
            "e 1/10 i 11748/40794: loss 5.747257232666016\n",
            "e 1/10 i 11749/40794: loss 5.881194114685059\n",
            "e 1/10 i 11750/40794: loss 6.008062839508057\n",
            "e 1/10 i 11751/40794: loss 6.064073085784912\n",
            "e 1/10 i 11752/40794: loss 5.983158111572266\n",
            "e 1/10 i 11753/40794: loss 6.060914993286133\n",
            "e 1/10 i 11754/40794: loss 6.082178592681885\n",
            "e 1/10 i 11755/40794: loss 6.173225402832031\n",
            "e 1/10 i 11756/40794: loss 5.9981231689453125\n",
            "e 1/10 i 11757/40794: loss 5.973213195800781\n",
            "e 1/10 i 11758/40794: loss 6.175664901733398\n",
            "e 1/10 i 11759/40794: loss 5.937923431396484\n",
            "e 1/10 i 11760/40794: loss 6.200068950653076\n",
            "e 1/10 i 11761/40794: loss 5.948154449462891\n",
            "e 1/10 i 11762/40794: loss 6.000847816467285\n",
            "e 1/10 i 11763/40794: loss 5.450431823730469\n",
            "e 1/10 i 11764/40794: loss 5.805814743041992\n",
            "e 1/10 i 11765/40794: loss 5.894283294677734\n",
            "e 1/10 i 11766/40794: loss 5.79377555847168\n",
            "e 1/10 i 11767/40794: loss 5.857114315032959\n",
            "e 1/10 i 11768/40794: loss 6.000526428222656\n",
            "e 1/10 i 11769/40794: loss 5.904566764831543\n",
            "e 1/10 i 11770/40794: loss 6.035455703735352\n",
            "e 1/10 i 11771/40794: loss 6.095063209533691\n",
            "e 1/10 i 11772/40794: loss 6.208502292633057\n",
            "e 1/10 i 11773/40794: loss 5.812548637390137\n",
            "e 1/10 i 11774/40794: loss 5.926374912261963\n",
            "e 1/10 i 11775/40794: loss 5.795747756958008\n",
            "e 1/10 i 11776/40794: loss 5.713393688201904\n",
            "e 1/10 i 11777/40794: loss 5.832321643829346\n",
            "e 1/10 i 11778/40794: loss 6.084056377410889\n",
            "e 1/10 i 11779/40794: loss 5.637396335601807\n",
            "e 1/10 i 11780/40794: loss 5.907016754150391\n",
            "e 1/10 i 11781/40794: loss 5.995844841003418\n",
            "e 1/10 i 11782/40794: loss 5.851768970489502\n",
            "e 1/10 i 11783/40794: loss 5.759469509124756\n",
            "e 1/10 i 11784/40794: loss 6.286044597625732\n",
            "e 1/10 i 11785/40794: loss 5.989538669586182\n",
            "e 1/10 i 11786/40794: loss 5.923366546630859\n",
            "e 1/10 i 11787/40794: loss 6.067160606384277\n",
            "e 1/10 i 11788/40794: loss 5.88092565536499\n",
            "e 1/10 i 11789/40794: loss 5.457797527313232\n",
            "e 1/10 i 11790/40794: loss 6.069156646728516\n",
            "e 1/10 i 11791/40794: loss 5.681555271148682\n",
            "e 1/10 i 11792/40794: loss 5.9133758544921875\n",
            "e 1/10 i 11793/40794: loss 5.576244831085205\n",
            "e 1/10 i 11794/40794: loss 6.147915840148926\n",
            "e 1/10 i 11795/40794: loss 5.898517608642578\n",
            "e 1/10 i 11796/40794: loss 5.796966552734375\n",
            "e 1/10 i 11797/40794: loss 6.007827281951904\n",
            "e 1/10 i 11798/40794: loss 5.6600117683410645\n",
            "e 1/10 i 11799/40794: loss 5.77567195892334\n",
            "e 1/10 i 11800/40794: loss 5.786813735961914\n",
            "e 1/10 i 11801/40794: loss 6.097799777984619\n",
            "e 1/10 i 11802/40794: loss 5.9716291427612305\n",
            "e 1/10 i 11803/40794: loss 5.734644412994385\n",
            "e 1/10 i 11804/40794: loss 6.077249526977539\n",
            "e 1/10 i 11805/40794: loss 6.072372913360596\n",
            "e 1/10 i 11806/40794: loss 6.299831867218018\n",
            "e 1/10 i 11807/40794: loss 6.253105163574219\n",
            "e 1/10 i 11808/40794: loss 6.173135280609131\n",
            "e 1/10 i 11809/40794: loss 5.6418867111206055\n",
            "e 1/10 i 11810/40794: loss 5.708866119384766\n",
            "e 1/10 i 11811/40794: loss 5.875185489654541\n",
            "e 1/10 i 11812/40794: loss 6.06585168838501\n",
            "e 1/10 i 11813/40794: loss 6.4161906242370605\n",
            "e 1/10 i 11814/40794: loss 5.840915203094482\n",
            "e 1/10 i 11815/40794: loss 6.09503173828125\n",
            "e 1/10 i 11816/40794: loss 5.849949836730957\n",
            "e 1/10 i 11817/40794: loss 5.766840934753418\n",
            "e 1/10 i 11818/40794: loss 5.976159572601318\n",
            "e 1/10 i 11819/40794: loss 5.773213863372803\n",
            "e 1/10 i 11820/40794: loss 5.888153553009033\n",
            "e 1/10 i 11821/40794: loss 5.823584079742432\n",
            "e 1/10 i 11822/40794: loss 6.198849678039551\n",
            "e 1/10 i 11823/40794: loss 5.540853500366211\n",
            "e 1/10 i 11824/40794: loss 5.891932487487793\n",
            "e 1/10 i 11825/40794: loss 5.904613494873047\n",
            "e 1/10 i 11826/40794: loss 5.931158065795898\n",
            "e 1/10 i 11827/40794: loss 5.705996990203857\n",
            "e 1/10 i 11828/40794: loss 6.0918779373168945\n",
            "e 1/10 i 11829/40794: loss 5.911945819854736\n",
            "e 1/10 i 11830/40794: loss 6.060732364654541\n",
            "e 1/10 i 11831/40794: loss 6.219578742980957\n",
            "e 1/10 i 11832/40794: loss 5.80549955368042\n",
            "e 1/10 i 11833/40794: loss 6.101312160491943\n",
            "e 1/10 i 11834/40794: loss 6.1189656257629395\n",
            "e 1/10 i 11835/40794: loss 6.063908100128174\n",
            "e 1/10 i 11836/40794: loss 5.851582050323486\n",
            "e 1/10 i 11837/40794: loss 6.2547607421875\n",
            "e 1/10 i 11838/40794: loss 6.139851093292236\n",
            "e 1/10 i 11839/40794: loss 5.9350762367248535\n",
            "e 1/10 i 11840/40794: loss 5.9976935386657715\n",
            "e 1/10 i 11841/40794: loss 5.868108749389648\n",
            "e 1/10 i 11842/40794: loss 6.034999847412109\n",
            "e 1/10 i 11843/40794: loss 5.8914031982421875\n",
            "e 1/10 i 11844/40794: loss 6.115981578826904\n",
            "e 1/10 i 11845/40794: loss 6.120950222015381\n",
            "e 1/10 i 11846/40794: loss 6.000764846801758\n",
            "e 1/10 i 11847/40794: loss 6.045229434967041\n",
            "e 1/10 i 11848/40794: loss 5.819088459014893\n",
            "e 1/10 i 11849/40794: loss 6.06754732131958\n",
            "e 1/10 i 11850/40794: loss 5.668966770172119\n",
            "e 1/10 i 11851/40794: loss 5.892351150512695\n",
            "e 1/10 i 11852/40794: loss 5.779815196990967\n",
            "e 1/10 i 11853/40794: loss 5.90224552154541\n",
            "e 1/10 i 11854/40794: loss 6.000889778137207\n",
            "e 1/10 i 11855/40794: loss 6.030900478363037\n",
            "e 1/10 i 11856/40794: loss 5.870024681091309\n",
            "e 1/10 i 11857/40794: loss 6.200596809387207\n",
            "e 1/10 i 11858/40794: loss 5.994814395904541\n",
            "e 1/10 i 11859/40794: loss 5.889503479003906\n",
            "e 1/10 i 11860/40794: loss 5.820611000061035\n",
            "e 1/10 i 11861/40794: loss 6.02877950668335\n",
            "e 1/10 i 11862/40794: loss 5.862519264221191\n",
            "e 1/10 i 11863/40794: loss 6.279403209686279\n",
            "e 1/10 i 11864/40794: loss 6.098412990570068\n",
            "e 1/10 i 11865/40794: loss 5.89547061920166\n",
            "e 1/10 i 11866/40794: loss 5.819240093231201\n",
            "e 1/10 i 11867/40794: loss 5.867244243621826\n",
            "e 1/10 i 11868/40794: loss 5.66502046585083\n",
            "e 1/10 i 11869/40794: loss 6.082420349121094\n",
            "e 1/10 i 11870/40794: loss 5.908038139343262\n",
            "e 1/10 i 11871/40794: loss 6.16216516494751\n",
            "e 1/10 i 11872/40794: loss 5.8374762535095215\n",
            "e 1/10 i 11873/40794: loss 6.372954368591309\n",
            "e 1/10 i 11874/40794: loss 5.882015705108643\n",
            "e 1/10 i 11875/40794: loss 5.820853233337402\n",
            "e 1/10 i 11876/40794: loss 5.900446891784668\n",
            "e 1/10 i 11877/40794: loss 6.045090198516846\n",
            "e 1/10 i 11878/40794: loss 5.949827671051025\n",
            "e 1/10 i 11879/40794: loss 6.097560882568359\n",
            "e 1/10 i 11880/40794: loss 5.970409393310547\n",
            "e 1/10 i 11881/40794: loss 6.141476154327393\n",
            "e 1/10 i 11882/40794: loss 5.801008701324463\n",
            "e 1/10 i 11883/40794: loss 5.884057521820068\n",
            "e 1/10 i 11884/40794: loss 5.87831974029541\n",
            "e 1/10 i 11885/40794: loss 5.897009372711182\n",
            "e 1/10 i 11886/40794: loss 5.712432861328125\n",
            "e 1/10 i 11887/40794: loss 5.609134674072266\n",
            "e 1/10 i 11888/40794: loss 6.222379207611084\n",
            "e 1/10 i 11889/40794: loss 5.934685230255127\n",
            "e 1/10 i 11890/40794: loss 5.754252910614014\n",
            "e 1/10 i 11891/40794: loss 6.090083599090576\n",
            "e 1/10 i 11892/40794: loss 5.8564653396606445\n",
            "e 1/10 i 11893/40794: loss 6.107846260070801\n",
            "e 1/10 i 11894/40794: loss 5.949642181396484\n",
            "e 1/10 i 11895/40794: loss 5.986599922180176\n",
            "e 1/10 i 11896/40794: loss 5.967327117919922\n",
            "e 1/10 i 11897/40794: loss 6.148736953735352\n",
            "e 1/10 i 11898/40794: loss 6.203598976135254\n",
            "e 1/10 i 11899/40794: loss 5.626698017120361\n",
            "e 1/10 i 11900/40794: loss 5.856145858764648\n",
            "e 1/10 i 11901/40794: loss 6.3757500648498535\n",
            "e 1/10 i 11902/40794: loss 5.905335426330566\n",
            "e 1/10 i 11903/40794: loss 5.581691741943359\n",
            "e 1/10 i 11904/40794: loss 5.578388214111328\n",
            "e 1/10 i 11905/40794: loss 5.95652437210083\n",
            "e 1/10 i 11906/40794: loss 5.94074821472168\n",
            "e 1/10 i 11907/40794: loss 5.799330234527588\n",
            "e 1/10 i 11908/40794: loss 5.788059234619141\n",
            "e 1/10 i 11909/40794: loss 5.625983715057373\n",
            "e 1/10 i 11910/40794: loss 6.0046796798706055\n",
            "e 1/10 i 11911/40794: loss 6.005886554718018\n",
            "e 1/10 i 11912/40794: loss 6.361422061920166\n",
            "e 1/10 i 11913/40794: loss 5.795951843261719\n",
            "e 1/10 i 11914/40794: loss 5.9539408683776855\n",
            "e 1/10 i 11915/40794: loss 6.285501003265381\n",
            "e 1/10 i 11916/40794: loss 5.928530693054199\n",
            "e 1/10 i 11917/40794: loss 5.876424312591553\n",
            "e 1/10 i 11918/40794: loss 6.0442728996276855\n",
            "e 1/10 i 11919/40794: loss 5.836850643157959\n",
            "e 1/10 i 11920/40794: loss 5.961906433105469\n",
            "e 1/10 i 11921/40794: loss 5.941036701202393\n",
            "e 1/10 i 11922/40794: loss 6.309807777404785\n",
            "e 1/10 i 11923/40794: loss 6.343342304229736\n",
            "e 1/10 i 11924/40794: loss 6.079864501953125\n",
            "e 1/10 i 11925/40794: loss 5.9570231437683105\n",
            "e 1/10 i 11926/40794: loss 5.609382152557373\n",
            "e 1/10 i 11927/40794: loss 5.693098545074463\n",
            "e 1/10 i 11928/40794: loss 5.873821258544922\n",
            "e 1/10 i 11929/40794: loss 5.982167720794678\n",
            "e 1/10 i 11930/40794: loss 5.8334760665893555\n",
            "e 1/10 i 11931/40794: loss 5.918912887573242\n",
            "e 1/10 i 11932/40794: loss 6.2146992683410645\n",
            "e 1/10 i 11933/40794: loss 6.103644371032715\n",
            "e 1/10 i 11934/40794: loss 5.788270950317383\n",
            "e 1/10 i 11935/40794: loss 5.759194374084473\n",
            "e 1/10 i 11936/40794: loss 6.181756973266602\n",
            "e 1/10 i 11937/40794: loss 5.8923187255859375\n",
            "e 1/10 i 11938/40794: loss 6.219624996185303\n",
            "e 1/10 i 11939/40794: loss 5.883858680725098\n",
            "e 1/10 i 11940/40794: loss 5.775717735290527\n",
            "e 1/10 i 11941/40794: loss 6.098668098449707\n",
            "e 1/10 i 11942/40794: loss 5.884530544281006\n",
            "e 1/10 i 11943/40794: loss 6.045645236968994\n",
            "e 1/10 i 11944/40794: loss 5.946595668792725\n",
            "e 1/10 i 11945/40794: loss 5.7759270668029785\n",
            "e 1/10 i 11946/40794: loss 5.932187557220459\n",
            "e 1/10 i 11947/40794: loss 6.105365753173828\n",
            "e 1/10 i 11948/40794: loss 5.633936405181885\n",
            "e 1/10 i 11949/40794: loss 5.799822807312012\n",
            "e 1/10 i 11950/40794: loss 6.137508392333984\n",
            "e 1/10 i 11951/40794: loss 5.760012626647949\n",
            "e 1/10 i 11952/40794: loss 5.948213577270508\n",
            "e 1/10 i 11953/40794: loss 6.213357925415039\n",
            "e 1/10 i 11954/40794: loss 6.234358310699463\n",
            "e 1/10 i 11955/40794: loss 5.990514278411865\n",
            "e 1/10 i 11956/40794: loss 6.036863803863525\n",
            "e 1/10 i 11957/40794: loss 5.769130229949951\n",
            "e 1/10 i 11958/40794: loss 5.903578281402588\n",
            "e 1/10 i 11959/40794: loss 6.106213569641113\n",
            "e 1/10 i 11960/40794: loss 5.875589370727539\n",
            "e 1/10 i 11961/40794: loss 6.100833892822266\n",
            "e 1/10 i 11962/40794: loss 5.93311882019043\n",
            "e 1/10 i 11963/40794: loss 6.002943992614746\n",
            "e 1/10 i 11964/40794: loss 6.214727401733398\n",
            "e 1/10 i 11965/40794: loss 6.126290798187256\n",
            "e 1/10 i 11966/40794: loss 5.978118419647217\n",
            "e 1/10 i 11967/40794: loss 6.142016887664795\n",
            "e 1/10 i 11968/40794: loss 5.981481075286865\n",
            "e 1/10 i 11969/40794: loss 5.993551254272461\n",
            "e 1/10 i 11970/40794: loss 5.869952201843262\n",
            "e 1/10 i 11971/40794: loss 5.769123077392578\n",
            "e 1/10 i 11972/40794: loss 5.614091396331787\n",
            "e 1/10 i 11973/40794: loss 5.95815896987915\n",
            "e 1/10 i 11974/40794: loss 5.828211307525635\n",
            "e 1/10 i 11975/40794: loss 5.82427978515625\n",
            "e 1/10 i 11976/40794: loss 5.635418891906738\n",
            "e 1/10 i 11977/40794: loss 6.008351802825928\n",
            "e 1/10 i 11978/40794: loss 6.2159318923950195\n",
            "e 1/10 i 11979/40794: loss 5.87864875793457\n",
            "e 1/10 i 11980/40794: loss 5.9455976486206055\n",
            "e 1/10 i 11981/40794: loss 5.84818696975708\n",
            "e 1/10 i 11982/40794: loss 5.966249465942383\n",
            "e 1/10 i 11983/40794: loss 5.711273670196533\n",
            "e 1/10 i 11984/40794: loss 5.979924201965332\n",
            "e 1/10 i 11985/40794: loss 5.725864410400391\n",
            "e 1/10 i 11986/40794: loss 5.9927897453308105\n",
            "e 1/10 i 11987/40794: loss 5.745885372161865\n",
            "e 1/10 i 11988/40794: loss 5.894830703735352\n",
            "e 1/10 i 11989/40794: loss 5.835138320922852\n",
            "e 1/10 i 11990/40794: loss 5.889103889465332\n",
            "e 1/10 i 11991/40794: loss 5.628876686096191\n",
            "e 1/10 i 11992/40794: loss 5.885063648223877\n",
            "e 1/10 i 11993/40794: loss 5.863888263702393\n",
            "e 1/10 i 11994/40794: loss 5.913638114929199\n",
            "e 1/10 i 11995/40794: loss 5.990049362182617\n",
            "e 1/10 i 11996/40794: loss 6.022819519042969\n",
            "e 1/10 i 11997/40794: loss 5.969803810119629\n",
            "e 1/10 i 11998/40794: loss 5.747582912445068\n",
            "e 1/10 i 11999/40794: loss 5.905877113342285\n",
            "e 1/10 i 12000/40794: loss 5.964727878570557\n",
            "e 1/10 i 12001/40794: loss 5.793731212615967\n",
            "e 1/10 i 12002/40794: loss 5.855632781982422\n",
            "e 1/10 i 12003/40794: loss 6.169768333435059\n",
            "e 1/10 i 12004/40794: loss 5.925137042999268\n",
            "e 1/10 i 12005/40794: loss 5.713838577270508\n",
            "e 1/10 i 12006/40794: loss 6.279242992401123\n",
            "e 1/10 i 12007/40794: loss 5.847908020019531\n",
            "e 1/10 i 12008/40794: loss 6.151467323303223\n",
            "e 1/10 i 12009/40794: loss 5.786921501159668\n",
            "e 1/10 i 12010/40794: loss 6.262692451477051\n",
            "e 1/10 i 12011/40794: loss 5.964679718017578\n",
            "e 1/10 i 12012/40794: loss 6.200864791870117\n",
            "e 1/10 i 12013/40794: loss 6.060117244720459\n",
            "e 1/10 i 12014/40794: loss 5.8002543449401855\n",
            "e 1/10 i 12015/40794: loss 5.924787521362305\n",
            "e 1/10 i 12016/40794: loss 5.77907657623291\n",
            "e 1/10 i 12017/40794: loss 5.805859565734863\n",
            "e 1/10 i 12018/40794: loss 6.1240081787109375\n",
            "e 1/10 i 12019/40794: loss 6.040661811828613\n",
            "e 1/10 i 12020/40794: loss 5.881612300872803\n",
            "e 1/10 i 12021/40794: loss 5.8034257888793945\n",
            "e 1/10 i 12022/40794: loss 5.893162727355957\n",
            "e 1/10 i 12023/40794: loss 5.701056957244873\n",
            "e 1/10 i 12024/40794: loss 5.7784318923950195\n",
            "e 1/10 i 12025/40794: loss 5.840979099273682\n",
            "e 1/10 i 12026/40794: loss 5.857359886169434\n",
            "e 1/10 i 12027/40794: loss 5.968974590301514\n",
            "e 1/10 i 12028/40794: loss 5.887424945831299\n",
            "e 1/10 i 12029/40794: loss 5.64045524597168\n",
            "e 1/10 i 12030/40794: loss 5.706711292266846\n",
            "e 1/10 i 12031/40794: loss 5.86706018447876\n",
            "e 1/10 i 12032/40794: loss 6.059659004211426\n",
            "e 1/10 i 12033/40794: loss 5.930965900421143\n",
            "e 1/10 i 12034/40794: loss 5.848094463348389\n",
            "e 1/10 i 12035/40794: loss 5.989017009735107\n",
            "e 1/10 i 12036/40794: loss 5.92706298828125\n",
            "e 1/10 i 12037/40794: loss 5.63810920715332\n",
            "e 1/10 i 12038/40794: loss 5.785177707672119\n",
            "e 1/10 i 12039/40794: loss 6.146138668060303\n",
            "e 1/10 i 12040/40794: loss 6.093950271606445\n",
            "e 1/10 i 12041/40794: loss 5.9908766746521\n",
            "e 1/10 i 12042/40794: loss 5.868062496185303\n",
            "e 1/10 i 12043/40794: loss 5.8186445236206055\n",
            "e 1/10 i 12044/40794: loss 5.817469596862793\n",
            "e 1/10 i 12045/40794: loss 6.205740928649902\n",
            "e 1/10 i 12046/40794: loss 6.002010345458984\n",
            "e 1/10 i 12047/40794: loss 5.728298664093018\n",
            "e 1/10 i 12048/40794: loss 5.8861165046691895\n",
            "e 1/10 i 12049/40794: loss 5.694430351257324\n",
            "e 1/10 i 12050/40794: loss 5.7001752853393555\n",
            "e 1/10 i 12051/40794: loss 5.89530611038208\n",
            "e 1/10 i 12052/40794: loss 5.799828052520752\n",
            "e 1/10 i 12053/40794: loss 6.339594841003418\n",
            "e 1/10 i 12054/40794: loss 6.258432865142822\n",
            "e 1/10 i 12055/40794: loss 6.01094913482666\n",
            "e 1/10 i 12056/40794: loss 5.942934513092041\n",
            "e 1/10 i 12057/40794: loss 5.648086071014404\n",
            "e 1/10 i 12058/40794: loss 5.880259990692139\n",
            "e 1/10 i 12059/40794: loss 6.004634380340576\n",
            "e 1/10 i 12060/40794: loss 5.96605920791626\n",
            "e 1/10 i 12061/40794: loss 5.693101406097412\n",
            "e 1/10 i 12062/40794: loss 5.367063045501709\n",
            "e 1/10 i 12063/40794: loss 6.030741214752197\n",
            "e 1/10 i 12064/40794: loss 5.585004806518555\n",
            "e 1/10 i 12065/40794: loss 6.007724761962891\n",
            "e 1/10 i 12066/40794: loss 5.901551723480225\n",
            "e 1/10 i 12067/40794: loss 6.013681888580322\n",
            "e 1/10 i 12068/40794: loss 5.917146682739258\n",
            "e 1/10 i 12069/40794: loss 5.718837261199951\n",
            "e 1/10 i 12070/40794: loss 5.8832573890686035\n",
            "e 1/10 i 12071/40794: loss 5.917476177215576\n",
            "e 1/10 i 12072/40794: loss 5.747528553009033\n",
            "e 1/10 i 12073/40794: loss 5.612986087799072\n",
            "e 1/10 i 12074/40794: loss 5.8403120040893555\n",
            "e 1/10 i 12075/40794: loss 5.970420837402344\n",
            "e 1/10 i 12076/40794: loss 6.067356586456299\n",
            "e 1/10 i 12077/40794: loss 6.002630710601807\n",
            "e 1/10 i 12078/40794: loss 5.821998119354248\n",
            "e 1/10 i 12079/40794: loss 5.983727931976318\n",
            "e 1/10 i 12080/40794: loss 5.840142726898193\n",
            "e 1/10 i 12081/40794: loss 5.705886363983154\n",
            "e 1/10 i 12082/40794: loss 5.966712474822998\n",
            "e 1/10 i 12083/40794: loss 6.119821071624756\n",
            "e 1/10 i 12084/40794: loss 6.017465114593506\n",
            "e 1/10 i 12085/40794: loss 6.163821220397949\n",
            "e 1/10 i 12086/40794: loss 5.811927318572998\n",
            "e 1/10 i 12087/40794: loss 6.340872287750244\n",
            "e 1/10 i 12088/40794: loss 6.095549583435059\n",
            "e 1/10 i 12089/40794: loss 5.8048014640808105\n",
            "e 1/10 i 12090/40794: loss 5.9612860679626465\n",
            "e 1/10 i 12091/40794: loss 6.041339874267578\n",
            "e 1/10 i 12092/40794: loss 5.990386486053467\n",
            "e 1/10 i 12093/40794: loss 5.855234146118164\n",
            "e 1/10 i 12094/40794: loss 5.67737340927124\n",
            "e 1/10 i 12095/40794: loss 5.776121139526367\n",
            "e 1/10 i 12096/40794: loss 5.784486770629883\n",
            "e 1/10 i 12097/40794: loss 5.616031646728516\n",
            "e 1/10 i 12098/40794: loss 6.250852584838867\n",
            "e 1/10 i 12099/40794: loss 5.771907329559326\n",
            "e 1/10 i 12100/40794: loss 6.082674503326416\n",
            "e 1/10 i 12101/40794: loss 5.745024681091309\n",
            "e 1/10 i 12102/40794: loss 5.610780239105225\n",
            "e 1/10 i 12103/40794: loss 5.596860885620117\n",
            "e 1/10 i 12104/40794: loss 5.662837505340576\n",
            "e 1/10 i 12105/40794: loss 5.54644250869751\n",
            "e 1/10 i 12106/40794: loss 5.997878074645996\n",
            "e 1/10 i 12107/40794: loss 5.890406608581543\n",
            "e 1/10 i 12108/40794: loss 5.87974214553833\n",
            "e 1/10 i 12109/40794: loss 6.278735160827637\n",
            "e 1/10 i 12110/40794: loss 6.070930480957031\n",
            "e 1/10 i 12111/40794: loss 5.520308017730713\n",
            "e 1/10 i 12112/40794: loss 5.902884483337402\n",
            "e 1/10 i 12113/40794: loss 5.9687652587890625\n",
            "e 1/10 i 12114/40794: loss 5.850532054901123\n",
            "e 1/10 i 12115/40794: loss 5.962972640991211\n",
            "e 1/10 i 12116/40794: loss 5.581241130828857\n",
            "e 1/10 i 12117/40794: loss 5.820310592651367\n",
            "e 1/10 i 12118/40794: loss 6.057940483093262\n",
            "e 1/10 i 12119/40794: loss 6.119381904602051\n",
            "e 1/10 i 12120/40794: loss 5.724194526672363\n",
            "e 1/10 i 12121/40794: loss 5.834072589874268\n",
            "e 1/10 i 12122/40794: loss 5.830382347106934\n",
            "e 1/10 i 12123/40794: loss 5.757580280303955\n",
            "e 1/10 i 12124/40794: loss 5.7964324951171875\n",
            "e 1/10 i 12125/40794: loss 5.796693325042725\n",
            "e 1/10 i 12126/40794: loss 5.715737819671631\n",
            "e 1/10 i 12127/40794: loss 6.13807487487793\n",
            "e 1/10 i 12128/40794: loss 6.2653374671936035\n",
            "e 1/10 i 12129/40794: loss 6.447778701782227\n",
            "e 1/10 i 12130/40794: loss 5.903354167938232\n",
            "e 1/10 i 12131/40794: loss 5.968096733093262\n",
            "e 1/10 i 12132/40794: loss 5.938495635986328\n",
            "e 1/10 i 12133/40794: loss 6.188515663146973\n",
            "e 1/10 i 12134/40794: loss 5.803860187530518\n",
            "e 1/10 i 12135/40794: loss 6.154666423797607\n",
            "e 1/10 i 12136/40794: loss 5.601412773132324\n",
            "e 1/10 i 12137/40794: loss 5.924360275268555\n",
            "e 1/10 i 12138/40794: loss 5.74068546295166\n",
            "e 1/10 i 12139/40794: loss 5.740721225738525\n",
            "e 1/10 i 12140/40794: loss 5.834734916687012\n",
            "e 1/10 i 12141/40794: loss 6.0375752449035645\n",
            "e 1/10 i 12142/40794: loss 5.566948413848877\n",
            "e 1/10 i 12143/40794: loss 6.293978214263916\n",
            "e 1/10 i 12144/40794: loss 5.74054479598999\n",
            "e 1/10 i 12145/40794: loss 6.300766468048096\n",
            "e 1/10 i 12146/40794: loss 5.773703098297119\n",
            "e 1/10 i 12147/40794: loss 5.963996410369873\n",
            "e 1/10 i 12148/40794: loss 5.833611965179443\n",
            "e 1/10 i 12149/40794: loss 5.940777778625488\n",
            "e 1/10 i 12150/40794: loss 5.960765838623047\n",
            "e 1/10 i 12151/40794: loss 5.979534149169922\n",
            "e 1/10 i 12152/40794: loss 6.186141014099121\n",
            "e 1/10 i 12153/40794: loss 5.843794345855713\n",
            "e 1/10 i 12154/40794: loss 6.137472629547119\n",
            "e 1/10 i 12155/40794: loss 6.0338544845581055\n",
            "e 1/10 i 12156/40794: loss 5.786613464355469\n",
            "e 1/10 i 12157/40794: loss 6.042797088623047\n",
            "e 1/10 i 12158/40794: loss 5.99675989151001\n",
            "e 1/10 i 12159/40794: loss 6.185327529907227\n",
            "e 1/10 i 12160/40794: loss 5.913366794586182\n",
            "e 1/10 i 12161/40794: loss 5.842568874359131\n",
            "e 1/10 i 12162/40794: loss 6.092038631439209\n",
            "e 1/10 i 12163/40794: loss 5.632773399353027\n",
            "e 1/10 i 12164/40794: loss 6.008176803588867\n",
            "e 1/10 i 12165/40794: loss 5.98072624206543\n",
            "e 1/10 i 12166/40794: loss 5.7596611976623535\n",
            "e 1/10 i 12167/40794: loss 6.026930809020996\n",
            "e 1/10 i 12168/40794: loss 5.961492538452148\n",
            "e 1/10 i 12169/40794: loss 5.654509544372559\n",
            "e 1/10 i 12170/40794: loss 6.094574928283691\n",
            "e 1/10 i 12171/40794: loss 5.96029806137085\n",
            "e 1/10 i 12172/40794: loss 6.127042293548584\n",
            "e 1/10 i 12173/40794: loss 5.672628402709961\n",
            "e 1/10 i 12174/40794: loss 5.814757823944092\n",
            "e 1/10 i 12175/40794: loss 6.200178146362305\n",
            "e 1/10 i 12176/40794: loss 5.915624618530273\n",
            "e 1/10 i 12177/40794: loss 5.595860004425049\n",
            "e 1/10 i 12178/40794: loss 5.914979457855225\n",
            "e 1/10 i 12179/40794: loss 6.137595176696777\n",
            "e 1/10 i 12180/40794: loss 6.038149833679199\n",
            "e 1/10 i 12181/40794: loss 6.08342981338501\n",
            "e 1/10 i 12182/40794: loss 6.168166637420654\n",
            "e 1/10 i 12183/40794: loss 5.93916130065918\n",
            "e 1/10 i 12184/40794: loss 6.157685279846191\n",
            "e 1/10 i 12185/40794: loss 6.287010669708252\n",
            "e 1/10 i 12186/40794: loss 5.884714126586914\n",
            "e 1/10 i 12187/40794: loss 6.026449203491211\n",
            "e 1/10 i 12188/40794: loss 5.700814723968506\n",
            "e 1/10 i 12189/40794: loss 6.017916202545166\n",
            "e 1/10 i 12190/40794: loss 5.790456771850586\n",
            "e 1/10 i 12191/40794: loss 6.33453893661499\n",
            "e 1/10 i 12192/40794: loss 6.1012444496154785\n",
            "e 1/10 i 12193/40794: loss 6.102220058441162\n",
            "e 1/10 i 12194/40794: loss 5.786906719207764\n",
            "e 1/10 i 12195/40794: loss 5.751524448394775\n",
            "e 1/10 i 12196/40794: loss 5.839805603027344\n",
            "e 1/10 i 12197/40794: loss 5.84966516494751\n",
            "e 1/10 i 12198/40794: loss 6.113650321960449\n",
            "e 1/10 i 12199/40794: loss 6.012345314025879\n",
            "e 1/10 i 12200/40794: loss 5.89969539642334\n",
            "e 1/10 i 12201/40794: loss 6.117824554443359\n",
            "e 1/10 i 12202/40794: loss 5.8874287605285645\n",
            "e 1/10 i 12203/40794: loss 5.926980972290039\n",
            "e 1/10 i 12204/40794: loss 5.931493282318115\n",
            "e 1/10 i 12205/40794: loss 5.573345184326172\n",
            "e 1/10 i 12206/40794: loss 5.944767475128174\n",
            "e 1/10 i 12207/40794: loss 5.784139633178711\n",
            "e 1/10 i 12208/40794: loss 6.0257248878479\n",
            "e 1/10 i 12209/40794: loss 5.7519450187683105\n",
            "e 1/10 i 12210/40794: loss 5.898634910583496\n",
            "e 1/10 i 12211/40794: loss 5.84788703918457\n",
            "e 1/10 i 12212/40794: loss 5.895872592926025\n",
            "e 1/10 i 12213/40794: loss 5.9469828605651855\n",
            "e 1/10 i 12214/40794: loss 6.022068977355957\n",
            "e 1/10 i 12215/40794: loss 5.632307052612305\n",
            "e 1/10 i 12216/40794: loss 5.532196044921875\n",
            "e 1/10 i 12217/40794: loss 5.927265167236328\n",
            "e 1/10 i 12218/40794: loss 6.203145503997803\n",
            "e 1/10 i 12219/40794: loss 6.1032257080078125\n",
            "e 1/10 i 12220/40794: loss 5.791990280151367\n",
            "e 1/10 i 12221/40794: loss 5.7968854904174805\n",
            "e 1/10 i 12222/40794: loss 5.898051738739014\n",
            "e 1/10 i 12223/40794: loss 5.917815208435059\n",
            "e 1/10 i 12224/40794: loss 5.972731113433838\n",
            "e 1/10 i 12225/40794: loss 5.581521511077881\n",
            "e 1/10 i 12226/40794: loss 6.0115132331848145\n",
            "e 1/10 i 12227/40794: loss 6.374811172485352\n",
            "e 1/10 i 12228/40794: loss 5.751204967498779\n",
            "e 1/10 i 12229/40794: loss 5.793142795562744\n",
            "e 1/10 i 12230/40794: loss 5.809561252593994\n",
            "e 1/10 i 12231/40794: loss 6.0915374755859375\n",
            "e 1/10 i 12232/40794: loss 6.376115322113037\n",
            "e 1/10 i 12233/40794: loss 6.063769340515137\n",
            "e 1/10 i 12234/40794: loss 5.728107929229736\n",
            "e 1/10 i 12235/40794: loss 6.005331993103027\n",
            "e 1/10 i 12236/40794: loss 5.6616692543029785\n",
            "e 1/10 i 12237/40794: loss 5.729329586029053\n",
            "e 1/10 i 12238/40794: loss 6.191911697387695\n",
            "e 1/10 i 12239/40794: loss 6.145671367645264\n",
            "e 1/10 i 12240/40794: loss 5.737069129943848\n",
            "e 1/10 i 12241/40794: loss 5.957234859466553\n",
            "e 1/10 i 12242/40794: loss 5.8360137939453125\n",
            "e 1/10 i 12243/40794: loss 6.089930534362793\n",
            "e 1/10 i 12244/40794: loss 5.538324356079102\n",
            "e 1/10 i 12245/40794: loss 6.083948135375977\n",
            "e 1/10 i 12246/40794: loss 5.621495246887207\n",
            "e 1/10 i 12247/40794: loss 5.782368183135986\n",
            "e 1/10 i 12248/40794: loss 6.2014336585998535\n",
            "e 1/10 i 12249/40794: loss 5.749088764190674\n",
            "e 1/10 i 12250/40794: loss 6.182733058929443\n",
            "e 1/10 i 12251/40794: loss 5.8432745933532715\n",
            "e 1/10 i 12252/40794: loss 5.792448997497559\n",
            "e 1/10 i 12253/40794: loss 6.0204925537109375\n",
            "e 1/10 i 12254/40794: loss 5.976958751678467\n",
            "e 1/10 i 12255/40794: loss 5.8909525871276855\n",
            "e 1/10 i 12256/40794: loss 5.939283847808838\n",
            "e 1/10 i 12257/40794: loss 6.084120750427246\n",
            "e 1/10 i 12258/40794: loss 5.944563388824463\n",
            "e 1/10 i 12259/40794: loss 6.34411096572876\n",
            "e 1/10 i 12260/40794: loss 6.003112316131592\n",
            "e 1/10 i 12261/40794: loss 5.832193374633789\n",
            "e 1/10 i 12262/40794: loss 5.868173122406006\n",
            "e 1/10 i 12263/40794: loss 5.8832807540893555\n",
            "e 1/10 i 12264/40794: loss 5.873874664306641\n",
            "e 1/10 i 12265/40794: loss 5.869014263153076\n",
            "e 1/10 i 12266/40794: loss 5.704426288604736\n",
            "e 1/10 i 12267/40794: loss 6.170285701751709\n",
            "e 1/10 i 12268/40794: loss 5.882248401641846\n",
            "e 1/10 i 12269/40794: loss 5.945544242858887\n",
            "e 1/10 i 12270/40794: loss 5.848803520202637\n",
            "e 1/10 i 12271/40794: loss 6.118471622467041\n",
            "e 1/10 i 12272/40794: loss 6.158267974853516\n",
            "e 1/10 i 12273/40794: loss 5.804686546325684\n",
            "e 1/10 i 12274/40794: loss 6.21664571762085\n",
            "e 1/10 i 12275/40794: loss 6.088606834411621\n",
            "e 1/10 i 12276/40794: loss 5.695639133453369\n",
            "e 1/10 i 12277/40794: loss 5.989382743835449\n",
            "e 1/10 i 12278/40794: loss 5.867806911468506\n",
            "e 1/10 i 12279/40794: loss 5.9969892501831055\n",
            "e 1/10 i 12280/40794: loss 6.128161907196045\n",
            "e 1/10 i 12281/40794: loss 5.71944522857666\n",
            "e 1/10 i 12282/40794: loss 6.3334059715271\n",
            "e 1/10 i 12283/40794: loss 5.992351055145264\n",
            "e 1/10 i 12284/40794: loss 5.752309799194336\n",
            "e 1/10 i 12285/40794: loss 6.042030334472656\n",
            "e 1/10 i 12286/40794: loss 5.753274440765381\n",
            "e 1/10 i 12287/40794: loss 5.847361087799072\n",
            "e 1/10 i 12288/40794: loss 5.963782787322998\n",
            "e 1/10 i 12289/40794: loss 6.014367580413818\n",
            "e 1/10 i 12290/40794: loss 6.167899131774902\n",
            "e 1/10 i 12291/40794: loss 5.8039422035217285\n",
            "e 1/10 i 12292/40794: loss 5.757279396057129\n",
            "e 1/10 i 12293/40794: loss 6.11229944229126\n",
            "e 1/10 i 12294/40794: loss 6.082472324371338\n",
            "e 1/10 i 12295/40794: loss 5.880012035369873\n",
            "e 1/10 i 12296/40794: loss 5.802576541900635\n",
            "e 1/10 i 12297/40794: loss 5.976410865783691\n",
            "e 1/10 i 12298/40794: loss 5.920400142669678\n",
            "e 1/10 i 12299/40794: loss 5.736043453216553\n",
            "e 1/10 i 12300/40794: loss 6.142569541931152\n",
            "e 1/10 i 12301/40794: loss 5.898171424865723\n",
            "e 1/10 i 12302/40794: loss 6.065537452697754\n",
            "e 1/10 i 12303/40794: loss 5.876017093658447\n",
            "e 1/10 i 12304/40794: loss 6.1178412437438965\n",
            "e 1/10 i 12305/40794: loss 5.908911228179932\n",
            "e 1/10 i 12306/40794: loss 6.243039131164551\n",
            "e 1/10 i 12307/40794: loss 5.589881420135498\n",
            "e 1/10 i 12308/40794: loss 5.786794185638428\n",
            "e 1/10 i 12309/40794: loss 5.857682228088379\n",
            "e 1/10 i 12310/40794: loss 5.910537242889404\n",
            "e 1/10 i 12311/40794: loss 6.016594409942627\n",
            "e 1/10 i 12312/40794: loss 5.952298164367676\n",
            "e 1/10 i 12313/40794: loss 6.189266204833984\n",
            "e 1/10 i 12314/40794: loss 5.797111511230469\n",
            "e 1/10 i 12315/40794: loss 5.676225185394287\n",
            "e 1/10 i 12316/40794: loss 6.230532646179199\n",
            "e 1/10 i 12317/40794: loss 5.962316513061523\n",
            "e 1/10 i 12318/40794: loss 5.944896697998047\n",
            "e 1/10 i 12319/40794: loss 5.898561000823975\n",
            "e 1/10 i 12320/40794: loss 5.639461040496826\n",
            "e 1/10 i 12321/40794: loss 5.922731876373291\n",
            "e 1/10 i 12322/40794: loss 5.954267501831055\n",
            "e 1/10 i 12323/40794: loss 6.136154651641846\n",
            "e 1/10 i 12324/40794: loss 5.784457206726074\n",
            "e 1/10 i 12325/40794: loss 5.925182342529297\n",
            "e 1/10 i 12326/40794: loss 5.967154026031494\n",
            "e 1/10 i 12327/40794: loss 5.823612213134766\n",
            "e 1/10 i 12328/40794: loss 6.030702590942383\n",
            "e 1/10 i 12329/40794: loss 5.890021324157715\n",
            "e 1/10 i 12330/40794: loss 5.738940238952637\n",
            "e 1/10 i 12331/40794: loss 5.704207897186279\n",
            "e 1/10 i 12332/40794: loss 6.163861274719238\n",
            "e 1/10 i 12333/40794: loss 6.015014171600342\n",
            "e 1/10 i 12334/40794: loss 5.91603422164917\n",
            "e 1/10 i 12335/40794: loss 6.1454033851623535\n",
            "e 1/10 i 12336/40794: loss 5.645872116088867\n",
            "e 1/10 i 12337/40794: loss 6.161505222320557\n",
            "e 1/10 i 12338/40794: loss 6.00907039642334\n",
            "e 1/10 i 12339/40794: loss 6.119815826416016\n",
            "e 1/10 i 12340/40794: loss 5.80940055847168\n",
            "e 1/10 i 12341/40794: loss 6.160257339477539\n",
            "e 1/10 i 12342/40794: loss 5.86973762512207\n",
            "e 1/10 i 12343/40794: loss 5.761471271514893\n",
            "e 1/10 i 12344/40794: loss 6.136046409606934\n",
            "e 1/10 i 12345/40794: loss 6.072871208190918\n",
            "e 1/10 i 12346/40794: loss 5.5867018699646\n",
            "e 1/10 i 12347/40794: loss 5.857030391693115\n",
            "e 1/10 i 12348/40794: loss 6.175681114196777\n",
            "e 1/10 i 12349/40794: loss 5.84111213684082\n",
            "e 1/10 i 12350/40794: loss 6.275886058807373\n",
            "e 1/10 i 12351/40794: loss 5.821399211883545\n",
            "e 1/10 i 12352/40794: loss 5.716075420379639\n",
            "e 1/10 i 12353/40794: loss 5.933716773986816\n",
            "e 1/10 i 12354/40794: loss 5.8567681312561035\n",
            "e 1/10 i 12355/40794: loss 6.246728897094727\n",
            "e 1/10 i 12356/40794: loss 6.094977378845215\n",
            "e 1/10 i 12357/40794: loss 5.9070515632629395\n",
            "e 1/10 i 12358/40794: loss 5.981009483337402\n",
            "e 1/10 i 12359/40794: loss 6.018503189086914\n",
            "e 1/10 i 12360/40794: loss 5.9083380699157715\n",
            "e 1/10 i 12361/40794: loss 5.946597576141357\n",
            "e 1/10 i 12362/40794: loss 5.947729587554932\n",
            "e 1/10 i 12363/40794: loss 5.925987720489502\n",
            "e 1/10 i 12364/40794: loss 6.068719387054443\n",
            "e 1/10 i 12365/40794: loss 5.935343265533447\n",
            "e 1/10 i 12366/40794: loss 5.9967780113220215\n",
            "e 1/10 i 12367/40794: loss 5.768978118896484\n",
            "e 1/10 i 12368/40794: loss 5.77968168258667\n",
            "e 1/10 i 12369/40794: loss 5.958430767059326\n",
            "e 1/10 i 12370/40794: loss 5.697347640991211\n",
            "e 1/10 i 12371/40794: loss 5.744085311889648\n",
            "e 1/10 i 12372/40794: loss 5.7078351974487305\n",
            "e 1/10 i 12373/40794: loss 5.898745536804199\n",
            "e 1/10 i 12374/40794: loss 5.990037441253662\n",
            "e 1/10 i 12375/40794: loss 5.740786075592041\n",
            "e 1/10 i 12376/40794: loss 5.846479892730713\n",
            "e 1/10 i 12377/40794: loss 6.058948993682861\n",
            "e 1/10 i 12378/40794: loss 6.040375232696533\n",
            "e 1/10 i 12379/40794: loss 6.259674549102783\n",
            "e 1/10 i 12380/40794: loss 5.981026649475098\n",
            "e 1/10 i 12381/40794: loss 5.7214250564575195\n",
            "e 1/10 i 12382/40794: loss 5.6882123947143555\n",
            "e 1/10 i 12383/40794: loss 6.2862701416015625\n",
            "e 1/10 i 12384/40794: loss 5.883157253265381\n",
            "e 1/10 i 12385/40794: loss 6.240665912628174\n",
            "e 1/10 i 12386/40794: loss 6.16379451751709\n",
            "e 1/10 i 12387/40794: loss 6.118968963623047\n",
            "e 1/10 i 12388/40794: loss 5.878509998321533\n",
            "e 1/10 i 12389/40794: loss 5.604827880859375\n",
            "e 1/10 i 12390/40794: loss 5.882375717163086\n",
            "e 1/10 i 12391/40794: loss 5.722987174987793\n",
            "e 1/10 i 12392/40794: loss 5.606790065765381\n",
            "e 1/10 i 12393/40794: loss 6.296337604522705\n",
            "e 1/10 i 12394/40794: loss 5.725032806396484\n",
            "e 1/10 i 12395/40794: loss 6.027830600738525\n",
            "e 1/10 i 12396/40794: loss 5.610511779785156\n",
            "e 1/10 i 12397/40794: loss 5.785610675811768\n",
            "e 1/10 i 12398/40794: loss 5.517467975616455\n",
            "e 1/10 i 12399/40794: loss 5.621951580047607\n",
            "e 1/10 i 12400/40794: loss 5.917261600494385\n",
            "e 1/10 i 12401/40794: loss 5.907772541046143\n",
            "e 1/10 i 12402/40794: loss 5.961223602294922\n",
            "e 1/10 i 12403/40794: loss 5.869690418243408\n",
            "e 1/10 i 12404/40794: loss 5.737937927246094\n",
            "e 1/10 i 12405/40794: loss 5.782660484313965\n",
            "e 1/10 i 12406/40794: loss 5.92426061630249\n",
            "e 1/10 i 12407/40794: loss 5.7038679122924805\n",
            "e 1/10 i 12408/40794: loss 5.812990665435791\n",
            "e 1/10 i 12409/40794: loss 5.885733127593994\n",
            "e 1/10 i 12410/40794: loss 5.954063415527344\n",
            "e 1/10 i 12411/40794: loss 5.563750267028809\n",
            "e 1/10 i 12412/40794: loss 5.950957298278809\n",
            "e 1/10 i 12413/40794: loss 6.306215286254883\n",
            "e 1/10 i 12414/40794: loss 5.9559454917907715\n",
            "e 1/10 i 12415/40794: loss 6.0966267585754395\n",
            "e 1/10 i 12416/40794: loss 5.994540691375732\n",
            "e 1/10 i 12417/40794: loss 5.392457485198975\n",
            "e 1/10 i 12418/40794: loss 5.8183393478393555\n",
            "e 1/10 i 12419/40794: loss 5.910808086395264\n",
            "e 1/10 i 12420/40794: loss 5.941474914550781\n",
            "e 1/10 i 12421/40794: loss 6.175920009613037\n",
            "e 1/10 i 12422/40794: loss 5.914968490600586\n",
            "e 1/10 i 12423/40794: loss 5.543208599090576\n",
            "e 1/10 i 12424/40794: loss 5.699012279510498\n",
            "e 1/10 i 12425/40794: loss 5.790661334991455\n",
            "e 1/10 i 12426/40794: loss 5.752190589904785\n",
            "e 1/10 i 12427/40794: loss 6.052134037017822\n",
            "e 1/10 i 12428/40794: loss 5.700326442718506\n",
            "e 1/10 i 12429/40794: loss 6.0730204582214355\n",
            "e 1/10 i 12430/40794: loss 5.956843852996826\n",
            "e 1/10 i 12431/40794: loss 6.0803608894348145\n",
            "e 1/10 i 12432/40794: loss 5.8213653564453125\n",
            "e 1/10 i 12433/40794: loss 6.340025424957275\n",
            "e 1/10 i 12434/40794: loss 5.713623046875\n",
            "e 1/10 i 12435/40794: loss 6.156054973602295\n",
            "e 1/10 i 12436/40794: loss 5.769911289215088\n",
            "e 1/10 i 12437/40794: loss 5.87957239151001\n",
            "e 1/10 i 12438/40794: loss 6.0179572105407715\n",
            "e 1/10 i 12439/40794: loss 5.689608573913574\n",
            "e 1/10 i 12440/40794: loss 6.001645565032959\n",
            "e 1/10 i 12441/40794: loss 5.816659450531006\n",
            "e 1/10 i 12442/40794: loss 5.997430324554443\n",
            "e 1/10 i 12443/40794: loss 5.680480003356934\n",
            "e 1/10 i 12444/40794: loss 5.793069362640381\n",
            "e 1/10 i 12445/40794: loss 5.999177932739258\n",
            "e 1/10 i 12446/40794: loss 5.727496147155762\n",
            "e 1/10 i 12447/40794: loss 5.930000305175781\n",
            "e 1/10 i 12448/40794: loss 6.0127129554748535\n",
            "e 1/10 i 12449/40794: loss 6.2161173820495605\n",
            "e 1/10 i 12450/40794: loss 5.931965351104736\n",
            "e 1/10 i 12451/40794: loss 6.1669020652771\n",
            "e 1/10 i 12452/40794: loss 6.115744590759277\n",
            "e 1/10 i 12453/40794: loss 6.249091625213623\n",
            "e 1/10 i 12454/40794: loss 5.989686965942383\n",
            "e 1/10 i 12455/40794: loss 5.670658588409424\n",
            "e 1/10 i 12456/40794: loss 5.7537617683410645\n",
            "e 1/10 i 12457/40794: loss 6.248823642730713\n",
            "e 1/10 i 12458/40794: loss 5.786710262298584\n",
            "e 1/10 i 12459/40794: loss 6.039379596710205\n",
            "e 1/10 i 12460/40794: loss 5.867785930633545\n",
            "e 1/10 i 12461/40794: loss 5.9640936851501465\n",
            "e 1/10 i 12462/40794: loss 5.843258380889893\n",
            "e 1/10 i 12463/40794: loss 5.7630462646484375\n",
            "e 1/10 i 12464/40794: loss 6.024643898010254\n",
            "e 1/10 i 12465/40794: loss 5.804243564605713\n",
            "e 1/10 i 12466/40794: loss 5.955887317657471\n",
            "e 1/10 i 12467/40794: loss 5.916370868682861\n",
            "e 1/10 i 12468/40794: loss 5.791137218475342\n",
            "e 1/10 i 12469/40794: loss 5.885084629058838\n",
            "e 1/10 i 12470/40794: loss 6.072664260864258\n",
            "e 1/10 i 12471/40794: loss 6.139851093292236\n",
            "e 1/10 i 12472/40794: loss 5.851998805999756\n",
            "e 1/10 i 12473/40794: loss 5.54575777053833\n",
            "e 1/10 i 12474/40794: loss 6.1002984046936035\n",
            "e 1/10 i 12475/40794: loss 5.774169921875\n",
            "e 1/10 i 12476/40794: loss 5.677095890045166\n",
            "e 1/10 i 12477/40794: loss 5.800597667694092\n",
            "e 1/10 i 12478/40794: loss 6.167487144470215\n",
            "e 1/10 i 12479/40794: loss 5.985130310058594\n",
            "e 1/10 i 12480/40794: loss 6.01663064956665\n",
            "e 1/10 i 12481/40794: loss 5.925448894500732\n",
            "e 1/10 i 12482/40794: loss 5.752341270446777\n",
            "e 1/10 i 12483/40794: loss 5.8612542152404785\n",
            "e 1/10 i 12484/40794: loss 6.017512321472168\n",
            "e 1/10 i 12485/40794: loss 6.1355814933776855\n",
            "e 1/10 i 12486/40794: loss 5.868022441864014\n",
            "e 1/10 i 12487/40794: loss 5.797572612762451\n",
            "e 1/10 i 12488/40794: loss 5.752633571624756\n",
            "e 1/10 i 12489/40794: loss 5.961029052734375\n",
            "e 1/10 i 12490/40794: loss 5.874709129333496\n",
            "e 1/10 i 12491/40794: loss 5.813798904418945\n",
            "e 1/10 i 12492/40794: loss 5.806850433349609\n",
            "e 1/10 i 12493/40794: loss 5.734742164611816\n",
            "e 1/10 i 12494/40794: loss 5.866662502288818\n",
            "e 1/10 i 12495/40794: loss 5.936455726623535\n",
            "e 1/10 i 12496/40794: loss 5.944340705871582\n",
            "e 1/10 i 12497/40794: loss 5.945204734802246\n",
            "e 1/10 i 12498/40794: loss 5.99104118347168\n",
            "e 1/10 i 12499/40794: loss 6.151618003845215\n",
            "e 1/10 i 12500/40794: loss 6.135608673095703\n",
            "e 1/10 i 12501/40794: loss 5.94579553604126\n",
            "e 1/10 i 12502/40794: loss 5.913179397583008\n",
            "e 1/10 i 12503/40794: loss 5.988286018371582\n",
            "e 1/10 i 12504/40794: loss 5.904928684234619\n",
            "e 1/10 i 12505/40794: loss 6.167105197906494\n",
            "e 1/10 i 12506/40794: loss 5.950750827789307\n",
            "e 1/10 i 12507/40794: loss 5.997664928436279\n",
            "e 1/10 i 12508/40794: loss 5.824463844299316\n",
            "e 1/10 i 12509/40794: loss 5.833134174346924\n",
            "e 1/10 i 12510/40794: loss 5.728883266448975\n",
            "e 1/10 i 12511/40794: loss 5.588222980499268\n",
            "e 1/10 i 12512/40794: loss 5.683304786682129\n",
            "e 1/10 i 12513/40794: loss 6.030951976776123\n",
            "e 1/10 i 12514/40794: loss 5.813342094421387\n",
            "e 1/10 i 12515/40794: loss 6.116844177246094\n",
            "e 1/10 i 12516/40794: loss 5.7998528480529785\n",
            "e 1/10 i 12517/40794: loss 5.91678524017334\n",
            "e 1/10 i 12518/40794: loss 5.828362464904785\n",
            "e 1/10 i 12519/40794: loss 5.660602569580078\n",
            "e 1/10 i 12520/40794: loss 5.793949604034424\n",
            "e 1/10 i 12521/40794: loss 6.020124912261963\n",
            "e 1/10 i 12522/40794: loss 5.6390767097473145\n",
            "e 1/10 i 12523/40794: loss 5.8628058433532715\n",
            "e 1/10 i 12524/40794: loss 5.90504264831543\n",
            "e 1/10 i 12525/40794: loss 5.756324291229248\n",
            "e 1/10 i 12526/40794: loss 5.7959675788879395\n",
            "e 1/10 i 12527/40794: loss 5.905953407287598\n",
            "e 1/10 i 12528/40794: loss 5.947017192840576\n",
            "e 1/10 i 12529/40794: loss 5.842966079711914\n",
            "e 1/10 i 12530/40794: loss 6.149194240570068\n",
            "e 1/10 i 12531/40794: loss 6.064002990722656\n",
            "e 1/10 i 12532/40794: loss 5.812563896179199\n",
            "e 1/10 i 12533/40794: loss 5.965106964111328\n",
            "e 1/10 i 12534/40794: loss 5.610506057739258\n",
            "e 1/10 i 12535/40794: loss 5.706872463226318\n",
            "e 1/10 i 12536/40794: loss 5.996007919311523\n",
            "e 1/10 i 12537/40794: loss 5.9111647605896\n",
            "e 1/10 i 12538/40794: loss 5.948794841766357\n",
            "e 1/10 i 12539/40794: loss 5.906121730804443\n",
            "e 1/10 i 12540/40794: loss 6.076215744018555\n",
            "e 1/10 i 12541/40794: loss 6.039905071258545\n",
            "e 1/10 i 12542/40794: loss 5.7499237060546875\n",
            "e 1/10 i 12543/40794: loss 5.811993598937988\n",
            "e 1/10 i 12544/40794: loss 5.930180549621582\n",
            "e 1/10 i 12545/40794: loss 5.610546112060547\n",
            "e 1/10 i 12546/40794: loss 5.931426525115967\n",
            "e 1/10 i 12547/40794: loss 5.756831169128418\n",
            "e 1/10 i 12548/40794: loss 5.640048027038574\n",
            "e 1/10 i 12549/40794: loss 6.045111656188965\n",
            "e 1/10 i 12550/40794: loss 6.079115867614746\n",
            "e 1/10 i 12551/40794: loss 5.98590087890625\n",
            "e 1/10 i 12552/40794: loss 5.548928260803223\n",
            "e 1/10 i 12553/40794: loss 6.079642295837402\n",
            "e 1/10 i 12554/40794: loss 6.040950775146484\n",
            "e 1/10 i 12555/40794: loss 5.784390926361084\n",
            "e 1/10 i 12556/40794: loss 6.054743766784668\n",
            "e 1/10 i 12557/40794: loss 6.017096996307373\n",
            "e 1/10 i 12558/40794: loss 6.032919406890869\n",
            "e 1/10 i 12559/40794: loss 5.721535682678223\n",
            "e 1/10 i 12560/40794: loss 5.963616371154785\n",
            "e 1/10 i 12561/40794: loss 5.862252235412598\n",
            "e 1/10 i 12562/40794: loss 5.727922439575195\n",
            "e 1/10 i 12563/40794: loss 5.857910633087158\n",
            "e 1/10 i 12564/40794: loss 5.960496425628662\n",
            "e 1/10 i 12565/40794: loss 6.05505895614624\n",
            "e 1/10 i 12566/40794: loss 5.7357096672058105\n",
            "e 1/10 i 12567/40794: loss 5.773724555969238\n",
            "e 1/10 i 12568/40794: loss 5.896845817565918\n",
            "e 1/10 i 12569/40794: loss 5.6100029945373535\n",
            "e 1/10 i 12570/40794: loss 6.078287601470947\n",
            "e 1/10 i 12571/40794: loss 5.913048267364502\n",
            "e 1/10 i 12572/40794: loss 5.932199478149414\n",
            "e 1/10 i 12573/40794: loss 5.675891876220703\n",
            "e 1/10 i 12574/40794: loss 5.726140022277832\n",
            "e 1/10 i 12575/40794: loss 5.89588737487793\n",
            "e 1/10 i 12576/40794: loss 5.632144927978516\n",
            "e 1/10 i 12577/40794: loss 5.957677364349365\n",
            "e 1/10 i 12578/40794: loss 6.199400901794434\n",
            "e 1/10 i 12579/40794: loss 5.9613447189331055\n",
            "e 1/10 i 12580/40794: loss 5.56583833694458\n",
            "e 1/10 i 12581/40794: loss 5.7428998947143555\n",
            "e 1/10 i 12582/40794: loss 6.06720495223999\n",
            "e 1/10 i 12583/40794: loss 5.5729660987854\n",
            "e 1/10 i 12584/40794: loss 6.041904449462891\n",
            "e 1/10 i 12585/40794: loss 5.472134113311768\n",
            "e 1/10 i 12586/40794: loss 5.769519329071045\n",
            "e 1/10 i 12587/40794: loss 5.804257869720459\n",
            "e 1/10 i 12588/40794: loss 5.883895397186279\n",
            "e 1/10 i 12589/40794: loss 5.862429141998291\n",
            "e 1/10 i 12590/40794: loss 5.835651397705078\n",
            "e 1/10 i 12591/40794: loss 6.012065410614014\n",
            "e 1/10 i 12592/40794: loss 6.0270771980285645\n",
            "e 1/10 i 12593/40794: loss 6.1846842765808105\n",
            "e 1/10 i 12594/40794: loss 5.999581813812256\n",
            "e 1/10 i 12595/40794: loss 5.373397350311279\n",
            "e 1/10 i 12596/40794: loss 5.903979778289795\n",
            "e 1/10 i 12597/40794: loss 5.984421730041504\n",
            "e 1/10 i 12598/40794: loss 5.935905933380127\n",
            "e 1/10 i 12599/40794: loss 6.157341957092285\n",
            "e 1/10 i 12600/40794: loss 5.7235589027404785\n",
            "e 1/10 i 12601/40794: loss 5.940031051635742\n",
            "e 1/10 i 12602/40794: loss 5.754270553588867\n",
            "e 1/10 i 12603/40794: loss 5.931551456451416\n",
            "e 1/10 i 12604/40794: loss 5.778741836547852\n",
            "e 1/10 i 12605/40794: loss 5.964844703674316\n",
            "e 1/10 i 12606/40794: loss 5.609453201293945\n",
            "e 1/10 i 12607/40794: loss 5.558431148529053\n",
            "e 1/10 i 12608/40794: loss 5.920558929443359\n",
            "e 1/10 i 12609/40794: loss 6.116677761077881\n",
            "e 1/10 i 12610/40794: loss 5.9753618240356445\n",
            "e 1/10 i 12611/40794: loss 5.777483940124512\n",
            "e 1/10 i 12612/40794: loss 6.008184909820557\n",
            "e 1/10 i 12613/40794: loss 6.246718883514404\n",
            "e 1/10 i 12614/40794: loss 5.795446872711182\n",
            "e 1/10 i 12615/40794: loss 5.9175214767456055\n",
            "e 1/10 i 12616/40794: loss 6.010343074798584\n",
            "e 1/10 i 12617/40794: loss 5.829892158508301\n",
            "e 1/10 i 12618/40794: loss 6.138559341430664\n",
            "e 1/10 i 12619/40794: loss 5.784606456756592\n",
            "e 1/10 i 12620/40794: loss 5.938952445983887\n",
            "e 1/10 i 12621/40794: loss 5.786352634429932\n",
            "e 1/10 i 12622/40794: loss 6.014124870300293\n",
            "e 1/10 i 12623/40794: loss 5.999967575073242\n",
            "e 1/10 i 12624/40794: loss 5.8865647315979\n",
            "e 1/10 i 12625/40794: loss 5.623122215270996\n",
            "e 1/10 i 12626/40794: loss 6.194596767425537\n",
            "e 1/10 i 12627/40794: loss 6.20525598526001\n",
            "e 1/10 i 12628/40794: loss 5.857595920562744\n",
            "e 1/10 i 12629/40794: loss 5.9890031814575195\n",
            "e 1/10 i 12630/40794: loss 6.317905426025391\n",
            "e 1/10 i 12631/40794: loss 5.843054294586182\n",
            "e 1/10 i 12632/40794: loss 5.841484546661377\n",
            "e 1/10 i 12633/40794: loss 5.739912509918213\n",
            "e 1/10 i 12634/40794: loss 6.208624839782715\n",
            "e 1/10 i 12635/40794: loss 6.243497848510742\n",
            "e 1/10 i 12636/40794: loss 6.166038990020752\n",
            "e 1/10 i 12637/40794: loss 5.919885158538818\n",
            "e 1/10 i 12638/40794: loss 5.890682697296143\n",
            "e 1/10 i 12639/40794: loss 5.874911308288574\n",
            "e 1/10 i 12640/40794: loss 5.967595100402832\n",
            "e 1/10 i 12641/40794: loss 6.180082321166992\n",
            "e 1/10 i 12642/40794: loss 5.80079460144043\n",
            "e 1/10 i 12643/40794: loss 5.888505458831787\n",
            "e 1/10 i 12644/40794: loss 5.752184867858887\n",
            "e 1/10 i 12645/40794: loss 5.8241047859191895\n",
            "e 1/10 i 12646/40794: loss 5.996355056762695\n",
            "e 1/10 i 12647/40794: loss 5.955497741699219\n",
            "e 1/10 i 12648/40794: loss 5.702016830444336\n",
            "e 1/10 i 12649/40794: loss 6.004213333129883\n",
            "e 1/10 i 12650/40794: loss 6.158295154571533\n",
            "e 1/10 i 12651/40794: loss 5.563035488128662\n",
            "e 1/10 i 12652/40794: loss 5.717357635498047\n",
            "e 1/10 i 12653/40794: loss 5.689639568328857\n",
            "e 1/10 i 12654/40794: loss 5.644402027130127\n",
            "e 1/10 i 12655/40794: loss 5.608830451965332\n",
            "e 1/10 i 12656/40794: loss 5.940169334411621\n",
            "e 1/10 i 12657/40794: loss 5.867432117462158\n",
            "e 1/10 i 12658/40794: loss 5.7167067527771\n",
            "e 1/10 i 12659/40794: loss 6.256506443023682\n",
            "e 1/10 i 12660/40794: loss 5.780367374420166\n",
            "e 1/10 i 12661/40794: loss 6.007524490356445\n",
            "e 1/10 i 12662/40794: loss 6.183998107910156\n",
            "e 1/10 i 12663/40794: loss 5.793910503387451\n",
            "e 1/10 i 12664/40794: loss 5.934099197387695\n",
            "e 1/10 i 12665/40794: loss 5.797786712646484\n",
            "e 1/10 i 12666/40794: loss 6.146830081939697\n",
            "e 1/10 i 12667/40794: loss 5.874972343444824\n",
            "e 1/10 i 12668/40794: loss 6.254520416259766\n",
            "e 1/10 i 12669/40794: loss 6.033860206604004\n",
            "e 1/10 i 12670/40794: loss 5.817230224609375\n",
            "e 1/10 i 12671/40794: loss 6.191268444061279\n",
            "e 1/10 i 12672/40794: loss 5.910894870758057\n",
            "e 1/10 i 12673/40794: loss 6.06095552444458\n",
            "e 1/10 i 12674/40794: loss 5.900379657745361\n",
            "e 1/10 i 12675/40794: loss 5.961514472961426\n",
            "e 1/10 i 12676/40794: loss 6.094429969787598\n",
            "e 1/10 i 12677/40794: loss 5.784825801849365\n",
            "e 1/10 i 12678/40794: loss 5.6982011795043945\n",
            "e 1/10 i 12679/40794: loss 6.219693660736084\n",
            "e 1/10 i 12680/40794: loss 6.017671585083008\n",
            "e 1/10 i 12681/40794: loss 6.157483100891113\n",
            "e 1/10 i 12682/40794: loss 5.975653171539307\n",
            "e 1/10 i 12683/40794: loss 5.871360778808594\n",
            "e 1/10 i 12684/40794: loss 6.0478291511535645\n",
            "e 1/10 i 12685/40794: loss 5.875707149505615\n",
            "e 1/10 i 12686/40794: loss 6.024896621704102\n",
            "e 1/10 i 12687/40794: loss 5.9637908935546875\n",
            "e 1/10 i 12688/40794: loss 6.114834785461426\n",
            "e 1/10 i 12689/40794: loss 6.11753511428833\n",
            "e 1/10 i 12690/40794: loss 6.2584309577941895\n",
            "e 1/10 i 12691/40794: loss 6.068105697631836\n",
            "e 1/10 i 12692/40794: loss 5.882272243499756\n",
            "e 1/10 i 12693/40794: loss 6.18687629699707\n",
            "e 1/10 i 12694/40794: loss 5.909731388092041\n",
            "e 1/10 i 12695/40794: loss 5.9360880851745605\n",
            "e 1/10 i 12696/40794: loss 5.670035362243652\n",
            "e 1/10 i 12697/40794: loss 6.045982360839844\n",
            "e 1/10 i 12698/40794: loss 5.643203258514404\n",
            "e 1/10 i 12699/40794: loss 5.913900852203369\n",
            "e 1/10 i 12700/40794: loss 6.286815643310547\n",
            "e 1/10 i 12701/40794: loss 5.965756416320801\n",
            "e 1/10 i 12702/40794: loss 6.02716588973999\n",
            "e 1/10 i 12703/40794: loss 5.939218997955322\n",
            "e 1/10 i 12704/40794: loss 5.971833229064941\n",
            "e 1/10 i 12705/40794: loss 5.986159801483154\n",
            "e 1/10 i 12706/40794: loss 5.930566310882568\n",
            "e 1/10 i 12707/40794: loss 6.084704399108887\n",
            "e 1/10 i 12708/40794: loss 5.84342098236084\n",
            "e 1/10 i 12709/40794: loss 5.991902828216553\n",
            "e 1/10 i 12710/40794: loss 6.423192024230957\n",
            "e 1/10 i 12711/40794: loss 6.017902374267578\n",
            "e 1/10 i 12712/40794: loss 6.208763599395752\n",
            "e 1/10 i 12713/40794: loss 6.038166522979736\n",
            "e 1/10 i 12714/40794: loss 5.6890339851379395\n",
            "e 1/10 i 12715/40794: loss 5.794017791748047\n",
            "e 1/10 i 12716/40794: loss 5.995738506317139\n",
            "e 1/10 i 12717/40794: loss 6.044864177703857\n",
            "e 1/10 i 12718/40794: loss 6.012722492218018\n",
            "e 1/10 i 12719/40794: loss 5.930519104003906\n",
            "e 1/10 i 12720/40794: loss 6.175741672515869\n",
            "e 1/10 i 12721/40794: loss 6.205007553100586\n",
            "e 1/10 i 12722/40794: loss 5.988039493560791\n",
            "e 1/10 i 12723/40794: loss 6.0019731521606445\n",
            "e 1/10 i 12724/40794: loss 6.115828037261963\n",
            "e 1/10 i 12725/40794: loss 5.948334217071533\n",
            "e 1/10 i 12726/40794: loss 5.806529998779297\n",
            "e 1/10 i 12727/40794: loss 6.088332176208496\n",
            "e 1/10 i 12728/40794: loss 5.762732028961182\n",
            "e 1/10 i 12729/40794: loss 5.8877787590026855\n",
            "e 1/10 i 12730/40794: loss 5.738332748413086\n",
            "e 1/10 i 12731/40794: loss 5.870419979095459\n",
            "e 1/10 i 12732/40794: loss 6.0149149894714355\n",
            "e 1/10 i 12733/40794: loss 6.119488716125488\n",
            "e 1/10 i 12734/40794: loss 5.98643684387207\n",
            "e 1/10 i 12735/40794: loss 6.122486114501953\n",
            "e 1/10 i 12736/40794: loss 5.969715595245361\n",
            "e 1/10 i 12737/40794: loss 6.272623538970947\n",
            "e 1/10 i 12738/40794: loss 6.192025661468506\n",
            "e 1/10 i 12739/40794: loss 5.860685348510742\n",
            "e 1/10 i 12740/40794: loss 6.008490085601807\n",
            "e 1/10 i 12741/40794: loss 6.066694736480713\n",
            "e 1/10 i 12742/40794: loss 5.782688140869141\n",
            "e 1/10 i 12743/40794: loss 6.0023512840271\n",
            "e 1/10 i 12744/40794: loss 5.948093414306641\n",
            "e 1/10 i 12745/40794: loss 6.374114513397217\n",
            "e 1/10 i 12746/40794: loss 5.663592338562012\n",
            "e 1/10 i 12747/40794: loss 5.751849174499512\n",
            "e 1/10 i 12748/40794: loss 6.106794834136963\n",
            "e 1/10 i 12749/40794: loss 6.105968475341797\n",
            "e 1/10 i 12750/40794: loss 6.031020641326904\n",
            "e 1/10 i 12751/40794: loss 5.82887077331543\n",
            "e 1/10 i 12752/40794: loss 5.914909839630127\n",
            "e 1/10 i 12753/40794: loss 5.938841819763184\n",
            "e 1/10 i 12754/40794: loss 5.804607391357422\n",
            "e 1/10 i 12755/40794: loss 5.80857515335083\n",
            "e 1/10 i 12756/40794: loss 6.044468879699707\n",
            "e 1/10 i 12757/40794: loss 5.839764595031738\n",
            "e 1/10 i 12758/40794: loss 5.913032054901123\n",
            "e 1/10 i 12759/40794: loss 5.884293556213379\n",
            "e 1/10 i 12760/40794: loss 5.988058090209961\n",
            "e 1/10 i 12761/40794: loss 6.214200973510742\n",
            "e 1/10 i 12762/40794: loss 5.4892354011535645\n",
            "e 1/10 i 12763/40794: loss 5.866547584533691\n",
            "e 1/10 i 12764/40794: loss 5.657772064208984\n",
            "e 1/10 i 12765/40794: loss 6.1002888679504395\n",
            "e 1/10 i 12766/40794: loss 6.047059535980225\n",
            "e 1/10 i 12767/40794: loss 5.967391014099121\n",
            "e 1/10 i 12768/40794: loss 5.751233100891113\n",
            "e 1/10 i 12769/40794: loss 6.327950954437256\n",
            "e 1/10 i 12770/40794: loss 5.818570137023926\n",
            "e 1/10 i 12771/40794: loss 6.071483612060547\n",
            "e 1/10 i 12772/40794: loss 5.644962787628174\n",
            "e 1/10 i 12773/40794: loss 5.933263778686523\n",
            "e 1/10 i 12774/40794: loss 5.980891227722168\n",
            "e 1/10 i 12775/40794: loss 6.0799760818481445\n",
            "e 1/10 i 12776/40794: loss 5.611807823181152\n",
            "e 1/10 i 12777/40794: loss 5.824275970458984\n",
            "e 1/10 i 12778/40794: loss 6.026223182678223\n",
            "e 1/10 i 12779/40794: loss 6.192872524261475\n",
            "e 1/10 i 12780/40794: loss 5.990746974945068\n",
            "e 1/10 i 12781/40794: loss 5.858903884887695\n",
            "e 1/10 i 12782/40794: loss 5.867417812347412\n",
            "e 1/10 i 12783/40794: loss 5.913134574890137\n",
            "e 1/10 i 12784/40794: loss 5.753152847290039\n",
            "e 1/10 i 12785/40794: loss 5.758203506469727\n",
            "e 1/10 i 12786/40794: loss 5.926762104034424\n",
            "e 1/10 i 12787/40794: loss 5.887310028076172\n",
            "e 1/10 i 12788/40794: loss 6.305424213409424\n",
            "e 1/10 i 12789/40794: loss 5.961585521697998\n",
            "e 1/10 i 12790/40794: loss 6.114513397216797\n",
            "e 1/10 i 12791/40794: loss 5.9916534423828125\n",
            "e 1/10 i 12792/40794: loss 5.992697715759277\n",
            "e 1/10 i 12793/40794: loss 6.068274021148682\n",
            "e 1/10 i 12794/40794: loss 5.7743940353393555\n",
            "e 1/10 i 12795/40794: loss 5.8827104568481445\n",
            "e 1/10 i 12796/40794: loss 5.992596626281738\n",
            "e 1/10 i 12797/40794: loss 6.134786128997803\n",
            "e 1/10 i 12798/40794: loss 5.917057037353516\n",
            "e 1/10 i 12799/40794: loss 5.663012504577637\n",
            "e 1/10 i 12800/40794: loss 5.822779178619385\n",
            "e 1/10 i 12801/40794: loss 5.829905986785889\n",
            "e 1/10 i 12802/40794: loss 5.952642917633057\n",
            "e 1/10 i 12803/40794: loss 5.809773921966553\n",
            "e 1/10 i 12804/40794: loss 5.774159908294678\n",
            "e 1/10 i 12805/40794: loss 6.077062606811523\n",
            "e 1/10 i 12806/40794: loss 5.996490955352783\n",
            "e 1/10 i 12807/40794: loss 5.85590934753418\n",
            "e 1/10 i 12808/40794: loss 6.234761714935303\n",
            "e 1/10 i 12809/40794: loss 5.924002647399902\n",
            "e 1/10 i 12810/40794: loss 6.080787658691406\n",
            "e 1/10 i 12811/40794: loss 5.876457691192627\n",
            "e 1/10 i 12812/40794: loss 5.747450828552246\n",
            "e 1/10 i 12813/40794: loss 6.267443656921387\n",
            "e 1/10 i 12814/40794: loss 5.897129058837891\n",
            "e 1/10 i 12815/40794: loss 5.862887859344482\n",
            "e 1/10 i 12816/40794: loss 5.925785064697266\n",
            "e 1/10 i 12817/40794: loss 6.170191764831543\n",
            "e 1/10 i 12818/40794: loss 5.935617446899414\n",
            "e 1/10 i 12819/40794: loss 5.9470534324646\n",
            "e 1/10 i 12820/40794: loss 6.2718658447265625\n",
            "e 1/10 i 12821/40794: loss 6.057126998901367\n",
            "e 1/10 i 12822/40794: loss 5.885948181152344\n",
            "e 1/10 i 12823/40794: loss 5.762439250946045\n",
            "e 1/10 i 12824/40794: loss 5.548398017883301\n",
            "e 1/10 i 12825/40794: loss 6.174960613250732\n",
            "e 1/10 i 12826/40794: loss 6.1168212890625\n",
            "e 1/10 i 12827/40794: loss 6.001719951629639\n",
            "e 1/10 i 12828/40794: loss 5.83348274230957\n",
            "e 1/10 i 12829/40794: loss 5.772421360015869\n",
            "e 1/10 i 12830/40794: loss 5.896706581115723\n",
            "e 1/10 i 12831/40794: loss 5.87297248840332\n",
            "e 1/10 i 12832/40794: loss 5.832413196563721\n",
            "e 1/10 i 12833/40794: loss 5.987030982971191\n",
            "e 1/10 i 12834/40794: loss 5.852138042449951\n",
            "e 1/10 i 12835/40794: loss 5.89617919921875\n",
            "e 1/10 i 12836/40794: loss 5.997006893157959\n",
            "e 1/10 i 12837/40794: loss 5.71705436706543\n",
            "e 1/10 i 12838/40794: loss 5.9272141456604\n",
            "e 1/10 i 12839/40794: loss 6.204847812652588\n",
            "e 1/10 i 12840/40794: loss 5.657073020935059\n",
            "e 1/10 i 12841/40794: loss 5.8769659996032715\n",
            "e 1/10 i 12842/40794: loss 5.802262306213379\n",
            "e 1/10 i 12843/40794: loss 6.093667984008789\n",
            "e 1/10 i 12844/40794: loss 5.868890285491943\n",
            "e 1/10 i 12845/40794: loss 5.921023368835449\n",
            "e 1/10 i 12846/40794: loss 5.960683345794678\n",
            "e 1/10 i 12847/40794: loss 5.782691955566406\n",
            "e 1/10 i 12848/40794: loss 6.039501667022705\n",
            "e 1/10 i 12849/40794: loss 5.937614917755127\n",
            "e 1/10 i 12850/40794: loss 6.057717323303223\n",
            "e 1/10 i 12851/40794: loss 6.274589538574219\n",
            "e 1/10 i 12852/40794: loss 6.079248905181885\n",
            "e 1/10 i 12853/40794: loss 6.306708335876465\n",
            "e 1/10 i 12854/40794: loss 5.978115558624268\n",
            "e 1/10 i 12855/40794: loss 5.995172023773193\n",
            "e 1/10 i 12856/40794: loss 5.9107136726379395\n",
            "e 1/10 i 12857/40794: loss 6.002654075622559\n",
            "e 1/10 i 12858/40794: loss 6.190445899963379\n",
            "e 1/10 i 12859/40794: loss 6.072525501251221\n",
            "e 1/10 i 12860/40794: loss 5.860856056213379\n",
            "e 1/10 i 12861/40794: loss 6.017658710479736\n",
            "e 1/10 i 12862/40794: loss 6.027348518371582\n",
            "e 1/10 i 12863/40794: loss 5.8381547927856445\n",
            "e 1/10 i 12864/40794: loss 5.9593329429626465\n",
            "e 1/10 i 12865/40794: loss 5.676558971405029\n",
            "e 1/10 i 12866/40794: loss 6.107107162475586\n",
            "e 1/10 i 12867/40794: loss 6.057069301605225\n",
            "e 1/10 i 12868/40794: loss 5.753279685974121\n",
            "e 1/10 i 12869/40794: loss 5.890726566314697\n",
            "e 1/10 i 12870/40794: loss 5.684293270111084\n",
            "e 1/10 i 12871/40794: loss 5.803252696990967\n",
            "e 1/10 i 12872/40794: loss 6.36043119430542\n",
            "e 1/10 i 12873/40794: loss 5.867945194244385\n",
            "e 1/10 i 12874/40794: loss 5.943059921264648\n",
            "e 1/10 i 12875/40794: loss 5.940781116485596\n",
            "e 1/10 i 12876/40794: loss 6.008373260498047\n",
            "e 1/10 i 12877/40794: loss 5.821545600891113\n",
            "e 1/10 i 12878/40794: loss 6.031368255615234\n",
            "e 1/10 i 12879/40794: loss 5.821863174438477\n",
            "e 1/10 i 12880/40794: loss 5.987068176269531\n",
            "e 1/10 i 12881/40794: loss 5.74021053314209\n",
            "e 1/10 i 12882/40794: loss 5.95400333404541\n",
            "e 1/10 i 12883/40794: loss 5.999829292297363\n",
            "e 1/10 i 12884/40794: loss 6.077760219573975\n",
            "e 1/10 i 12885/40794: loss 6.067142009735107\n",
            "e 1/10 i 12886/40794: loss 5.863447189331055\n",
            "e 1/10 i 12887/40794: loss 5.905224323272705\n",
            "e 1/10 i 12888/40794: loss 6.109729766845703\n",
            "e 1/10 i 12889/40794: loss 6.343471050262451\n",
            "e 1/10 i 12890/40794: loss 6.287683010101318\n",
            "e 1/10 i 12891/40794: loss 5.957096576690674\n",
            "e 1/10 i 12892/40794: loss 6.216080665588379\n",
            "e 1/10 i 12893/40794: loss 5.851306438446045\n",
            "e 1/10 i 12894/40794: loss 6.111837863922119\n",
            "e 1/10 i 12895/40794: loss 5.91736364364624\n",
            "e 1/10 i 12896/40794: loss 5.833888530731201\n",
            "e 1/10 i 12897/40794: loss 6.2608208656311035\n",
            "e 1/10 i 12898/40794: loss 6.105538845062256\n",
            "e 1/10 i 12899/40794: loss 5.783502101898193\n",
            "e 1/10 i 12900/40794: loss 5.823178291320801\n",
            "e 1/10 i 12901/40794: loss 5.970042705535889\n",
            "e 1/10 i 12902/40794: loss 5.775604248046875\n",
            "e 1/10 i 12903/40794: loss 5.950341701507568\n",
            "e 1/10 i 12904/40794: loss 6.0051422119140625\n",
            "e 1/10 i 12905/40794: loss 5.942455291748047\n",
            "e 1/10 i 12906/40794: loss 5.966284275054932\n",
            "e 1/10 i 12907/40794: loss 5.841926574707031\n",
            "e 1/10 i 12908/40794: loss 5.976100444793701\n",
            "e 1/10 i 12909/40794: loss 5.786416053771973\n",
            "e 1/10 i 12910/40794: loss 5.837987899780273\n",
            "e 1/10 i 12911/40794: loss 5.891862869262695\n",
            "e 1/10 i 12912/40794: loss 5.776431560516357\n",
            "e 1/10 i 12913/40794: loss 5.8863911628723145\n",
            "e 1/10 i 12914/40794: loss 6.107985019683838\n",
            "e 1/10 i 12915/40794: loss 5.96856689453125\n",
            "e 1/10 i 12916/40794: loss 5.618104934692383\n",
            "e 1/10 i 12917/40794: loss 5.8960957527160645\n",
            "e 1/10 i 12918/40794: loss 6.023922920227051\n",
            "e 1/10 i 12919/40794: loss 6.036377906799316\n",
            "e 1/10 i 12920/40794: loss 5.903468608856201\n",
            "e 1/10 i 12921/40794: loss 5.897249698638916\n",
            "e 1/10 i 12922/40794: loss 5.953832149505615\n",
            "e 1/10 i 12923/40794: loss 5.7237443923950195\n",
            "e 1/10 i 12924/40794: loss 6.216482162475586\n",
            "e 1/10 i 12925/40794: loss 5.9614787101745605\n",
            "e 1/10 i 12926/40794: loss 6.264469146728516\n",
            "e 1/10 i 12927/40794: loss 5.922704696655273\n",
            "e 1/10 i 12928/40794: loss 5.795803546905518\n",
            "e 1/10 i 12929/40794: loss 5.898895740509033\n",
            "e 1/10 i 12930/40794: loss 5.965450763702393\n",
            "e 1/10 i 12931/40794: loss 5.908005237579346\n",
            "e 1/10 i 12932/40794: loss 5.808658599853516\n",
            "e 1/10 i 12933/40794: loss 6.172949314117432\n",
            "e 1/10 i 12934/40794: loss 6.012674808502197\n",
            "e 1/10 i 12935/40794: loss 5.908413887023926\n",
            "e 1/10 i 12936/40794: loss 5.843982219696045\n",
            "e 1/10 i 12937/40794: loss 5.746838092803955\n",
            "e 1/10 i 12938/40794: loss 5.7233567237854\n",
            "e 1/10 i 12939/40794: loss 6.1365132331848145\n",
            "e 1/10 i 12940/40794: loss 5.84852409362793\n",
            "e 1/10 i 12941/40794: loss 5.724506378173828\n",
            "e 1/10 i 12942/40794: loss 6.131021499633789\n",
            "e 1/10 i 12943/40794: loss 5.720831871032715\n",
            "e 1/10 i 12944/40794: loss 5.803226947784424\n",
            "e 1/10 i 12945/40794: loss 6.17404317855835\n",
            "e 1/10 i 12946/40794: loss 5.829253196716309\n",
            "e 1/10 i 12947/40794: loss 5.888491630554199\n",
            "e 1/10 i 12948/40794: loss 5.981334209442139\n",
            "e 1/10 i 12949/40794: loss 5.871726036071777\n",
            "e 1/10 i 12950/40794: loss 5.7456278800964355\n",
            "e 1/10 i 12951/40794: loss 5.768675327301025\n",
            "e 1/10 i 12952/40794: loss 6.1851582527160645\n",
            "e 1/10 i 12953/40794: loss 5.715136528015137\n",
            "e 1/10 i 12954/40794: loss 5.940152645111084\n",
            "e 1/10 i 12955/40794: loss 6.083127975463867\n",
            "e 1/10 i 12956/40794: loss 5.68684720993042\n",
            "e 1/10 i 12957/40794: loss 5.958333969116211\n",
            "e 1/10 i 12958/40794: loss 5.825536251068115\n",
            "e 1/10 i 12959/40794: loss 5.830862522125244\n",
            "e 1/10 i 12960/40794: loss 6.18856954574585\n",
            "e 1/10 i 12961/40794: loss 5.73814582824707\n",
            "e 1/10 i 12962/40794: loss 5.896646976470947\n",
            "e 1/10 i 12963/40794: loss 5.847752571105957\n",
            "e 1/10 i 12964/40794: loss 5.829341888427734\n",
            "e 1/10 i 12965/40794: loss 5.834805965423584\n",
            "e 1/10 i 12966/40794: loss 6.1789231300354\n",
            "e 1/10 i 12967/40794: loss 5.863887310028076\n",
            "e 1/10 i 12968/40794: loss 6.044398784637451\n",
            "e 1/10 i 12969/40794: loss 5.872891426086426\n",
            "e 1/10 i 12970/40794: loss 5.7166266441345215\n",
            "e 1/10 i 12971/40794: loss 5.930257797241211\n",
            "e 1/10 i 12972/40794: loss 5.875773906707764\n",
            "e 1/10 i 12973/40794: loss 5.8201003074646\n",
            "e 1/10 i 12974/40794: loss 6.00844669342041\n",
            "e 1/10 i 12975/40794: loss 6.074812889099121\n",
            "e 1/10 i 12976/40794: loss 5.903709411621094\n",
            "e 1/10 i 12977/40794: loss 5.463967323303223\n",
            "e 1/10 i 12978/40794: loss 5.887785911560059\n",
            "e 1/10 i 12979/40794: loss 6.001862525939941\n",
            "e 1/10 i 12980/40794: loss 6.036115646362305\n",
            "e 1/10 i 12981/40794: loss 6.059315204620361\n",
            "e 1/10 i 12982/40794: loss 5.991213321685791\n",
            "e 1/10 i 12983/40794: loss 5.945319652557373\n",
            "e 1/10 i 12984/40794: loss 5.8570780754089355\n",
            "e 1/10 i 12985/40794: loss 5.967333793640137\n",
            "e 1/10 i 12986/40794: loss 6.007680892944336\n",
            "e 1/10 i 12987/40794: loss 6.01759672164917\n",
            "e 1/10 i 12988/40794: loss 6.033901691436768\n",
            "e 1/10 i 12989/40794: loss 5.969840049743652\n",
            "e 1/10 i 12990/40794: loss 5.824314594268799\n",
            "e 1/10 i 12991/40794: loss 6.070769309997559\n",
            "e 1/10 i 12992/40794: loss 5.750731945037842\n",
            "e 1/10 i 12993/40794: loss 5.974243640899658\n",
            "e 1/10 i 12994/40794: loss 6.163909435272217\n",
            "e 1/10 i 12995/40794: loss 5.787278652191162\n",
            "e 1/10 i 12996/40794: loss 5.924711227416992\n",
            "e 1/10 i 12997/40794: loss 5.712212085723877\n",
            "e 1/10 i 12998/40794: loss 6.007803440093994\n",
            "e 1/10 i 12999/40794: loss 5.81892204284668\n",
            "e 1/10 i 13000/40794: loss 6.058343410491943\n",
            "e 1/10 i 13001/40794: loss 5.544302940368652\n",
            "e 1/10 i 13002/40794: loss 5.766532897949219\n",
            "e 1/10 i 13003/40794: loss 5.76505708694458\n",
            "e 1/10 i 13004/40794: loss 5.811065196990967\n",
            "e 1/10 i 13005/40794: loss 6.002456188201904\n",
            "e 1/10 i 13006/40794: loss 5.789412021636963\n",
            "e 1/10 i 13007/40794: loss 5.918183326721191\n",
            "e 1/10 i 13008/40794: loss 5.761321067810059\n",
            "e 1/10 i 13009/40794: loss 6.137235164642334\n",
            "e 1/10 i 13010/40794: loss 6.367338180541992\n",
            "e 1/10 i 13011/40794: loss 5.743974208831787\n",
            "e 1/10 i 13012/40794: loss 6.007285118103027\n",
            "e 1/10 i 13013/40794: loss 6.3447370529174805\n",
            "e 1/10 i 13014/40794: loss 5.704787731170654\n",
            "e 1/10 i 13015/40794: loss 6.126119613647461\n",
            "e 1/10 i 13016/40794: loss 6.01369047164917\n",
            "e 1/10 i 13017/40794: loss 6.212940692901611\n",
            "e 1/10 i 13018/40794: loss 5.859346866607666\n",
            "e 1/10 i 13019/40794: loss 5.853128910064697\n",
            "e 1/10 i 13020/40794: loss 5.901873588562012\n",
            "e 1/10 i 13021/40794: loss 5.989891052246094\n",
            "e 1/10 i 13022/40794: loss 6.046860218048096\n",
            "e 1/10 i 13023/40794: loss 6.029179096221924\n",
            "e 1/10 i 13024/40794: loss 6.064935684204102\n",
            "e 1/10 i 13025/40794: loss 6.025298118591309\n",
            "e 1/10 i 13026/40794: loss 5.910793304443359\n",
            "e 1/10 i 13027/40794: loss 5.901237964630127\n",
            "e 1/10 i 13028/40794: loss 5.839662075042725\n",
            "e 1/10 i 13029/40794: loss 5.845318794250488\n",
            "e 1/10 i 13030/40794: loss 5.877946853637695\n",
            "e 1/10 i 13031/40794: loss 5.935616493225098\n",
            "e 1/10 i 13032/40794: loss 5.809081077575684\n",
            "e 1/10 i 13033/40794: loss 5.767261981964111\n",
            "e 1/10 i 13034/40794: loss 5.8828444480896\n",
            "e 1/10 i 13035/40794: loss 6.009097576141357\n",
            "e 1/10 i 13036/40794: loss 6.05818510055542\n",
            "e 1/10 i 13037/40794: loss 6.139617919921875\n",
            "e 1/10 i 13038/40794: loss 5.925787925720215\n",
            "e 1/10 i 13039/40794: loss 5.848755359649658\n",
            "e 1/10 i 13040/40794: loss 5.648405075073242\n",
            "e 1/10 i 13041/40794: loss 5.890952110290527\n",
            "e 1/10 i 13042/40794: loss 6.378014087677002\n",
            "e 1/10 i 13043/40794: loss 5.745636940002441\n",
            "e 1/10 i 13044/40794: loss 5.786922931671143\n",
            "e 1/10 i 13045/40794: loss 5.947652816772461\n",
            "e 1/10 i 13046/40794: loss 5.867748260498047\n",
            "e 1/10 i 13047/40794: loss 5.832106113433838\n",
            "e 1/10 i 13048/40794: loss 5.677921772003174\n",
            "e 1/10 i 13049/40794: loss 5.760700702667236\n",
            "e 1/10 i 13050/40794: loss 6.074063777923584\n",
            "e 1/10 i 13051/40794: loss 5.6765241622924805\n",
            "e 1/10 i 13052/40794: loss 5.878686428070068\n",
            "e 1/10 i 13053/40794: loss 6.075166702270508\n",
            "e 1/10 i 13054/40794: loss 5.753861427307129\n",
            "e 1/10 i 13055/40794: loss 6.338575839996338\n",
            "e 1/10 i 13056/40794: loss 5.585303783416748\n",
            "e 1/10 i 13057/40794: loss 5.7589945793151855\n",
            "e 1/10 i 13058/40794: loss 6.152463912963867\n",
            "e 1/10 i 13059/40794: loss 6.226497650146484\n",
            "e 1/10 i 13060/40794: loss 5.9472479820251465\n",
            "e 1/10 i 13061/40794: loss 5.937503814697266\n",
            "e 1/10 i 13062/40794: loss 5.800492286682129\n",
            "e 1/10 i 13063/40794: loss 6.185039520263672\n",
            "e 1/10 i 13064/40794: loss 5.86828088760376\n",
            "e 1/10 i 13065/40794: loss 5.826542377471924\n",
            "e 1/10 i 13066/40794: loss 6.017129421234131\n",
            "e 1/10 i 13067/40794: loss 5.861264228820801\n",
            "e 1/10 i 13068/40794: loss 5.880538463592529\n",
            "e 1/10 i 13069/40794: loss 6.174854278564453\n",
            "e 1/10 i 13070/40794: loss 5.820728302001953\n",
            "e 1/10 i 13071/40794: loss 5.835334777832031\n",
            "e 1/10 i 13072/40794: loss 5.819765090942383\n",
            "e 1/10 i 13073/40794: loss 5.8405561447143555\n",
            "e 1/10 i 13074/40794: loss 5.719619274139404\n",
            "e 1/10 i 13075/40794: loss 5.68800163269043\n",
            "e 1/10 i 13076/40794: loss 5.941019058227539\n",
            "e 1/10 i 13077/40794: loss 5.85271692276001\n",
            "e 1/10 i 13078/40794: loss 5.520608901977539\n",
            "e 1/10 i 13079/40794: loss 5.898754596710205\n",
            "e 1/10 i 13080/40794: loss 5.942189693450928\n",
            "e 1/10 i 13081/40794: loss 5.821553707122803\n",
            "e 1/10 i 13082/40794: loss 6.207177639007568\n",
            "e 1/10 i 13083/40794: loss 5.92286491394043\n",
            "e 1/10 i 13084/40794: loss 6.303073406219482\n",
            "e 1/10 i 13085/40794: loss 5.872171401977539\n",
            "e 1/10 i 13086/40794: loss 6.126681327819824\n",
            "e 1/10 i 13087/40794: loss 5.940732955932617\n",
            "e 1/10 i 13088/40794: loss 5.664330959320068\n",
            "e 1/10 i 13089/40794: loss 5.5998687744140625\n",
            "e 1/10 i 13090/40794: loss 6.074464797973633\n",
            "e 1/10 i 13091/40794: loss 5.935205459594727\n",
            "e 1/10 i 13092/40794: loss 5.871709823608398\n",
            "e 1/10 i 13093/40794: loss 5.863543510437012\n",
            "e 1/10 i 13094/40794: loss 6.111896991729736\n",
            "e 1/10 i 13095/40794: loss 5.9025187492370605\n",
            "e 1/10 i 13096/40794: loss 5.861140251159668\n",
            "e 1/10 i 13097/40794: loss 6.023710250854492\n",
            "e 1/10 i 13098/40794: loss 5.855098724365234\n",
            "e 1/10 i 13099/40794: loss 5.858902931213379\n",
            "e 1/10 i 13100/40794: loss 5.855840682983398\n",
            "e 1/10 i 13101/40794: loss 5.8167724609375\n",
            "e 1/10 i 13102/40794: loss 5.695034503936768\n",
            "e 1/10 i 13103/40794: loss 5.940315246582031\n",
            "e 1/10 i 13104/40794: loss 6.151534080505371\n",
            "e 1/10 i 13105/40794: loss 6.034668445587158\n",
            "e 1/10 i 13106/40794: loss 5.805928707122803\n",
            "e 1/10 i 13107/40794: loss 5.939676761627197\n",
            "e 1/10 i 13108/40794: loss 5.8066582679748535\n",
            "e 1/10 i 13109/40794: loss 5.8020548820495605\n",
            "e 1/10 i 13110/40794: loss 6.226428031921387\n",
            "e 1/10 i 13111/40794: loss 5.884353160858154\n",
            "e 1/10 i 13112/40794: loss 6.085418701171875\n",
            "e 1/10 i 13113/40794: loss 6.163082122802734\n",
            "e 1/10 i 13114/40794: loss 5.7951555252075195\n",
            "e 1/10 i 13115/40794: loss 6.056186676025391\n",
            "e 1/10 i 13116/40794: loss 5.91109037399292\n",
            "e 1/10 i 13117/40794: loss 5.5387115478515625\n",
            "e 1/10 i 13118/40794: loss 5.888036251068115\n",
            "e 1/10 i 13119/40794: loss 5.608612537384033\n",
            "e 1/10 i 13120/40794: loss 6.201785087585449\n",
            "e 1/10 i 13121/40794: loss 6.242695331573486\n",
            "e 1/10 i 13122/40794: loss 5.8373003005981445\n",
            "e 1/10 i 13123/40794: loss 5.901569366455078\n",
            "e 1/10 i 13124/40794: loss 5.951264381408691\n",
            "e 1/10 i 13125/40794: loss 6.195641994476318\n",
            "e 1/10 i 13126/40794: loss 5.808310508728027\n",
            "e 1/10 i 13127/40794: loss 6.085494041442871\n",
            "e 1/10 i 13128/40794: loss 5.866847515106201\n",
            "e 1/10 i 13129/40794: loss 5.979894638061523\n",
            "e 1/10 i 13130/40794: loss 6.266901016235352\n",
            "e 1/10 i 13131/40794: loss 6.069721221923828\n",
            "e 1/10 i 13132/40794: loss 5.867009162902832\n",
            "e 1/10 i 13133/40794: loss 6.170845031738281\n",
            "e 1/10 i 13134/40794: loss 6.132901191711426\n",
            "e 1/10 i 13135/40794: loss 5.781363010406494\n",
            "e 1/10 i 13136/40794: loss 5.744605541229248\n",
            "e 1/10 i 13137/40794: loss 5.881150722503662\n",
            "e 1/10 i 13138/40794: loss 6.158656597137451\n",
            "e 1/10 i 13139/40794: loss 6.175344467163086\n",
            "e 1/10 i 13140/40794: loss 5.769334316253662\n",
            "e 1/10 i 13141/40794: loss 6.051187515258789\n",
            "e 1/10 i 13142/40794: loss 5.697993755340576\n",
            "e 1/10 i 13143/40794: loss 6.106030464172363\n",
            "e 1/10 i 13144/40794: loss 5.6740522384643555\n",
            "e 1/10 i 13145/40794: loss 6.263285160064697\n",
            "e 1/10 i 13146/40794: loss 5.821282863616943\n",
            "e 1/10 i 13147/40794: loss 6.0507941246032715\n",
            "e 1/10 i 13148/40794: loss 5.859585762023926\n",
            "e 1/10 i 13149/40794: loss 5.769762992858887\n",
            "e 1/10 i 13150/40794: loss 6.190242767333984\n",
            "e 1/10 i 13151/40794: loss 6.024700164794922\n",
            "e 1/10 i 13152/40794: loss 5.824408531188965\n",
            "e 1/10 i 13153/40794: loss 5.865468978881836\n",
            "e 1/10 i 13154/40794: loss 6.266377925872803\n",
            "e 1/10 i 13155/40794: loss 5.985100269317627\n",
            "e 1/10 i 13156/40794: loss 6.039255142211914\n",
            "e 1/10 i 13157/40794: loss 6.1372389793396\n",
            "e 1/10 i 13158/40794: loss 5.894344806671143\n",
            "e 1/10 i 13159/40794: loss 5.713138103485107\n",
            "e 1/10 i 13160/40794: loss 5.73859167098999\n",
            "e 1/10 i 13161/40794: loss 6.3445611000061035\n",
            "e 1/10 i 13162/40794: loss 6.191722869873047\n",
            "e 1/10 i 13163/40794: loss 6.006819248199463\n",
            "e 1/10 i 13164/40794: loss 6.059323787689209\n",
            "e 1/10 i 13165/40794: loss 5.917147636413574\n",
            "e 1/10 i 13166/40794: loss 6.050454139709473\n",
            "e 1/10 i 13167/40794: loss 5.79983377456665\n",
            "e 1/10 i 13168/40794: loss 6.174323081970215\n",
            "e 1/10 i 13169/40794: loss 6.069729328155518\n",
            "e 1/10 i 13170/40794: loss 5.802696704864502\n",
            "e 1/10 i 13171/40794: loss 5.770742416381836\n",
            "e 1/10 i 13172/40794: loss 5.849417209625244\n",
            "e 1/10 i 13173/40794: loss 6.235871315002441\n",
            "e 1/10 i 13174/40794: loss 5.9533209800720215\n",
            "e 1/10 i 13175/40794: loss 5.963146686553955\n",
            "e 1/10 i 13176/40794: loss 5.943521976470947\n",
            "e 1/10 i 13177/40794: loss 5.9470624923706055\n",
            "e 1/10 i 13178/40794: loss 5.8295722007751465\n",
            "e 1/10 i 13179/40794: loss 5.952619552612305\n",
            "e 1/10 i 13180/40794: loss 6.116082668304443\n",
            "e 1/10 i 13181/40794: loss 5.981252670288086\n",
            "e 1/10 i 13182/40794: loss 5.872034072875977\n",
            "e 1/10 i 13183/40794: loss 6.1551995277404785\n",
            "e 1/10 i 13184/40794: loss 5.851240158081055\n",
            "e 1/10 i 13185/40794: loss 6.097170829772949\n",
            "e 1/10 i 13186/40794: loss 6.058633804321289\n",
            "e 1/10 i 13187/40794: loss 5.824265480041504\n",
            "e 1/10 i 13188/40794: loss 5.748449325561523\n",
            "e 1/10 i 13189/40794: loss 5.829026699066162\n",
            "e 1/10 i 13190/40794: loss 5.761041641235352\n",
            "e 1/10 i 13191/40794: loss 6.154729843139648\n",
            "e 1/10 i 13192/40794: loss 5.688142776489258\n",
            "e 1/10 i 13193/40794: loss 6.1780571937561035\n",
            "e 1/10 i 13194/40794: loss 5.979067802429199\n",
            "e 1/10 i 13195/40794: loss 5.899964332580566\n",
            "e 1/10 i 13196/40794: loss 6.170419692993164\n",
            "e 1/10 i 13197/40794: loss 5.596829891204834\n",
            "e 1/10 i 13198/40794: loss 6.159374237060547\n",
            "e 1/10 i 13199/40794: loss 6.219325542449951\n",
            "e 1/10 i 13200/40794: loss 5.671760559082031\n",
            "e 1/10 i 13201/40794: loss 5.718792915344238\n",
            "e 1/10 i 13202/40794: loss 6.092358589172363\n",
            "e 1/10 i 13203/40794: loss 5.69407844543457\n",
            "e 1/10 i 13204/40794: loss 6.022757530212402\n",
            "e 1/10 i 13205/40794: loss 5.571567535400391\n",
            "e 1/10 i 13206/40794: loss 5.772933483123779\n",
            "e 1/10 i 13207/40794: loss 5.850841522216797\n",
            "e 1/10 i 13208/40794: loss 6.12224006652832\n",
            "e 1/10 i 13209/40794: loss 6.077507972717285\n",
            "e 1/10 i 13210/40794: loss 5.5371575355529785\n",
            "e 1/10 i 13211/40794: loss 5.7928385734558105\n",
            "e 1/10 i 13212/40794: loss 6.083638668060303\n",
            "e 1/10 i 13213/40794: loss 5.914389133453369\n",
            "e 1/10 i 13214/40794: loss 5.65317964553833\n",
            "e 1/10 i 13215/40794: loss 5.647566795349121\n",
            "e 1/10 i 13216/40794: loss 5.881316661834717\n",
            "e 1/10 i 13217/40794: loss 5.830893516540527\n",
            "e 1/10 i 13218/40794: loss 5.998257160186768\n",
            "e 1/10 i 13219/40794: loss 6.0040283203125\n",
            "e 1/10 i 13220/40794: loss 6.16155481338501\n",
            "e 1/10 i 13221/40794: loss 5.744562149047852\n",
            "e 1/10 i 13222/40794: loss 6.037456512451172\n",
            "e 1/10 i 13223/40794: loss 6.009253978729248\n",
            "e 1/10 i 13224/40794: loss 5.895260334014893\n",
            "e 1/10 i 13225/40794: loss 5.916659355163574\n",
            "e 1/10 i 13226/40794: loss 5.696511268615723\n",
            "e 1/10 i 13227/40794: loss 5.86007022857666\n",
            "e 1/10 i 13228/40794: loss 6.055270671844482\n",
            "e 1/10 i 13229/40794: loss 6.089781284332275\n",
            "e 1/10 i 13230/40794: loss 5.7416229248046875\n",
            "e 1/10 i 13231/40794: loss 5.695487976074219\n",
            "e 1/10 i 13232/40794: loss 5.637598514556885\n",
            "e 1/10 i 13233/40794: loss 5.971075057983398\n",
            "e 1/10 i 13234/40794: loss 5.811949253082275\n",
            "e 1/10 i 13235/40794: loss 5.970571041107178\n",
            "e 1/10 i 13236/40794: loss 6.081353664398193\n",
            "e 1/10 i 13237/40794: loss 6.12222146987915\n",
            "e 1/10 i 13238/40794: loss 5.859766006469727\n",
            "e 1/10 i 13239/40794: loss 5.992386817932129\n",
            "e 1/10 i 13240/40794: loss 5.793829917907715\n",
            "e 1/10 i 13241/40794: loss 5.717054843902588\n",
            "e 1/10 i 13242/40794: loss 6.3461480140686035\n",
            "e 1/10 i 13243/40794: loss 5.917419910430908\n",
            "e 1/10 i 13244/40794: loss 6.196712970733643\n",
            "e 1/10 i 13245/40794: loss 5.646887302398682\n",
            "e 1/10 i 13246/40794: loss 6.074565410614014\n",
            "e 1/10 i 13247/40794: loss 5.915854454040527\n",
            "e 1/10 i 13248/40794: loss 5.809142112731934\n",
            "e 1/10 i 13249/40794: loss 6.3177170753479\n",
            "e 1/10 i 13250/40794: loss 5.740756034851074\n",
            "e 1/10 i 13251/40794: loss 6.011826038360596\n",
            "e 1/10 i 13252/40794: loss 5.929818630218506\n",
            "e 1/10 i 13253/40794: loss 6.289586067199707\n",
            "e 1/10 i 13254/40794: loss 6.106316089630127\n",
            "e 1/10 i 13255/40794: loss 6.187828540802002\n",
            "e 1/10 i 13256/40794: loss 5.809402942657471\n",
            "e 1/10 i 13257/40794: loss 6.086320400238037\n",
            "e 1/10 i 13258/40794: loss 5.969404220581055\n",
            "e 1/10 i 13259/40794: loss 5.908184051513672\n",
            "e 1/10 i 13260/40794: loss 5.717463493347168\n",
            "e 1/10 i 13261/40794: loss 5.857806205749512\n",
            "e 1/10 i 13262/40794: loss 6.230897903442383\n",
            "e 1/10 i 13263/40794: loss 6.012205600738525\n",
            "e 1/10 i 13264/40794: loss 5.4717230796813965\n",
            "e 1/10 i 13265/40794: loss 5.551978588104248\n",
            "e 1/10 i 13266/40794: loss 6.280490875244141\n",
            "e 1/10 i 13267/40794: loss 5.526600360870361\n",
            "e 1/10 i 13268/40794: loss 6.082686901092529\n",
            "e 1/10 i 13269/40794: loss 5.988920211791992\n",
            "e 1/10 i 13270/40794: loss 6.1359171867370605\n",
            "e 1/10 i 13271/40794: loss 6.045076370239258\n",
            "e 1/10 i 13272/40794: loss 6.032094955444336\n",
            "e 1/10 i 13273/40794: loss 5.773186206817627\n",
            "e 1/10 i 13274/40794: loss 6.080836296081543\n",
            "e 1/10 i 13275/40794: loss 5.920092582702637\n",
            "e 1/10 i 13276/40794: loss 6.128776550292969\n",
            "e 1/10 i 13277/40794: loss 6.3050079345703125\n",
            "e 1/10 i 13278/40794: loss 5.936901092529297\n",
            "e 1/10 i 13279/40794: loss 5.647627830505371\n",
            "e 1/10 i 13280/40794: loss 5.919780254364014\n",
            "e 1/10 i 13281/40794: loss 5.722741603851318\n",
            "e 1/10 i 13282/40794: loss 5.841254234313965\n",
            "e 1/10 i 13283/40794: loss 5.9508376121521\n",
            "e 1/10 i 13284/40794: loss 6.075979232788086\n",
            "e 1/10 i 13285/40794: loss 5.920807838439941\n",
            "e 1/10 i 13286/40794: loss 6.091976642608643\n",
            "e 1/10 i 13287/40794: loss 5.832239627838135\n",
            "e 1/10 i 13288/40794: loss 5.92501163482666\n",
            "e 1/10 i 13289/40794: loss 6.266896724700928\n",
            "e 1/10 i 13290/40794: loss 6.162747859954834\n",
            "e 1/10 i 13291/40794: loss 6.174936294555664\n",
            "e 1/10 i 13292/40794: loss 5.884623050689697\n",
            "e 1/10 i 13293/40794: loss 6.056612014770508\n",
            "e 1/10 i 13294/40794: loss 5.796721458435059\n",
            "e 1/10 i 13295/40794: loss 5.671576023101807\n",
            "e 1/10 i 13296/40794: loss 6.042635917663574\n",
            "e 1/10 i 13297/40794: loss 5.726457595825195\n",
            "e 1/10 i 13298/40794: loss 5.738786220550537\n",
            "e 1/10 i 13299/40794: loss 6.043268203735352\n",
            "e 1/10 i 13300/40794: loss 5.683108806610107\n",
            "e 1/10 i 13301/40794: loss 5.981614589691162\n",
            "e 1/10 i 13302/40794: loss 5.902276039123535\n",
            "e 1/10 i 13303/40794: loss 5.862050533294678\n",
            "e 1/10 i 13304/40794: loss 6.021812438964844\n",
            "e 1/10 i 13305/40794: loss 5.877382755279541\n",
            "e 1/10 i 13306/40794: loss 5.777733325958252\n",
            "e 1/10 i 13307/40794: loss 6.149767875671387\n",
            "e 1/10 i 13308/40794: loss 5.749552249908447\n",
            "e 1/10 i 13309/40794: loss 5.932770252227783\n",
            "e 1/10 i 13310/40794: loss 5.927559852600098\n",
            "e 1/10 i 13311/40794: loss 6.114346504211426\n",
            "e 1/10 i 13312/40794: loss 5.680768966674805\n",
            "e 1/10 i 13313/40794: loss 5.8438801765441895\n",
            "e 1/10 i 13314/40794: loss 6.0460429191589355\n",
            "e 1/10 i 13315/40794: loss 6.056334495544434\n",
            "e 1/10 i 13316/40794: loss 5.903265476226807\n",
            "e 1/10 i 13317/40794: loss 5.835903167724609\n",
            "e 1/10 i 13318/40794: loss 6.0604248046875\n",
            "e 1/10 i 13319/40794: loss 5.865985870361328\n",
            "e 1/10 i 13320/40794: loss 5.892660140991211\n",
            "e 1/10 i 13321/40794: loss 5.6401448249816895\n",
            "e 1/10 i 13322/40794: loss 6.233494758605957\n",
            "e 1/10 i 13323/40794: loss 5.907135963439941\n",
            "e 1/10 i 13324/40794: loss 6.152442932128906\n",
            "e 1/10 i 13325/40794: loss 5.930171966552734\n",
            "e 1/10 i 13326/40794: loss 6.013753890991211\n",
            "e 1/10 i 13327/40794: loss 5.95339298248291\n",
            "e 1/10 i 13328/40794: loss 5.86839485168457\n",
            "e 1/10 i 13329/40794: loss 5.890440464019775\n",
            "e 1/10 i 13330/40794: loss 5.785932540893555\n",
            "e 1/10 i 13331/40794: loss 5.618786811828613\n",
            "e 1/10 i 13332/40794: loss 5.962985992431641\n",
            "e 1/10 i 13333/40794: loss 5.917801856994629\n",
            "e 1/10 i 13334/40794: loss 6.195992946624756\n",
            "e 1/10 i 13335/40794: loss 5.849274158477783\n",
            "e 1/10 i 13336/40794: loss 6.314294338226318\n",
            "e 1/10 i 13337/40794: loss 5.885567665100098\n",
            "e 1/10 i 13338/40794: loss 5.961333751678467\n",
            "e 1/10 i 13339/40794: loss 6.101278781890869\n",
            "e 1/10 i 13340/40794: loss 6.2061944007873535\n",
            "e 1/10 i 13341/40794: loss 5.95302152633667\n",
            "e 1/10 i 13342/40794: loss 5.990087032318115\n",
            "e 1/10 i 13343/40794: loss 5.550353527069092\n",
            "e 1/10 i 13344/40794: loss 5.936525821685791\n",
            "e 1/10 i 13345/40794: loss 5.997885704040527\n",
            "e 1/10 i 13346/40794: loss 5.893349647521973\n",
            "e 1/10 i 13347/40794: loss 6.107967853546143\n",
            "e 1/10 i 13348/40794: loss 5.944253444671631\n",
            "e 1/10 i 13349/40794: loss 5.45913028717041\n",
            "e 1/10 i 13350/40794: loss 6.147777080535889\n",
            "e 1/10 i 13351/40794: loss 5.770310878753662\n",
            "e 1/10 i 13352/40794: loss 5.575570583343506\n",
            "e 1/10 i 13353/40794: loss 5.877614974975586\n",
            "e 1/10 i 13354/40794: loss 5.892637729644775\n",
            "e 1/10 i 13355/40794: loss 5.908988952636719\n",
            "e 1/10 i 13356/40794: loss 5.942355632781982\n",
            "e 1/10 i 13357/40794: loss 5.704117298126221\n",
            "e 1/10 i 13358/40794: loss 5.877682685852051\n",
            "e 1/10 i 13359/40794: loss 5.97896146774292\n",
            "e 1/10 i 13360/40794: loss 5.954015731811523\n",
            "e 1/10 i 13361/40794: loss 6.097386360168457\n",
            "e 1/10 i 13362/40794: loss 5.6947431564331055\n",
            "e 1/10 i 13363/40794: loss 5.570542335510254\n",
            "e 1/10 i 13364/40794: loss 5.819849014282227\n",
            "e 1/10 i 13365/40794: loss 5.671269416809082\n",
            "e 1/10 i 13366/40794: loss 5.971615314483643\n",
            "e 1/10 i 13367/40794: loss 6.113130569458008\n",
            "e 1/10 i 13368/40794: loss 6.15421724319458\n",
            "e 1/10 i 13369/40794: loss 5.625880718231201\n",
            "e 1/10 i 13370/40794: loss 5.9931135177612305\n",
            "e 1/10 i 13371/40794: loss 6.167296409606934\n",
            "e 1/10 i 13372/40794: loss 5.713301181793213\n",
            "e 1/10 i 13373/40794: loss 5.641348361968994\n",
            "e 1/10 i 13374/40794: loss 5.697174549102783\n",
            "e 1/10 i 13375/40794: loss 6.031254768371582\n",
            "e 1/10 i 13376/40794: loss 5.8029608726501465\n",
            "e 1/10 i 13377/40794: loss 5.8702921867370605\n",
            "e 1/10 i 13378/40794: loss 6.070977687835693\n",
            "e 1/10 i 13379/40794: loss 5.819393157958984\n",
            "e 1/10 i 13380/40794: loss 6.124203681945801\n",
            "e 1/10 i 13381/40794: loss 5.719698905944824\n",
            "e 1/10 i 13382/40794: loss 5.583865642547607\n",
            "e 1/10 i 13383/40794: loss 5.6339240074157715\n",
            "e 1/10 i 13384/40794: loss 5.795247554779053\n",
            "e 1/10 i 13385/40794: loss 6.123083114624023\n",
            "e 1/10 i 13386/40794: loss 5.990925312042236\n",
            "e 1/10 i 13387/40794: loss 6.0186767578125\n",
            "e 1/10 i 13388/40794: loss 6.071721076965332\n",
            "e 1/10 i 13389/40794: loss 5.642242908477783\n",
            "e 1/10 i 13390/40794: loss 5.872069835662842\n",
            "e 1/10 i 13391/40794: loss 5.582359790802002\n",
            "e 1/10 i 13392/40794: loss 5.498307704925537\n",
            "e 1/10 i 13393/40794: loss 6.1137542724609375\n",
            "e 1/10 i 13394/40794: loss 6.2927093505859375\n",
            "e 1/10 i 13395/40794: loss 6.016613006591797\n",
            "e 1/10 i 13396/40794: loss 5.8425726890563965\n",
            "e 1/10 i 13397/40794: loss 6.231092929840088\n",
            "e 1/10 i 13398/40794: loss 5.484681129455566\n",
            "e 1/10 i 13399/40794: loss 5.872102737426758\n",
            "e 1/10 i 13400/40794: loss 6.054785251617432\n",
            "e 1/10 i 13401/40794: loss 6.0984086990356445\n",
            "e 1/10 i 13402/40794: loss 5.785967826843262\n",
            "e 1/10 i 13403/40794: loss 5.814598083496094\n",
            "e 1/10 i 13404/40794: loss 5.853370666503906\n",
            "e 1/10 i 13405/40794: loss 5.658247470855713\n",
            "e 1/10 i 13406/40794: loss 5.7379150390625\n",
            "e 1/10 i 13407/40794: loss 5.732456684112549\n",
            "e 1/10 i 13408/40794: loss 5.8543572425842285\n",
            "e 1/10 i 13409/40794: loss 5.811584949493408\n",
            "e 1/10 i 13410/40794: loss 5.854883670806885\n",
            "e 1/10 i 13411/40794: loss 5.865034580230713\n",
            "e 1/10 i 13412/40794: loss 6.012250900268555\n",
            "e 1/10 i 13413/40794: loss 5.978525638580322\n",
            "e 1/10 i 13414/40794: loss 6.241812229156494\n",
            "e 1/10 i 13415/40794: loss 5.991440296173096\n",
            "e 1/10 i 13416/40794: loss 5.560614585876465\n",
            "e 1/10 i 13417/40794: loss 6.13864278793335\n",
            "e 1/10 i 13418/40794: loss 5.659998893737793\n",
            "e 1/10 i 13419/40794: loss 6.054389953613281\n",
            "e 1/10 i 13420/40794: loss 6.011810779571533\n",
            "e 1/10 i 13421/40794: loss 5.964847564697266\n",
            "e 1/10 i 13422/40794: loss 6.235938549041748\n",
            "e 1/10 i 13423/40794: loss 5.730781555175781\n",
            "e 1/10 i 13424/40794: loss 5.871108531951904\n",
            "e 1/10 i 13425/40794: loss 5.753612041473389\n",
            "e 1/10 i 13426/40794: loss 6.17482852935791\n",
            "e 1/10 i 13427/40794: loss 5.986756324768066\n",
            "e 1/10 i 13428/40794: loss 6.107274055480957\n",
            "e 1/10 i 13429/40794: loss 5.831081867218018\n",
            "e 1/10 i 13430/40794: loss 5.743831634521484\n",
            "e 1/10 i 13431/40794: loss 5.685283184051514\n",
            "e 1/10 i 13432/40794: loss 6.289841175079346\n",
            "e 1/10 i 13433/40794: loss 6.138438701629639\n",
            "e 1/10 i 13434/40794: loss 6.31934928894043\n",
            "e 1/10 i 13435/40794: loss 6.08644962310791\n",
            "e 1/10 i 13436/40794: loss 6.048920631408691\n",
            "e 1/10 i 13437/40794: loss 5.983541965484619\n",
            "e 1/10 i 13438/40794: loss 6.0004963874816895\n",
            "e 1/10 i 13439/40794: loss 6.176607608795166\n",
            "e 1/10 i 13440/40794: loss 6.1712141036987305\n",
            "e 1/10 i 13441/40794: loss 5.776529312133789\n",
            "e 1/10 i 13442/40794: loss 5.866884708404541\n",
            "e 1/10 i 13443/40794: loss 6.028636932373047\n",
            "e 1/10 i 13444/40794: loss 6.096012592315674\n",
            "e 1/10 i 13445/40794: loss 6.440447807312012\n",
            "e 1/10 i 13446/40794: loss 6.125281810760498\n",
            "e 1/10 i 13447/40794: loss 5.921413421630859\n",
            "e 1/10 i 13448/40794: loss 6.000631332397461\n",
            "e 1/10 i 13449/40794: loss 6.026566028594971\n",
            "e 1/10 i 13450/40794: loss 5.692463397979736\n",
            "e 1/10 i 13451/40794: loss 5.693418502807617\n",
            "e 1/10 i 13452/40794: loss 5.852956771850586\n",
            "e 1/10 i 13453/40794: loss 6.037662982940674\n",
            "e 1/10 i 13454/40794: loss 5.998984336853027\n",
            "e 1/10 i 13455/40794: loss 5.833935260772705\n",
            "e 1/10 i 13456/40794: loss 6.25480842590332\n",
            "e 1/10 i 13457/40794: loss 5.789512634277344\n",
            "e 1/10 i 13458/40794: loss 5.655745506286621\n",
            "e 1/10 i 13459/40794: loss 6.057545185089111\n",
            "e 1/10 i 13460/40794: loss 5.975772857666016\n",
            "e 1/10 i 13461/40794: loss 6.1413750648498535\n",
            "e 1/10 i 13462/40794: loss 5.838464736938477\n",
            "e 1/10 i 13463/40794: loss 6.118927001953125\n",
            "e 1/10 i 13464/40794: loss 5.697420597076416\n",
            "e 1/10 i 13465/40794: loss 6.197436332702637\n",
            "e 1/10 i 13466/40794: loss 5.929235458374023\n",
            "e 1/10 i 13467/40794: loss 6.061391353607178\n",
            "e 1/10 i 13468/40794: loss 6.1175360679626465\n",
            "e 1/10 i 13469/40794: loss 5.923948764801025\n",
            "e 1/10 i 13470/40794: loss 6.069460868835449\n",
            "e 1/10 i 13471/40794: loss 6.080051422119141\n",
            "e 1/10 i 13472/40794: loss 5.814333915710449\n",
            "e 1/10 i 13473/40794: loss 5.781632900238037\n",
            "e 1/10 i 13474/40794: loss 6.211580753326416\n",
            "e 1/10 i 13475/40794: loss 5.714033603668213\n",
            "e 1/10 i 13476/40794: loss 5.843086242675781\n",
            "e 1/10 i 13477/40794: loss 5.912263870239258\n",
            "e 1/10 i 13478/40794: loss 5.8292036056518555\n",
            "e 1/10 i 13479/40794: loss 5.912103652954102\n",
            "e 1/10 i 13480/40794: loss 6.151088714599609\n",
            "e 1/10 i 13481/40794: loss 5.682984828948975\n",
            "e 1/10 i 13482/40794: loss 6.347462177276611\n",
            "e 1/10 i 13483/40794: loss 5.922943115234375\n",
            "e 1/10 i 13484/40794: loss 5.985496520996094\n",
            "e 1/10 i 13485/40794: loss 5.716318607330322\n",
            "e 1/10 i 13486/40794: loss 6.138854503631592\n",
            "e 1/10 i 13487/40794: loss 5.620758056640625\n",
            "e 1/10 i 13488/40794: loss 5.89173698425293\n",
            "e 1/10 i 13489/40794: loss 6.117451190948486\n",
            "e 1/10 i 13490/40794: loss 5.808340072631836\n",
            "e 1/10 i 13491/40794: loss 5.640871047973633\n",
            "e 1/10 i 13492/40794: loss 5.6819844245910645\n",
            "e 1/10 i 13493/40794: loss 6.319681644439697\n",
            "e 1/10 i 13494/40794: loss 5.919240951538086\n",
            "e 1/10 i 13495/40794: loss 5.992569923400879\n",
            "e 1/10 i 13496/40794: loss 5.739316940307617\n",
            "e 1/10 i 13497/40794: loss 5.760797500610352\n",
            "e 1/10 i 13498/40794: loss 5.757920265197754\n",
            "e 1/10 i 13499/40794: loss 6.214994430541992\n",
            "e 1/10 i 13500/40794: loss 6.301848888397217\n",
            "e 1/10 i 13501/40794: loss 5.981390953063965\n",
            "e 1/10 i 13502/40794: loss 5.952769756317139\n",
            "e 1/10 i 13503/40794: loss 5.894275188446045\n",
            "e 1/10 i 13504/40794: loss 5.819064617156982\n",
            "e 1/10 i 13505/40794: loss 5.550381183624268\n",
            "e 1/10 i 13506/40794: loss 5.958958148956299\n",
            "e 1/10 i 13507/40794: loss 5.830240726470947\n",
            "e 1/10 i 13508/40794: loss 6.303709030151367\n",
            "e 1/10 i 13509/40794: loss 6.078696250915527\n",
            "e 1/10 i 13510/40794: loss 5.933072566986084\n",
            "e 1/10 i 13511/40794: loss 6.151335716247559\n",
            "e 1/10 i 13512/40794: loss 5.972780227661133\n",
            "e 1/10 i 13513/40794: loss 5.700582504272461\n",
            "e 1/10 i 13514/40794: loss 5.821657657623291\n",
            "e 1/10 i 13515/40794: loss 5.813220024108887\n",
            "e 1/10 i 13516/40794: loss 5.987849712371826\n",
            "e 1/10 i 13517/40794: loss 5.8862152099609375\n",
            "e 1/10 i 13518/40794: loss 5.710163593292236\n",
            "e 1/10 i 13519/40794: loss 6.075615882873535\n",
            "e 1/10 i 13520/40794: loss 5.79745626449585\n",
            "e 1/10 i 13521/40794: loss 5.994246006011963\n",
            "e 1/10 i 13522/40794: loss 6.197350978851318\n",
            "e 1/10 i 13523/40794: loss 6.008541107177734\n",
            "e 1/10 i 13524/40794: loss 6.100295543670654\n",
            "e 1/10 i 13525/40794: loss 6.21822452545166\n",
            "e 1/10 i 13526/40794: loss 5.608006954193115\n",
            "e 1/10 i 13527/40794: loss 6.215085983276367\n",
            "e 1/10 i 13528/40794: loss 5.916143894195557\n",
            "e 1/10 i 13529/40794: loss 5.808862209320068\n",
            "e 1/10 i 13530/40794: loss 6.04042387008667\n",
            "e 1/10 i 13531/40794: loss 5.755739688873291\n",
            "e 1/10 i 13532/40794: loss 6.043081283569336\n",
            "e 1/10 i 13533/40794: loss 6.0381340980529785\n",
            "e 1/10 i 13534/40794: loss 6.272897720336914\n",
            "e 1/10 i 13535/40794: loss 5.712646484375\n",
            "e 1/10 i 13536/40794: loss 6.10978364944458\n",
            "e 1/10 i 13537/40794: loss 5.797458648681641\n",
            "e 1/10 i 13538/40794: loss 5.859915256500244\n",
            "e 1/10 i 13539/40794: loss 6.080765724182129\n",
            "e 1/10 i 13540/40794: loss 5.7733330726623535\n",
            "e 1/10 i 13541/40794: loss 5.367447853088379\n",
            "e 1/10 i 13542/40794: loss 6.121584892272949\n",
            "e 1/10 i 13543/40794: loss 5.498909950256348\n",
            "e 1/10 i 13544/40794: loss 5.802112579345703\n",
            "e 1/10 i 13545/40794: loss 5.568840980529785\n",
            "e 1/10 i 13546/40794: loss 5.789298057556152\n",
            "e 1/10 i 13547/40794: loss 5.9293293952941895\n",
            "e 1/10 i 13548/40794: loss 6.0220818519592285\n",
            "e 1/10 i 13549/40794: loss 5.730858325958252\n",
            "e 1/10 i 13550/40794: loss 6.091024875640869\n",
            "e 1/10 i 13551/40794: loss 5.904583930969238\n",
            "e 1/10 i 13552/40794: loss 5.64036226272583\n",
            "e 1/10 i 13553/40794: loss 5.931107997894287\n",
            "e 1/10 i 13554/40794: loss 6.069060325622559\n",
            "e 1/10 i 13555/40794: loss 5.984209060668945\n",
            "e 1/10 i 13556/40794: loss 5.956901550292969\n",
            "e 1/10 i 13557/40794: loss 5.7843780517578125\n",
            "e 1/10 i 13558/40794: loss 5.73969841003418\n",
            "e 1/10 i 13559/40794: loss 5.529876232147217\n",
            "e 1/10 i 13560/40794: loss 5.938328742980957\n",
            "e 1/10 i 13561/40794: loss 5.9517822265625\n",
            "e 1/10 i 13562/40794: loss 5.607973098754883\n",
            "e 1/10 i 13563/40794: loss 5.810696601867676\n",
            "e 1/10 i 13564/40794: loss 5.679717540740967\n",
            "e 1/10 i 13565/40794: loss 5.767418384552002\n",
            "e 1/10 i 13566/40794: loss 5.940831661224365\n",
            "e 1/10 i 13567/40794: loss 5.828200817108154\n",
            "e 1/10 i 13568/40794: loss 5.936695098876953\n",
            "e 1/10 i 13569/40794: loss 5.695756435394287\n",
            "e 1/10 i 13570/40794: loss 6.041511058807373\n",
            "e 1/10 i 13571/40794: loss 6.258368015289307\n",
            "e 1/10 i 13572/40794: loss 6.030280590057373\n",
            "e 1/10 i 13573/40794: loss 5.922329425811768\n",
            "e 1/10 i 13574/40794: loss 6.210906028747559\n",
            "e 1/10 i 13575/40794: loss 5.482311248779297\n",
            "e 1/10 i 13576/40794: loss 5.812568664550781\n",
            "e 1/10 i 13577/40794: loss 5.88002872467041\n",
            "e 1/10 i 13578/40794: loss 6.103015899658203\n",
            "e 1/10 i 13579/40794: loss 5.851751327514648\n",
            "e 1/10 i 13580/40794: loss 6.1046857833862305\n",
            "e 1/10 i 13581/40794: loss 5.933180809020996\n",
            "e 1/10 i 13582/40794: loss 6.033787250518799\n",
            "e 1/10 i 13583/40794: loss 6.050658226013184\n",
            "e 1/10 i 13584/40794: loss 5.853733062744141\n",
            "e 1/10 i 13585/40794: loss 5.863339900970459\n",
            "e 1/10 i 13586/40794: loss 5.566461086273193\n",
            "e 1/10 i 13587/40794: loss 6.233148097991943\n",
            "e 1/10 i 13588/40794: loss 6.130949020385742\n",
            "e 1/10 i 13589/40794: loss 5.663806915283203\n",
            "e 1/10 i 13590/40794: loss 6.024052143096924\n",
            "e 1/10 i 13591/40794: loss 5.9639410972595215\n",
            "e 1/10 i 13592/40794: loss 5.983193874359131\n",
            "e 1/10 i 13593/40794: loss 6.171990394592285\n",
            "e 1/10 i 13594/40794: loss 5.870319843292236\n",
            "e 1/10 i 13595/40794: loss 5.893072128295898\n",
            "e 1/10 i 13596/40794: loss 5.923060417175293\n",
            "e 1/10 i 13597/40794: loss 6.141888618469238\n",
            "e 1/10 i 13598/40794: loss 6.142223358154297\n",
            "e 1/10 i 13599/40794: loss 6.238025188446045\n",
            "e 1/10 i 13600/40794: loss 5.951918125152588\n",
            "e 1/10 i 13601/40794: loss 5.75864839553833\n",
            "e 1/10 i 13602/40794: loss 5.891840934753418\n",
            "e 1/10 i 13603/40794: loss 6.068751811981201\n",
            "e 1/10 i 13604/40794: loss 6.005294322967529\n",
            "e 1/10 i 13605/40794: loss 5.969870090484619\n",
            "e 1/10 i 13606/40794: loss 6.266565799713135\n",
            "e 1/10 i 13607/40794: loss 6.361520767211914\n",
            "e 1/10 i 13608/40794: loss 5.972002029418945\n",
            "e 1/10 i 13609/40794: loss 5.899220943450928\n",
            "e 1/10 i 13610/40794: loss 5.8702569007873535\n",
            "e 1/10 i 13611/40794: loss 5.955003261566162\n",
            "e 1/10 i 13612/40794: loss 6.2378106117248535\n",
            "e 1/10 i 13613/40794: loss 5.98925256729126\n",
            "e 1/10 i 13614/40794: loss 6.135321617126465\n",
            "e 1/10 i 13615/40794: loss 5.915283679962158\n",
            "e 1/10 i 13616/40794: loss 5.562840461730957\n",
            "e 1/10 i 13617/40794: loss 6.137365341186523\n",
            "e 1/10 i 13618/40794: loss 6.108015537261963\n",
            "e 1/10 i 13619/40794: loss 5.9815592765808105\n",
            "e 1/10 i 13620/40794: loss 5.846030235290527\n",
            "e 1/10 i 13621/40794: loss 6.112064361572266\n",
            "e 1/10 i 13622/40794: loss 6.0667243003845215\n",
            "e 1/10 i 13623/40794: loss 5.928872585296631\n",
            "e 1/10 i 13624/40794: loss 5.998857498168945\n",
            "e 1/10 i 13625/40794: loss 5.629382610321045\n",
            "e 1/10 i 13626/40794: loss 5.934939384460449\n",
            "e 1/10 i 13627/40794: loss 6.246857166290283\n",
            "e 1/10 i 13628/40794: loss 6.066708564758301\n",
            "e 1/10 i 13629/40794: loss 5.862893581390381\n",
            "e 1/10 i 13630/40794: loss 5.935471057891846\n",
            "e 1/10 i 13631/40794: loss 6.258564472198486\n",
            "e 1/10 i 13632/40794: loss 5.8357930183410645\n",
            "e 1/10 i 13633/40794: loss 6.17543363571167\n",
            "e 1/10 i 13634/40794: loss 6.141202449798584\n",
            "e 1/10 i 13635/40794: loss 5.881568908691406\n",
            "e 1/10 i 13636/40794: loss 6.034545421600342\n",
            "e 1/10 i 13637/40794: loss 6.317471981048584\n",
            "e 1/10 i 13638/40794: loss 5.853150367736816\n",
            "e 1/10 i 13639/40794: loss 5.918216228485107\n",
            "e 1/10 i 13640/40794: loss 5.912233352661133\n",
            "e 1/10 i 13641/40794: loss 5.983474254608154\n",
            "e 1/10 i 13642/40794: loss 6.328381538391113\n",
            "e 1/10 i 13643/40794: loss 5.9068193435668945\n",
            "e 1/10 i 13644/40794: loss 6.113300800323486\n",
            "e 1/10 i 13645/40794: loss 5.680513858795166\n",
            "e 1/10 i 13646/40794: loss 5.834849834442139\n",
            "e 1/10 i 13647/40794: loss 5.696509838104248\n",
            "e 1/10 i 13648/40794: loss 5.83676815032959\n",
            "e 1/10 i 13649/40794: loss 6.006689548492432\n",
            "e 1/10 i 13650/40794: loss 6.114469051361084\n",
            "e 1/10 i 13651/40794: loss 5.7173309326171875\n",
            "e 1/10 i 13652/40794: loss 6.227343559265137\n",
            "e 1/10 i 13653/40794: loss 5.983293533325195\n",
            "e 1/10 i 13654/40794: loss 5.786922931671143\n",
            "e 1/10 i 13655/40794: loss 5.8658318519592285\n",
            "e 1/10 i 13656/40794: loss 6.0026679039001465\n",
            "e 1/10 i 13657/40794: loss 6.131359100341797\n",
            "e 1/10 i 13658/40794: loss 6.000435829162598\n",
            "e 1/10 i 13659/40794: loss 6.188192367553711\n",
            "e 1/10 i 13660/40794: loss 5.755074977874756\n",
            "e 1/10 i 13661/40794: loss 6.096349716186523\n",
            "e 1/10 i 13662/40794: loss 5.871776580810547\n",
            "e 1/10 i 13663/40794: loss 6.094893455505371\n",
            "e 1/10 i 13664/40794: loss 5.869112491607666\n",
            "e 1/10 i 13665/40794: loss 6.159936428070068\n",
            "e 1/10 i 13666/40794: loss 5.878753662109375\n",
            "e 1/10 i 13667/40794: loss 5.992940902709961\n",
            "e 1/10 i 13668/40794: loss 6.248488903045654\n",
            "e 1/10 i 13669/40794: loss 5.617208003997803\n",
            "e 1/10 i 13670/40794: loss 6.157894611358643\n",
            "e 1/10 i 13671/40794: loss 5.945772647857666\n",
            "e 1/10 i 13672/40794: loss 5.8347978591918945\n",
            "e 1/10 i 13673/40794: loss 6.307882308959961\n",
            "e 1/10 i 13674/40794: loss 5.867771148681641\n",
            "e 1/10 i 13675/40794: loss 5.903379917144775\n",
            "e 1/10 i 13676/40794: loss 6.029803276062012\n",
            "e 1/10 i 13677/40794: loss 6.690673828125\n",
            "e 1/10 i 13678/40794: loss 5.866372108459473\n",
            "e 1/10 i 13679/40794: loss 5.917886257171631\n",
            "e 1/10 i 13680/40794: loss 6.236268520355225\n",
            "e 1/10 i 13681/40794: loss 6.300858020782471\n",
            "e 1/10 i 13682/40794: loss 6.155961036682129\n",
            "e 1/10 i 13683/40794: loss 5.849274635314941\n",
            "e 1/10 i 13684/40794: loss 5.971589088439941\n",
            "e 1/10 i 13685/40794: loss 5.730284214019775\n",
            "e 1/10 i 13686/40794: loss 5.937657833099365\n",
            "e 1/10 i 13687/40794: loss 5.662478446960449\n",
            "e 1/10 i 13688/40794: loss 6.127813816070557\n",
            "e 1/10 i 13689/40794: loss 5.516815185546875\n",
            "e 1/10 i 13690/40794: loss 5.588362216949463\n",
            "e 1/10 i 13691/40794: loss 5.873518943786621\n",
            "e 1/10 i 13692/40794: loss 5.907806396484375\n",
            "e 1/10 i 13693/40794: loss 5.886744022369385\n",
            "e 1/10 i 13694/40794: loss 6.186964988708496\n",
            "e 1/10 i 13695/40794: loss 6.003538608551025\n",
            "e 1/10 i 13696/40794: loss 5.748281002044678\n",
            "e 1/10 i 13697/40794: loss 6.057342529296875\n",
            "e 1/10 i 13698/40794: loss 5.632250785827637\n",
            "e 1/10 i 13699/40794: loss 5.977252960205078\n",
            "e 1/10 i 13700/40794: loss 5.927608489990234\n",
            "e 1/10 i 13701/40794: loss 6.014866352081299\n",
            "e 1/10 i 13702/40794: loss 6.370118141174316\n",
            "e 1/10 i 13703/40794: loss 5.811447620391846\n",
            "e 1/10 i 13704/40794: loss 5.82277774810791\n",
            "e 1/10 i 13705/40794: loss 5.86223840713501\n",
            "e 1/10 i 13706/40794: loss 5.644870758056641\n",
            "e 1/10 i 13707/40794: loss 6.0846099853515625\n",
            "e 1/10 i 13708/40794: loss 6.4966325759887695\n",
            "e 1/10 i 13709/40794: loss 6.195291996002197\n",
            "e 1/10 i 13710/40794: loss 5.915795803070068\n",
            "e 1/10 i 13711/40794: loss 6.022790908813477\n",
            "e 1/10 i 13712/40794: loss 6.123643398284912\n",
            "e 1/10 i 13713/40794: loss 6.003528118133545\n",
            "e 1/10 i 13714/40794: loss 6.12057638168335\n",
            "e 1/10 i 13715/40794: loss 6.1491498947143555\n",
            "e 1/10 i 13716/40794: loss 5.951009750366211\n",
            "e 1/10 i 13717/40794: loss 6.0373005867004395\n",
            "e 1/10 i 13718/40794: loss 6.178873062133789\n",
            "e 1/10 i 13719/40794: loss 5.9517502784729\n",
            "e 1/10 i 13720/40794: loss 6.028450012207031\n",
            "e 1/10 i 13721/40794: loss 6.203874111175537\n",
            "e 1/10 i 13722/40794: loss 6.019980430603027\n",
            "e 1/10 i 13723/40794: loss 6.040775775909424\n",
            "e 1/10 i 13724/40794: loss 5.884213447570801\n",
            "e 1/10 i 13725/40794: loss 5.713975429534912\n",
            "e 1/10 i 13726/40794: loss 6.045431137084961\n",
            "e 1/10 i 13727/40794: loss 5.587596893310547\n",
            "e 1/10 i 13728/40794: loss 6.04531192779541\n",
            "e 1/10 i 13729/40794: loss 5.624828815460205\n",
            "e 1/10 i 13730/40794: loss 5.8761396408081055\n",
            "e 1/10 i 13731/40794: loss 5.857220649719238\n",
            "e 1/10 i 13732/40794: loss 5.874500751495361\n",
            "e 1/10 i 13733/40794: loss 5.944415092468262\n",
            "e 1/10 i 13734/40794: loss 6.148056983947754\n",
            "e 1/10 i 13735/40794: loss 5.959996223449707\n",
            "e 1/10 i 13736/40794: loss 5.896109580993652\n",
            "e 1/10 i 13737/40794: loss 5.738702774047852\n",
            "e 1/10 i 13738/40794: loss 5.9441070556640625\n",
            "e 1/10 i 13739/40794: loss 5.896480560302734\n",
            "e 1/10 i 13740/40794: loss 5.796946048736572\n",
            "e 1/10 i 13741/40794: loss 6.085033893585205\n",
            "e 1/10 i 13742/40794: loss 6.028933048248291\n",
            "e 1/10 i 13743/40794: loss 5.807281970977783\n",
            "e 1/10 i 13744/40794: loss 5.880186557769775\n",
            "e 1/10 i 13745/40794: loss 5.844056129455566\n",
            "e 1/10 i 13746/40794: loss 5.8272480964660645\n",
            "e 1/10 i 13747/40794: loss 5.876279830932617\n",
            "e 1/10 i 13748/40794: loss 5.7058587074279785\n",
            "e 1/10 i 13749/40794: loss 5.9747090339660645\n",
            "e 1/10 i 13750/40794: loss 5.836807727813721\n",
            "e 1/10 i 13751/40794: loss 5.837418556213379\n",
            "e 1/10 i 13752/40794: loss 5.6821370124816895\n",
            "e 1/10 i 13753/40794: loss 6.4078545570373535\n",
            "e 1/10 i 13754/40794: loss 5.660656929016113\n",
            "e 1/10 i 13755/40794: loss 5.660144805908203\n",
            "e 1/10 i 13756/40794: loss 6.021364688873291\n",
            "e 1/10 i 13757/40794: loss 6.193487644195557\n",
            "e 1/10 i 13758/40794: loss 5.947754383087158\n",
            "e 1/10 i 13759/40794: loss 5.804971694946289\n",
            "e 1/10 i 13760/40794: loss 5.793336391448975\n",
            "e 1/10 i 13761/40794: loss 5.7280073165893555\n",
            "e 1/10 i 13762/40794: loss 5.891001224517822\n",
            "e 1/10 i 13763/40794: loss 5.84981107711792\n",
            "e 1/10 i 13764/40794: loss 6.0240254402160645\n",
            "e 1/10 i 13765/40794: loss 5.988221168518066\n",
            "e 1/10 i 13766/40794: loss 5.878177165985107\n",
            "e 1/10 i 13767/40794: loss 5.729229927062988\n",
            "e 1/10 i 13768/40794: loss 5.917904376983643\n",
            "e 1/10 i 13769/40794: loss 6.0414252281188965\n",
            "e 1/10 i 13770/40794: loss 5.67004919052124\n",
            "e 1/10 i 13771/40794: loss 5.709760665893555\n",
            "e 1/10 i 13772/40794: loss 5.840551376342773\n",
            "e 1/10 i 13773/40794: loss 5.773377418518066\n",
            "e 1/10 i 13774/40794: loss 6.112135887145996\n",
            "e 1/10 i 13775/40794: loss 6.5191731452941895\n",
            "e 1/10 i 13776/40794: loss 5.869232654571533\n",
            "e 1/10 i 13777/40794: loss 5.799835205078125\n",
            "e 1/10 i 13778/40794: loss 5.983628273010254\n",
            "e 1/10 i 13779/40794: loss 5.816784381866455\n",
            "e 1/10 i 13780/40794: loss 5.696242809295654\n",
            "e 1/10 i 13781/40794: loss 5.7634100914001465\n",
            "e 1/10 i 13782/40794: loss 5.561169147491455\n",
            "e 1/10 i 13783/40794: loss 6.165641784667969\n",
            "e 1/10 i 13784/40794: loss 5.926231861114502\n",
            "e 1/10 i 13785/40794: loss 5.912359714508057\n",
            "e 1/10 i 13786/40794: loss 5.624031066894531\n",
            "e 1/10 i 13787/40794: loss 6.002354145050049\n",
            "e 1/10 i 13788/40794: loss 6.001097679138184\n",
            "e 1/10 i 13789/40794: loss 5.940179347991943\n",
            "e 1/10 i 13790/40794: loss 6.128001689910889\n",
            "e 1/10 i 13791/40794: loss 5.837147235870361\n",
            "e 1/10 i 13792/40794: loss 6.0778656005859375\n",
            "e 1/10 i 13793/40794: loss 6.1772379875183105\n",
            "e 1/10 i 13794/40794: loss 5.881435394287109\n",
            "e 1/10 i 13795/40794: loss 5.9637627601623535\n",
            "e 1/10 i 13796/40794: loss 6.096726417541504\n",
            "e 1/10 i 13797/40794: loss 6.024942874908447\n",
            "e 1/10 i 13798/40794: loss 6.270445823669434\n",
            "e 1/10 i 13799/40794: loss 6.063445091247559\n",
            "e 1/10 i 13800/40794: loss 6.066718101501465\n",
            "e 1/10 i 13801/40794: loss 6.134215354919434\n",
            "e 1/10 i 13802/40794: loss 5.802590370178223\n",
            "e 1/10 i 13803/40794: loss 5.604221820831299\n",
            "e 1/10 i 13804/40794: loss 6.125168323516846\n",
            "e 1/10 i 13805/40794: loss 5.994478225708008\n",
            "e 1/10 i 13806/40794: loss 5.930995464324951\n",
            "e 1/10 i 13807/40794: loss 6.2930450439453125\n",
            "e 1/10 i 13808/40794: loss 5.824615001678467\n",
            "e 1/10 i 13809/40794: loss 5.5756988525390625\n",
            "e 1/10 i 13810/40794: loss 6.029470920562744\n",
            "e 1/10 i 13811/40794: loss 5.618159294128418\n",
            "e 1/10 i 13812/40794: loss 5.786106109619141\n",
            "e 1/10 i 13813/40794: loss 5.761605739593506\n",
            "e 1/10 i 13814/40794: loss 5.798779010772705\n",
            "e 1/10 i 13815/40794: loss 5.93890380859375\n",
            "e 1/10 i 13816/40794: loss 5.890530586242676\n",
            "e 1/10 i 13817/40794: loss 6.25060510635376\n",
            "e 1/10 i 13818/40794: loss 5.807768821716309\n",
            "e 1/10 i 13819/40794: loss 6.239484786987305\n",
            "e 1/10 i 13820/40794: loss 5.9361395835876465\n",
            "e 1/10 i 13821/40794: loss 5.986484050750732\n",
            "e 1/10 i 13822/40794: loss 6.259551525115967\n",
            "e 1/10 i 13823/40794: loss 5.861794948577881\n",
            "e 1/10 i 13824/40794: loss 6.000126838684082\n",
            "e 1/10 i 13825/40794: loss 5.855465888977051\n",
            "e 1/10 i 13826/40794: loss 5.991711139678955\n",
            "e 1/10 i 13827/40794: loss 5.865149021148682\n",
            "e 1/10 i 13828/40794: loss 5.622243404388428\n",
            "e 1/10 i 13829/40794: loss 5.949862957000732\n",
            "e 1/10 i 13830/40794: loss 5.960770606994629\n",
            "e 1/10 i 13831/40794: loss 6.283533573150635\n",
            "e 1/10 i 13832/40794: loss 6.211483478546143\n",
            "e 1/10 i 13833/40794: loss 5.973659992218018\n",
            "e 1/10 i 13834/40794: loss 6.030959606170654\n",
            "e 1/10 i 13835/40794: loss 5.8996195793151855\n",
            "e 1/10 i 13836/40794: loss 5.780162811279297\n",
            "e 1/10 i 13837/40794: loss 5.9897565841674805\n",
            "e 1/10 i 13838/40794: loss 6.4215521812438965\n",
            "e 1/10 i 13839/40794: loss 5.918422698974609\n",
            "e 1/10 i 13840/40794: loss 5.882342338562012\n",
            "e 1/10 i 13841/40794: loss 5.988346576690674\n",
            "e 1/10 i 13842/40794: loss 6.182588577270508\n",
            "e 1/10 i 13843/40794: loss 6.152078151702881\n",
            "e 1/10 i 13844/40794: loss 5.904839992523193\n",
            "e 1/10 i 13845/40794: loss 5.970180511474609\n",
            "e 1/10 i 13846/40794: loss 5.910541534423828\n",
            "e 1/10 i 13847/40794: loss 5.923795700073242\n",
            "e 1/10 i 13848/40794: loss 6.1094818115234375\n",
            "e 1/10 i 13849/40794: loss 6.079901218414307\n",
            "e 1/10 i 13850/40794: loss 6.258570671081543\n",
            "e 1/10 i 13851/40794: loss 5.947547435760498\n",
            "e 1/10 i 13852/40794: loss 5.944979190826416\n",
            "e 1/10 i 13853/40794: loss 6.000531196594238\n",
            "e 1/10 i 13854/40794: loss 6.030827045440674\n",
            "e 1/10 i 13855/40794: loss 5.844156742095947\n",
            "e 1/10 i 13856/40794: loss 5.860614776611328\n",
            "e 1/10 i 13857/40794: loss 5.8483781814575195\n",
            "e 1/10 i 13858/40794: loss 5.865250587463379\n",
            "e 1/10 i 13859/40794: loss 6.019201755523682\n",
            "e 1/10 i 13860/40794: loss 5.857760906219482\n",
            "e 1/10 i 13861/40794: loss 6.108871936798096\n",
            "e 1/10 i 13862/40794: loss 5.896195411682129\n",
            "e 1/10 i 13863/40794: loss 6.203271865844727\n",
            "e 1/10 i 13864/40794: loss 6.167313098907471\n",
            "e 1/10 i 13865/40794: loss 6.068251609802246\n",
            "e 1/10 i 13866/40794: loss 5.8005571365356445\n",
            "e 1/10 i 13867/40794: loss 6.0790228843688965\n",
            "e 1/10 i 13868/40794: loss 5.90235710144043\n",
            "e 1/10 i 13869/40794: loss 5.700381278991699\n",
            "e 1/10 i 13870/40794: loss 5.840973377227783\n",
            "e 1/10 i 13871/40794: loss 5.994348526000977\n",
            "e 1/10 i 13872/40794: loss 5.871033191680908\n",
            "e 1/10 i 13873/40794: loss 5.869026184082031\n",
            "e 1/10 i 13874/40794: loss 5.988686561584473\n",
            "e 1/10 i 13875/40794: loss 5.808713436126709\n",
            "e 1/10 i 13876/40794: loss 5.985756874084473\n",
            "e 1/10 i 13877/40794: loss 6.084453105926514\n",
            "e 1/10 i 13878/40794: loss 5.776149272918701\n",
            "e 1/10 i 13879/40794: loss 5.823265075683594\n",
            "e 1/10 i 13880/40794: loss 5.74580717086792\n",
            "e 1/10 i 13881/40794: loss 5.898411750793457\n",
            "e 1/10 i 13882/40794: loss 5.779049873352051\n",
            "e 1/10 i 13883/40794: loss 6.182847499847412\n",
            "e 1/10 i 13884/40794: loss 5.94799280166626\n",
            "e 1/10 i 13885/40794: loss 6.018489360809326\n",
            "e 1/10 i 13886/40794: loss 5.682811737060547\n",
            "e 1/10 i 13887/40794: loss 5.431087493896484\n",
            "e 1/10 i 13888/40794: loss 6.174606800079346\n",
            "e 1/10 i 13889/40794: loss 5.899777412414551\n",
            "e 1/10 i 13890/40794: loss 5.909367561340332\n",
            "e 1/10 i 13891/40794: loss 5.884550094604492\n",
            "e 1/10 i 13892/40794: loss 6.040297031402588\n",
            "e 1/10 i 13893/40794: loss 5.959672451019287\n",
            "e 1/10 i 13894/40794: loss 6.120826244354248\n",
            "e 1/10 i 13895/40794: loss 6.122246742248535\n",
            "e 1/10 i 13896/40794: loss 6.1119561195373535\n",
            "e 1/10 i 13897/40794: loss 5.855243682861328\n",
            "e 1/10 i 13898/40794: loss 5.879572868347168\n",
            "e 1/10 i 13899/40794: loss 5.681324005126953\n",
            "e 1/10 i 13900/40794: loss 5.7977166175842285\n",
            "e 1/10 i 13901/40794: loss 5.702537536621094\n",
            "e 1/10 i 13902/40794: loss 5.745541572570801\n",
            "e 1/10 i 13903/40794: loss 5.80565071105957\n",
            "e 1/10 i 13904/40794: loss 5.83840274810791\n",
            "e 1/10 i 13905/40794: loss 5.984989166259766\n",
            "e 1/10 i 13906/40794: loss 5.756858825683594\n",
            "e 1/10 i 13907/40794: loss 5.780683517456055\n",
            "e 1/10 i 13908/40794: loss 6.087959289550781\n",
            "e 1/10 i 13909/40794: loss 6.013098239898682\n",
            "e 1/10 i 13910/40794: loss 5.750020980834961\n",
            "e 1/10 i 13911/40794: loss 5.721722602844238\n",
            "e 1/10 i 13912/40794: loss 5.8569488525390625\n",
            "e 1/10 i 13913/40794: loss 6.032966613769531\n",
            "e 1/10 i 13914/40794: loss 6.185358047485352\n",
            "e 1/10 i 13915/40794: loss 5.919852256774902\n",
            "e 1/10 i 13916/40794: loss 5.701321601867676\n",
            "e 1/10 i 13917/40794: loss 5.827479839324951\n",
            "e 1/10 i 13918/40794: loss 6.036773204803467\n",
            "e 1/10 i 13919/40794: loss 6.225555419921875\n",
            "e 1/10 i 13920/40794: loss 6.072183132171631\n",
            "e 1/10 i 13921/40794: loss 5.901483058929443\n",
            "e 1/10 i 13922/40794: loss 6.264608860015869\n",
            "e 1/10 i 13923/40794: loss 5.943202018737793\n",
            "e 1/10 i 13924/40794: loss 6.072329998016357\n",
            "e 1/10 i 13925/40794: loss 6.303103446960449\n",
            "e 1/10 i 13926/40794: loss 5.785765171051025\n",
            "e 1/10 i 13927/40794: loss 5.834525108337402\n",
            "e 1/10 i 13928/40794: loss 5.696110725402832\n",
            "e 1/10 i 13929/40794: loss 5.739816665649414\n",
            "e 1/10 i 13930/40794: loss 5.724133014678955\n",
            "e 1/10 i 13931/40794: loss 5.9640703201293945\n",
            "e 1/10 i 13932/40794: loss 5.6258134841918945\n",
            "e 1/10 i 13933/40794: loss 5.872636318206787\n",
            "e 1/10 i 13934/40794: loss 5.686773300170898\n",
            "e 1/10 i 13935/40794: loss 6.058443069458008\n",
            "e 1/10 i 13936/40794: loss 6.004739284515381\n",
            "e 1/10 i 13937/40794: loss 6.031089782714844\n",
            "e 1/10 i 13938/40794: loss 5.7413554191589355\n",
            "e 1/10 i 13939/40794: loss 5.723883628845215\n",
            "e 1/10 i 13940/40794: loss 5.981161117553711\n",
            "e 1/10 i 13941/40794: loss 5.976526737213135\n",
            "e 1/10 i 13942/40794: loss 5.934850215911865\n",
            "e 1/10 i 13943/40794: loss 6.069085121154785\n",
            "e 1/10 i 13944/40794: loss 5.770571708679199\n",
            "e 1/10 i 13945/40794: loss 6.330260276794434\n",
            "e 1/10 i 13946/40794: loss 5.905248641967773\n",
            "e 1/10 i 13947/40794: loss 5.836349964141846\n",
            "e 1/10 i 13948/40794: loss 5.906681537628174\n",
            "e 1/10 i 13949/40794: loss 5.760392189025879\n",
            "e 1/10 i 13950/40794: loss 5.720681190490723\n",
            "e 1/10 i 13951/40794: loss 5.850777626037598\n",
            "e 1/10 i 13952/40794: loss 6.013645648956299\n",
            "e 1/10 i 13953/40794: loss 5.814762592315674\n",
            "e 1/10 i 13954/40794: loss 6.038329601287842\n",
            "e 1/10 i 13955/40794: loss 6.220607757568359\n",
            "e 1/10 i 13956/40794: loss 5.635242462158203\n",
            "e 1/10 i 13957/40794: loss 5.963181495666504\n",
            "e 1/10 i 13958/40794: loss 6.046514987945557\n",
            "e 1/10 i 13959/40794: loss 6.257963180541992\n",
            "e 1/10 i 13960/40794: loss 6.013226509094238\n",
            "e 1/10 i 13961/40794: loss 5.998382568359375\n",
            "e 1/10 i 13962/40794: loss 5.939265251159668\n",
            "e 1/10 i 13963/40794: loss 5.960357666015625\n",
            "e 1/10 i 13964/40794: loss 6.153248310089111\n",
            "e 1/10 i 13965/40794: loss 5.843942642211914\n",
            "e 1/10 i 13966/40794: loss 5.966206073760986\n",
            "e 1/10 i 13967/40794: loss 6.015254497528076\n",
            "e 1/10 i 13968/40794: loss 5.8357133865356445\n",
            "e 1/10 i 13969/40794: loss 6.0145134925842285\n",
            "e 1/10 i 13970/40794: loss 5.926791667938232\n",
            "e 1/10 i 13971/40794: loss 5.991011619567871\n",
            "e 1/10 i 13972/40794: loss 5.842829704284668\n",
            "e 1/10 i 13973/40794: loss 6.010890483856201\n",
            "e 1/10 i 13974/40794: loss 5.788733005523682\n",
            "e 1/10 i 13975/40794: loss 5.785054683685303\n",
            "e 1/10 i 13976/40794: loss 5.883392333984375\n",
            "e 1/10 i 13977/40794: loss 5.55214786529541\n",
            "e 1/10 i 13978/40794: loss 5.933760643005371\n",
            "e 1/10 i 13979/40794: loss 5.8791184425354\n",
            "e 1/10 i 13980/40794: loss 6.060792446136475\n",
            "e 1/10 i 13981/40794: loss 6.027167320251465\n",
            "e 1/10 i 13982/40794: loss 5.889070987701416\n",
            "e 1/10 i 13983/40794: loss 5.905797958374023\n",
            "e 1/10 i 13984/40794: loss 6.145024299621582\n",
            "e 1/10 i 13985/40794: loss 5.821523189544678\n",
            "e 1/10 i 13986/40794: loss 5.884393692016602\n",
            "e 1/10 i 13987/40794: loss 6.000115394592285\n",
            "e 1/10 i 13988/40794: loss 5.925485610961914\n",
            "e 1/10 i 13989/40794: loss 6.125051498413086\n",
            "e 1/10 i 13990/40794: loss 5.981761455535889\n",
            "e 1/10 i 13991/40794: loss 5.69793176651001\n",
            "e 1/10 i 13992/40794: loss 5.885211944580078\n",
            "e 1/10 i 13993/40794: loss 5.946625232696533\n",
            "e 1/10 i 13994/40794: loss 5.774047374725342\n",
            "e 1/10 i 13995/40794: loss 6.067874431610107\n",
            "e 1/10 i 13996/40794: loss 6.068537712097168\n",
            "e 1/10 i 13997/40794: loss 5.703075885772705\n",
            "e 1/10 i 13998/40794: loss 5.78493070602417\n",
            "e 1/10 i 13999/40794: loss 5.847411632537842\n",
            "e 1/10 i 14000/40794: loss 5.885970115661621\n",
            "e 1/10 i 14001/40794: loss 5.762179851531982\n",
            "e 1/10 i 14002/40794: loss 5.560420989990234\n",
            "e 1/10 i 14003/40794: loss 5.858347415924072\n",
            "e 1/10 i 14004/40794: loss 5.783615589141846\n",
            "e 1/10 i 14005/40794: loss 5.947690963745117\n",
            "e 1/10 i 14006/40794: loss 5.948845386505127\n",
            "e 1/10 i 14007/40794: loss 5.7516584396362305\n",
            "e 1/10 i 14008/40794: loss 5.753016948699951\n",
            "e 1/10 i 14009/40794: loss 5.7218499183654785\n",
            "e 1/10 i 14010/40794: loss 5.734346866607666\n",
            "e 1/10 i 14011/40794: loss 6.1152167320251465\n",
            "e 1/10 i 14012/40794: loss 5.832791328430176\n",
            "e 1/10 i 14013/40794: loss 5.79664945602417\n",
            "e 1/10 i 14014/40794: loss 6.054626941680908\n",
            "e 1/10 i 14015/40794: loss 6.024681091308594\n",
            "e 1/10 i 14016/40794: loss 5.6526031494140625\n",
            "e 1/10 i 14017/40794: loss 5.862999439239502\n",
            "e 1/10 i 14018/40794: loss 6.086848735809326\n",
            "e 1/10 i 14019/40794: loss 5.840026378631592\n",
            "e 1/10 i 14020/40794: loss 5.995140552520752\n",
            "e 1/10 i 14021/40794: loss 5.834857940673828\n",
            "e 1/10 i 14022/40794: loss 6.235101699829102\n",
            "e 1/10 i 14023/40794: loss 5.5511908531188965\n",
            "e 1/10 i 14024/40794: loss 5.897732257843018\n",
            "e 1/10 i 14025/40794: loss 5.836270332336426\n",
            "e 1/10 i 14026/40794: loss 6.066563129425049\n",
            "e 1/10 i 14027/40794: loss 5.794445991516113\n",
            "e 1/10 i 14028/40794: loss 6.272496700286865\n",
            "e 1/10 i 14029/40794: loss 6.10477352142334\n",
            "e 1/10 i 14030/40794: loss 5.965805530548096\n",
            "e 1/10 i 14031/40794: loss 5.93277645111084\n",
            "e 1/10 i 14032/40794: loss 6.461233615875244\n",
            "e 1/10 i 14033/40794: loss 5.594567775726318\n",
            "e 1/10 i 14034/40794: loss 6.192546367645264\n",
            "e 1/10 i 14035/40794: loss 5.740401744842529\n",
            "e 1/10 i 14036/40794: loss 6.286322116851807\n",
            "e 1/10 i 14037/40794: loss 5.828818321228027\n",
            "e 1/10 i 14038/40794: loss 6.233541011810303\n",
            "e 1/10 i 14039/40794: loss 5.897098064422607\n",
            "e 1/10 i 14040/40794: loss 5.8256964683532715\n",
            "e 1/10 i 14041/40794: loss 5.780534744262695\n",
            "e 1/10 i 14042/40794: loss 6.117401123046875\n",
            "e 1/10 i 14043/40794: loss 6.082708358764648\n",
            "e 1/10 i 14044/40794: loss 6.095219135284424\n",
            "e 1/10 i 14045/40794: loss 6.0686492919921875\n",
            "e 1/10 i 14046/40794: loss 5.616275787353516\n",
            "e 1/10 i 14047/40794: loss 5.842151165008545\n",
            "e 1/10 i 14048/40794: loss 5.753072261810303\n",
            "e 1/10 i 14049/40794: loss 5.787775039672852\n",
            "e 1/10 i 14050/40794: loss 5.960590839385986\n",
            "e 1/10 i 14051/40794: loss 5.702156066894531\n",
            "e 1/10 i 14052/40794: loss 5.7488627433776855\n",
            "e 1/10 i 14053/40794: loss 5.9226884841918945\n",
            "e 1/10 i 14054/40794: loss 5.750918865203857\n",
            "e 1/10 i 14055/40794: loss 5.858911991119385\n",
            "e 1/10 i 14056/40794: loss 5.871711730957031\n",
            "e 1/10 i 14057/40794: loss 6.155391693115234\n",
            "e 1/10 i 14058/40794: loss 6.023698329925537\n",
            "e 1/10 i 14059/40794: loss 5.400978088378906\n",
            "e 1/10 i 14060/40794: loss 5.782824516296387\n",
            "e 1/10 i 14061/40794: loss 5.624056816101074\n",
            "e 1/10 i 14062/40794: loss 5.964367389678955\n",
            "e 1/10 i 14063/40794: loss 5.703176498413086\n",
            "e 1/10 i 14064/40794: loss 5.886899471282959\n",
            "e 1/10 i 14065/40794: loss 6.024406433105469\n",
            "e 1/10 i 14066/40794: loss 5.723461151123047\n",
            "e 1/10 i 14067/40794: loss 5.986828327178955\n",
            "e 1/10 i 14068/40794: loss 6.001993179321289\n",
            "e 1/10 i 14069/40794: loss 5.9600982666015625\n",
            "e 1/10 i 14070/40794: loss 5.81377649307251\n",
            "e 1/10 i 14071/40794: loss 5.5645575523376465\n",
            "e 1/10 i 14072/40794: loss 5.6323466300964355\n",
            "e 1/10 i 14073/40794: loss 5.647831916809082\n",
            "e 1/10 i 14074/40794: loss 5.886350154876709\n",
            "e 1/10 i 14075/40794: loss 5.82233190536499\n",
            "e 1/10 i 14076/40794: loss 6.544423580169678\n",
            "e 1/10 i 14077/40794: loss 6.044722557067871\n",
            "e 1/10 i 14078/40794: loss 5.9127092361450195\n",
            "e 1/10 i 14079/40794: loss 6.035592079162598\n",
            "e 1/10 i 14080/40794: loss 5.872040748596191\n",
            "e 1/10 i 14081/40794: loss 6.080250263214111\n",
            "e 1/10 i 14082/40794: loss 5.686400413513184\n",
            "e 1/10 i 14083/40794: loss 5.681728363037109\n",
            "e 1/10 i 14084/40794: loss 6.075169563293457\n",
            "e 1/10 i 14085/40794: loss 5.984036445617676\n",
            "e 1/10 i 14086/40794: loss 5.879842281341553\n",
            "e 1/10 i 14087/40794: loss 5.791830539703369\n",
            "e 1/10 i 14088/40794: loss 5.79290246963501\n",
            "e 1/10 i 14089/40794: loss 5.916008949279785\n",
            "e 1/10 i 14090/40794: loss 5.956972599029541\n",
            "e 1/10 i 14091/40794: loss 6.336238384246826\n",
            "e 1/10 i 14092/40794: loss 5.871175289154053\n",
            "e 1/10 i 14093/40794: loss 5.84611701965332\n",
            "e 1/10 i 14094/40794: loss 5.61205530166626\n",
            "e 1/10 i 14095/40794: loss 5.997531890869141\n",
            "e 1/10 i 14096/40794: loss 5.965376853942871\n",
            "e 1/10 i 14097/40794: loss 6.176127910614014\n",
            "e 1/10 i 14098/40794: loss 5.840451717376709\n",
            "e 1/10 i 14099/40794: loss 6.138469219207764\n",
            "e 1/10 i 14100/40794: loss 5.956870079040527\n",
            "e 1/10 i 14101/40794: loss 5.927378177642822\n",
            "e 1/10 i 14102/40794: loss 5.966392993927002\n",
            "e 1/10 i 14103/40794: loss 5.827276706695557\n",
            "e 1/10 i 14104/40794: loss 6.0230865478515625\n",
            "e 1/10 i 14105/40794: loss 5.733422756195068\n",
            "e 1/10 i 14106/40794: loss 5.866916656494141\n",
            "e 1/10 i 14107/40794: loss 5.90958309173584\n",
            "e 1/10 i 14108/40794: loss 5.840664386749268\n",
            "e 1/10 i 14109/40794: loss 5.8933305740356445\n",
            "e 1/10 i 14110/40794: loss 6.177567958831787\n",
            "e 1/10 i 14111/40794: loss 6.032530784606934\n",
            "e 1/10 i 14112/40794: loss 5.703306674957275\n",
            "e 1/10 i 14113/40794: loss 5.748343467712402\n",
            "e 1/10 i 14114/40794: loss 6.288141250610352\n",
            "e 1/10 i 14115/40794: loss 5.790563106536865\n",
            "e 1/10 i 14116/40794: loss 5.623208045959473\n",
            "e 1/10 i 14117/40794: loss 5.91645622253418\n",
            "e 1/10 i 14118/40794: loss 6.09548282623291\n",
            "e 1/10 i 14119/40794: loss 5.984914779663086\n",
            "e 1/10 i 14120/40794: loss 5.926417827606201\n",
            "e 1/10 i 14121/40794: loss 6.060141086578369\n",
            "e 1/10 i 14122/40794: loss 5.8339009284973145\n",
            "e 1/10 i 14123/40794: loss 6.160861968994141\n",
            "e 1/10 i 14124/40794: loss 6.336943626403809\n",
            "e 1/10 i 14125/40794: loss 5.795256614685059\n",
            "e 1/10 i 14126/40794: loss 5.7184247970581055\n",
            "e 1/10 i 14127/40794: loss 5.781809329986572\n",
            "e 1/10 i 14128/40794: loss 5.797027587890625\n",
            "e 1/10 i 14129/40794: loss 5.719642162322998\n",
            "e 1/10 i 14130/40794: loss 5.696128845214844\n",
            "e 1/10 i 14131/40794: loss 5.9348859786987305\n",
            "e 1/10 i 14132/40794: loss 6.058099269866943\n",
            "e 1/10 i 14133/40794: loss 6.120258331298828\n",
            "e 1/10 i 14134/40794: loss 5.56386137008667\n",
            "e 1/10 i 14135/40794: loss 5.94562292098999\n",
            "e 1/10 i 14136/40794: loss 6.06665563583374\n",
            "e 1/10 i 14137/40794: loss 5.648331642150879\n",
            "e 1/10 i 14138/40794: loss 5.632613182067871\n",
            "e 1/10 i 14139/40794: loss 6.020392417907715\n",
            "e 1/10 i 14140/40794: loss 6.047652244567871\n",
            "e 1/10 i 14141/40794: loss 6.039656162261963\n",
            "e 1/10 i 14142/40794: loss 6.12840461730957\n",
            "e 1/10 i 14143/40794: loss 5.766571521759033\n",
            "e 1/10 i 14144/40794: loss 5.858992576599121\n",
            "e 1/10 i 14145/40794: loss 5.972711563110352\n",
            "e 1/10 i 14146/40794: loss 6.063188552856445\n",
            "e 1/10 i 14147/40794: loss 5.8568596839904785\n",
            "e 1/10 i 14148/40794: loss 5.691251277923584\n",
            "e 1/10 i 14149/40794: loss 5.817251682281494\n",
            "e 1/10 i 14150/40794: loss 5.595128536224365\n",
            "e 1/10 i 14151/40794: loss 5.848224639892578\n",
            "e 1/10 i 14152/40794: loss 6.11027193069458\n",
            "e 1/10 i 14153/40794: loss 6.171656608581543\n",
            "e 1/10 i 14154/40794: loss 6.040105819702148\n",
            "e 1/10 i 14155/40794: loss 5.711689472198486\n",
            "e 1/10 i 14156/40794: loss 5.906707763671875\n",
            "e 1/10 i 14157/40794: loss 6.01472806930542\n",
            "e 1/10 i 14158/40794: loss 5.745121002197266\n",
            "e 1/10 i 14159/40794: loss 5.781195163726807\n",
            "e 1/10 i 14160/40794: loss 5.905184268951416\n",
            "e 1/10 i 14161/40794: loss 5.938351631164551\n",
            "e 1/10 i 14162/40794: loss 5.81456184387207\n",
            "e 1/10 i 14163/40794: loss 5.879519939422607\n",
            "e 1/10 i 14164/40794: loss 5.7017035484313965\n",
            "e 1/10 i 14165/40794: loss 5.861690998077393\n",
            "e 1/10 i 14166/40794: loss 6.1152567863464355\n",
            "e 1/10 i 14167/40794: loss 5.750890254974365\n",
            "e 1/10 i 14168/40794: loss 5.801849365234375\n",
            "e 1/10 i 14169/40794: loss 5.945805072784424\n",
            "e 1/10 i 14170/40794: loss 5.655277252197266\n",
            "e 1/10 i 14171/40794: loss 5.916087627410889\n",
            "e 1/10 i 14172/40794: loss 5.946380615234375\n",
            "e 1/10 i 14173/40794: loss 5.581428050994873\n",
            "e 1/10 i 14174/40794: loss 5.924262046813965\n",
            "e 1/10 i 14175/40794: loss 5.810642242431641\n",
            "e 1/10 i 14176/40794: loss 5.703484058380127\n",
            "e 1/10 i 14177/40794: loss 5.814994812011719\n",
            "e 1/10 i 14178/40794: loss 5.831551551818848\n",
            "e 1/10 i 14179/40794: loss 5.986804485321045\n",
            "e 1/10 i 14180/40794: loss 5.746832847595215\n",
            "e 1/10 i 14181/40794: loss 5.6907639503479\n",
            "e 1/10 i 14182/40794: loss 6.148635387420654\n",
            "e 1/10 i 14183/40794: loss 6.0261549949646\n",
            "e 1/10 i 14184/40794: loss 6.059539318084717\n",
            "e 1/10 i 14185/40794: loss 5.85581111907959\n",
            "e 1/10 i 14186/40794: loss 5.522568225860596\n",
            "e 1/10 i 14187/40794: loss 5.817705154418945\n",
            "e 1/10 i 14188/40794: loss 6.0191779136657715\n",
            "e 1/10 i 14189/40794: loss 5.903642177581787\n",
            "e 1/10 i 14190/40794: loss 5.962735176086426\n",
            "e 1/10 i 14191/40794: loss 5.511898517608643\n",
            "e 1/10 i 14192/40794: loss 5.679648399353027\n",
            "e 1/10 i 14193/40794: loss 5.963306903839111\n",
            "e 1/10 i 14194/40794: loss 6.251145362854004\n",
            "e 1/10 i 14195/40794: loss 5.563443660736084\n",
            "e 1/10 i 14196/40794: loss 6.21550989151001\n",
            "e 1/10 i 14197/40794: loss 5.971263408660889\n",
            "e 1/10 i 14198/40794: loss 5.896740913391113\n",
            "e 1/10 i 14199/40794: loss 5.858235836029053\n",
            "e 1/10 i 14200/40794: loss 5.972338676452637\n",
            "e 1/10 i 14201/40794: loss 6.05442476272583\n",
            "e 1/10 i 14202/40794: loss 5.826050281524658\n",
            "e 1/10 i 14203/40794: loss 5.832329273223877\n",
            "e 1/10 i 14204/40794: loss 6.165619373321533\n",
            "e 1/10 i 14205/40794: loss 5.798081398010254\n",
            "e 1/10 i 14206/40794: loss 6.117286205291748\n",
            "e 1/10 i 14207/40794: loss 6.145851135253906\n",
            "e 1/10 i 14208/40794: loss 6.017463207244873\n",
            "e 1/10 i 14209/40794: loss 5.759004592895508\n",
            "e 1/10 i 14210/40794: loss 5.802759647369385\n",
            "e 1/10 i 14211/40794: loss 5.765754699707031\n",
            "e 1/10 i 14212/40794: loss 6.213905334472656\n",
            "e 1/10 i 14213/40794: loss 6.120787620544434\n",
            "e 1/10 i 14214/40794: loss 5.921081066131592\n",
            "e 1/10 i 14215/40794: loss 6.112689018249512\n",
            "e 1/10 i 14216/40794: loss 5.803622722625732\n",
            "e 1/10 i 14217/40794: loss 5.992099761962891\n",
            "e 1/10 i 14218/40794: loss 6.058444499969482\n",
            "e 1/10 i 14219/40794: loss 5.849846363067627\n",
            "e 1/10 i 14220/40794: loss 5.813796043395996\n",
            "e 1/10 i 14221/40794: loss 6.167304039001465\n",
            "e 1/10 i 14222/40794: loss 5.892143726348877\n",
            "e 1/10 i 14223/40794: loss 5.79092264175415\n",
            "e 1/10 i 14224/40794: loss 6.038455486297607\n",
            "e 1/10 i 14225/40794: loss 5.7656145095825195\n",
            "e 1/10 i 14226/40794: loss 6.219273567199707\n",
            "e 1/10 i 14227/40794: loss 6.068417072296143\n",
            "e 1/10 i 14228/40794: loss 5.638452529907227\n",
            "e 1/10 i 14229/40794: loss 5.779729843139648\n",
            "e 1/10 i 14230/40794: loss 5.818920135498047\n",
            "e 1/10 i 14231/40794: loss 6.006410121917725\n",
            "e 1/10 i 14232/40794: loss 6.011839389801025\n",
            "e 1/10 i 14233/40794: loss 5.9795989990234375\n",
            "e 1/10 i 14234/40794: loss 6.171425819396973\n",
            "e 1/10 i 14235/40794: loss 6.185659408569336\n",
            "e 1/10 i 14236/40794: loss 5.889492988586426\n",
            "e 1/10 i 14237/40794: loss 5.942548751831055\n",
            "e 1/10 i 14238/40794: loss 5.774872303009033\n",
            "e 1/10 i 14239/40794: loss 6.029411792755127\n",
            "e 1/10 i 14240/40794: loss 5.814505577087402\n",
            "e 1/10 i 14241/40794: loss 5.9593329429626465\n",
            "e 1/10 i 14242/40794: loss 5.975779056549072\n",
            "e 1/10 i 14243/40794: loss 6.193246841430664\n",
            "e 1/10 i 14244/40794: loss 6.073413848876953\n",
            "e 1/10 i 14245/40794: loss 5.682666301727295\n",
            "e 1/10 i 14246/40794: loss 5.7920708656311035\n",
            "e 1/10 i 14247/40794: loss 5.926309108734131\n",
            "e 1/10 i 14248/40794: loss 5.748663425445557\n",
            "e 1/10 i 14249/40794: loss 5.719295024871826\n",
            "e 1/10 i 14250/40794: loss 5.652493000030518\n",
            "e 1/10 i 14251/40794: loss 5.804805278778076\n",
            "e 1/10 i 14252/40794: loss 6.172430515289307\n",
            "e 1/10 i 14253/40794: loss 5.849837779998779\n",
            "e 1/10 i 14254/40794: loss 6.050120830535889\n",
            "e 1/10 i 14255/40794: loss 6.138765335083008\n",
            "e 1/10 i 14256/40794: loss 5.845006942749023\n",
            "e 1/10 i 14257/40794: loss 5.803722858428955\n",
            "e 1/10 i 14258/40794: loss 6.261386394500732\n",
            "e 1/10 i 14259/40794: loss 6.157522678375244\n",
            "e 1/10 i 14260/40794: loss 5.834466457366943\n",
            "e 1/10 i 14261/40794: loss 6.158107757568359\n",
            "e 1/10 i 14262/40794: loss 5.845215320587158\n",
            "e 1/10 i 14263/40794: loss 5.824652671813965\n",
            "e 1/10 i 14264/40794: loss 5.714218616485596\n",
            "e 1/10 i 14265/40794: loss 5.785470962524414\n",
            "e 1/10 i 14266/40794: loss 6.097550392150879\n",
            "e 1/10 i 14267/40794: loss 5.854024410247803\n",
            "e 1/10 i 14268/40794: loss 6.039297580718994\n",
            "e 1/10 i 14269/40794: loss 6.185436248779297\n",
            "e 1/10 i 14270/40794: loss 5.841071128845215\n",
            "e 1/10 i 14271/40794: loss 5.687717437744141\n",
            "e 1/10 i 14272/40794: loss 5.8278093338012695\n",
            "e 1/10 i 14273/40794: loss 5.780024528503418\n",
            "e 1/10 i 14274/40794: loss 6.086087703704834\n",
            "e 1/10 i 14275/40794: loss 6.155799865722656\n",
            "e 1/10 i 14276/40794: loss 6.105001449584961\n",
            "e 1/10 i 14277/40794: loss 5.803387641906738\n",
            "e 1/10 i 14278/40794: loss 5.577481746673584\n",
            "e 1/10 i 14279/40794: loss 6.007361888885498\n",
            "e 1/10 i 14280/40794: loss 6.203166961669922\n",
            "e 1/10 i 14281/40794: loss 5.645358562469482\n",
            "e 1/10 i 14282/40794: loss 5.6016106605529785\n",
            "e 1/10 i 14283/40794: loss 6.17529296875\n",
            "e 1/10 i 14284/40794: loss 5.909979820251465\n",
            "e 1/10 i 14285/40794: loss 5.985183238983154\n",
            "e 1/10 i 14286/40794: loss 5.904904842376709\n",
            "e 1/10 i 14287/40794: loss 5.826691627502441\n",
            "e 1/10 i 14288/40794: loss 5.8974809646606445\n",
            "e 1/10 i 14289/40794: loss 5.799509048461914\n",
            "e 1/10 i 14290/40794: loss 5.80082368850708\n",
            "e 1/10 i 14291/40794: loss 5.8918304443359375\n",
            "e 1/10 i 14292/40794: loss 5.839229106903076\n",
            "e 1/10 i 14293/40794: loss 5.738490104675293\n",
            "e 1/10 i 14294/40794: loss 6.14133882522583\n",
            "e 1/10 i 14295/40794: loss 5.751088619232178\n",
            "e 1/10 i 14296/40794: loss 6.011728763580322\n",
            "e 1/10 i 14297/40794: loss 5.829116344451904\n",
            "e 1/10 i 14298/40794: loss 5.721947193145752\n",
            "e 1/10 i 14299/40794: loss 5.847759246826172\n",
            "e 1/10 i 14300/40794: loss 6.1639404296875\n",
            "e 1/10 i 14301/40794: loss 6.104520797729492\n",
            "e 1/10 i 14302/40794: loss 5.719091415405273\n",
            "e 1/10 i 14303/40794: loss 6.436158657073975\n",
            "e 1/10 i 14304/40794: loss 5.882807731628418\n",
            "e 1/10 i 14305/40794: loss 6.223433971405029\n",
            "e 1/10 i 14306/40794: loss 5.856555461883545\n",
            "e 1/10 i 14307/40794: loss 5.862667560577393\n",
            "e 1/10 i 14308/40794: loss 5.965158939361572\n",
            "e 1/10 i 14309/40794: loss 6.304902076721191\n",
            "e 1/10 i 14310/40794: loss 5.9312591552734375\n",
            "e 1/10 i 14311/40794: loss 5.822881698608398\n",
            "e 1/10 i 14312/40794: loss 5.949550151824951\n",
            "e 1/10 i 14313/40794: loss 6.084939002990723\n",
            "e 1/10 i 14314/40794: loss 5.607292652130127\n",
            "e 1/10 i 14315/40794: loss 5.8533830642700195\n",
            "e 1/10 i 14316/40794: loss 5.779881477355957\n",
            "e 1/10 i 14317/40794: loss 5.988119125366211\n",
            "e 1/10 i 14318/40794: loss 5.851351261138916\n",
            "e 1/10 i 14319/40794: loss 6.061897277832031\n",
            "e 1/10 i 14320/40794: loss 6.158830165863037\n",
            "e 1/10 i 14321/40794: loss 6.0437846183776855\n",
            "e 1/10 i 14322/40794: loss 6.041324615478516\n",
            "e 1/10 i 14323/40794: loss 6.12498140335083\n",
            "e 1/10 i 14324/40794: loss 5.834267616271973\n",
            "e 1/10 i 14325/40794: loss 6.184169292449951\n",
            "e 1/10 i 14326/40794: loss 6.16958475112915\n",
            "e 1/10 i 14327/40794: loss 6.094137191772461\n",
            "e 1/10 i 14328/40794: loss 6.096141338348389\n",
            "e 1/10 i 14329/40794: loss 5.914690017700195\n",
            "e 1/10 i 14330/40794: loss 6.163149833679199\n",
            "e 1/10 i 14331/40794: loss 5.83893346786499\n",
            "e 1/10 i 14332/40794: loss 5.862527847290039\n",
            "e 1/10 i 14333/40794: loss 6.017556667327881\n",
            "e 1/10 i 14334/40794: loss 5.841738224029541\n",
            "e 1/10 i 14335/40794: loss 6.18050479888916\n",
            "e 1/10 i 14336/40794: loss 6.153054237365723\n",
            "e 1/10 i 14337/40794: loss 5.936997890472412\n",
            "e 1/10 i 14338/40794: loss 6.062771320343018\n",
            "e 1/10 i 14339/40794: loss 6.101261615753174\n",
            "e 1/10 i 14340/40794: loss 6.022634506225586\n",
            "e 1/10 i 14341/40794: loss 5.839576244354248\n",
            "e 1/10 i 14342/40794: loss 5.862196445465088\n",
            "e 1/10 i 14343/40794: loss 5.957270622253418\n",
            "e 1/10 i 14344/40794: loss 6.216683387756348\n",
            "e 1/10 i 14345/40794: loss 5.731137275695801\n",
            "e 1/10 i 14346/40794: loss 5.7686991691589355\n",
            "e 1/10 i 14347/40794: loss 5.822061538696289\n",
            "e 1/10 i 14348/40794: loss 5.961404323577881\n",
            "e 1/10 i 14349/40794: loss 6.025276184082031\n",
            "e 1/10 i 14350/40794: loss 5.838354110717773\n",
            "e 1/10 i 14351/40794: loss 5.975611686706543\n",
            "e 1/10 i 14352/40794: loss 5.7448883056640625\n",
            "e 1/10 i 14353/40794: loss 6.363979816436768\n",
            "e 1/10 i 14354/40794: loss 5.698285102844238\n",
            "e 1/10 i 14355/40794: loss 5.7005205154418945\n",
            "e 1/10 i 14356/40794: loss 5.783253192901611\n",
            "e 1/10 i 14357/40794: loss 6.135102272033691\n",
            "e 1/10 i 14358/40794: loss 5.76487398147583\n",
            "e 1/10 i 14359/40794: loss 5.84232759475708\n",
            "e 1/10 i 14360/40794: loss 5.780523300170898\n",
            "e 1/10 i 14361/40794: loss 5.841735363006592\n",
            "e 1/10 i 14362/40794: loss 6.026174545288086\n",
            "e 1/10 i 14363/40794: loss 5.484137058258057\n",
            "e 1/10 i 14364/40794: loss 6.17603063583374\n",
            "e 1/10 i 14365/40794: loss 5.751044273376465\n",
            "e 1/10 i 14366/40794: loss 6.048960208892822\n",
            "e 1/10 i 14367/40794: loss 5.770127773284912\n",
            "e 1/10 i 14368/40794: loss 5.846933841705322\n",
            "e 1/10 i 14369/40794: loss 5.798286437988281\n",
            "e 1/10 i 14370/40794: loss 6.201293468475342\n",
            "e 1/10 i 14371/40794: loss 5.848405838012695\n",
            "e 1/10 i 14372/40794: loss 6.277122497558594\n",
            "e 1/10 i 14373/40794: loss 5.901456832885742\n",
            "e 1/10 i 14374/40794: loss 6.0856614112854\n",
            "e 1/10 i 14375/40794: loss 6.049954891204834\n",
            "e 1/10 i 14376/40794: loss 5.9181671142578125\n",
            "e 1/10 i 14377/40794: loss 5.966521263122559\n",
            "e 1/10 i 14378/40794: loss 5.8890557289123535\n",
            "e 1/10 i 14379/40794: loss 5.72290563583374\n",
            "e 1/10 i 14380/40794: loss 5.604247570037842\n",
            "e 1/10 i 14381/40794: loss 6.10707426071167\n",
            "e 1/10 i 14382/40794: loss 5.7747721672058105\n",
            "e 1/10 i 14383/40794: loss 5.680480480194092\n",
            "e 1/10 i 14384/40794: loss 6.189265727996826\n",
            "e 1/10 i 14385/40794: loss 5.918576240539551\n",
            "e 1/10 i 14386/40794: loss 6.188381671905518\n",
            "e 1/10 i 14387/40794: loss 5.652357578277588\n",
            "e 1/10 i 14388/40794: loss 6.142126560211182\n",
            "e 1/10 i 14389/40794: loss 5.876295566558838\n",
            "e 1/10 i 14390/40794: loss 5.897561550140381\n",
            "e 1/10 i 14391/40794: loss 6.312151908874512\n",
            "e 1/10 i 14392/40794: loss 5.883861541748047\n",
            "e 1/10 i 14393/40794: loss 6.062224864959717\n",
            "e 1/10 i 14394/40794: loss 5.67555046081543\n",
            "e 1/10 i 14395/40794: loss 6.278383731842041\n",
            "e 1/10 i 14396/40794: loss 5.936295032501221\n",
            "e 1/10 i 14397/40794: loss 6.039381504058838\n",
            "e 1/10 i 14398/40794: loss 5.926170349121094\n",
            "e 1/10 i 14399/40794: loss 6.176131248474121\n",
            "e 1/10 i 14400/40794: loss 5.613296985626221\n",
            "e 1/10 i 14401/40794: loss 5.789485454559326\n",
            "e 1/10 i 14402/40794: loss 5.911117076873779\n",
            "e 1/10 i 14403/40794: loss 5.635615825653076\n",
            "e 1/10 i 14404/40794: loss 6.065708637237549\n",
            "e 1/10 i 14405/40794: loss 5.941928863525391\n",
            "e 1/10 i 14406/40794: loss 6.162611484527588\n",
            "e 1/10 i 14407/40794: loss 6.044130325317383\n",
            "e 1/10 i 14408/40794: loss 6.387176990509033\n",
            "e 1/10 i 14409/40794: loss 5.974786281585693\n",
            "e 1/10 i 14410/40794: loss 6.041550636291504\n",
            "e 1/10 i 14411/40794: loss 5.965999603271484\n",
            "e 1/10 i 14412/40794: loss 5.872410297393799\n",
            "e 1/10 i 14413/40794: loss 5.611114978790283\n",
            "e 1/10 i 14414/40794: loss 5.911330223083496\n",
            "e 1/10 i 14415/40794: loss 6.0963664054870605\n",
            "e 1/10 i 14416/40794: loss 5.990354061126709\n",
            "e 1/10 i 14417/40794: loss 5.618401527404785\n",
            "e 1/10 i 14418/40794: loss 6.081783771514893\n",
            "e 1/10 i 14419/40794: loss 5.857475757598877\n",
            "e 1/10 i 14420/40794: loss 5.843301773071289\n",
            "e 1/10 i 14421/40794: loss 5.772637367248535\n",
            "e 1/10 i 14422/40794: loss 6.282960891723633\n",
            "e 1/10 i 14423/40794: loss 5.781564235687256\n",
            "e 1/10 i 14424/40794: loss 5.899965286254883\n",
            "e 1/10 i 14425/40794: loss 5.944447040557861\n",
            "e 1/10 i 14426/40794: loss 5.552074432373047\n",
            "e 1/10 i 14427/40794: loss 5.996362686157227\n",
            "e 1/10 i 14428/40794: loss 5.812775611877441\n",
            "e 1/10 i 14429/40794: loss 6.137626647949219\n",
            "e 1/10 i 14430/40794: loss 5.496152400970459\n",
            "e 1/10 i 14431/40794: loss 6.122849464416504\n",
            "e 1/10 i 14432/40794: loss 5.943490028381348\n",
            "e 1/10 i 14433/40794: loss 6.195476055145264\n",
            "e 1/10 i 14434/40794: loss 5.922449111938477\n",
            "e 1/10 i 14435/40794: loss 5.83597993850708\n",
            "e 1/10 i 14436/40794: loss 5.635204315185547\n",
            "e 1/10 i 14437/40794: loss 5.854297161102295\n",
            "e 1/10 i 14438/40794: loss 6.408127307891846\n",
            "e 1/10 i 14439/40794: loss 5.692729949951172\n",
            "e 1/10 i 14440/40794: loss 6.11358642578125\n",
            "e 1/10 i 14441/40794: loss 6.002915382385254\n",
            "e 1/10 i 14442/40794: loss 6.063453674316406\n",
            "e 1/10 i 14443/40794: loss 5.922440052032471\n",
            "e 1/10 i 14444/40794: loss 5.827498435974121\n",
            "e 1/10 i 14445/40794: loss 5.681776523590088\n",
            "e 1/10 i 14446/40794: loss 5.995203018188477\n",
            "e 1/10 i 14447/40794: loss 5.8552141189575195\n",
            "e 1/10 i 14448/40794: loss 6.145834922790527\n",
            "e 1/10 i 14449/40794: loss 6.080952167510986\n",
            "e 1/10 i 14450/40794: loss 5.868106365203857\n",
            "e 1/10 i 14451/40794: loss 5.911677837371826\n",
            "e 1/10 i 14452/40794: loss 5.660152435302734\n",
            "e 1/10 i 14453/40794: loss 6.017219066619873\n",
            "e 1/10 i 14454/40794: loss 6.009823322296143\n",
            "e 1/10 i 14455/40794: loss 5.898916244506836\n",
            "e 1/10 i 14456/40794: loss 5.813446998596191\n",
            "e 1/10 i 14457/40794: loss 5.950961589813232\n",
            "e 1/10 i 14458/40794: loss 6.264219284057617\n",
            "e 1/10 i 14459/40794: loss 5.82731294631958\n",
            "e 1/10 i 14460/40794: loss 6.171460151672363\n",
            "e 1/10 i 14461/40794: loss 6.071341514587402\n",
            "e 1/10 i 14462/40794: loss 5.929745674133301\n",
            "e 1/10 i 14463/40794: loss 6.353033065795898\n",
            "e 1/10 i 14464/40794: loss 6.110157489776611\n",
            "e 1/10 i 14465/40794: loss 6.042605876922607\n",
            "e 1/10 i 14466/40794: loss 5.803962707519531\n",
            "e 1/10 i 14467/40794: loss 5.968532085418701\n",
            "e 1/10 i 14468/40794: loss 6.196319103240967\n",
            "e 1/10 i 14469/40794: loss 5.58508825302124\n",
            "e 1/10 i 14470/40794: loss 5.504334449768066\n",
            "e 1/10 i 14471/40794: loss 5.899040222167969\n",
            "e 1/10 i 14472/40794: loss 6.260037899017334\n",
            "e 1/10 i 14473/40794: loss 5.80918550491333\n",
            "e 1/10 i 14474/40794: loss 5.970214366912842\n",
            "e 1/10 i 14475/40794: loss 6.06752872467041\n",
            "e 1/10 i 14476/40794: loss 5.81633996963501\n",
            "e 1/10 i 14477/40794: loss 5.888086318969727\n",
            "e 1/10 i 14478/40794: loss 5.716910362243652\n",
            "e 1/10 i 14479/40794: loss 6.095945358276367\n",
            "e 1/10 i 14480/40794: loss 5.667232513427734\n",
            "e 1/10 i 14481/40794: loss 5.653719425201416\n",
            "e 1/10 i 14482/40794: loss 5.923871040344238\n",
            "e 1/10 i 14483/40794: loss 5.954113483428955\n",
            "e 1/10 i 14484/40794: loss 5.904534339904785\n",
            "e 1/10 i 14485/40794: loss 5.693230628967285\n",
            "e 1/10 i 14486/40794: loss 5.882652759552002\n",
            "e 1/10 i 14487/40794: loss 6.098033428192139\n",
            "e 1/10 i 14488/40794: loss 5.990846633911133\n",
            "e 1/10 i 14489/40794: loss 6.001810550689697\n",
            "e 1/10 i 14490/40794: loss 6.071527004241943\n",
            "e 1/10 i 14491/40794: loss 5.968613624572754\n",
            "e 1/10 i 14492/40794: loss 5.847674369812012\n",
            "e 1/10 i 14493/40794: loss 5.88223934173584\n",
            "e 1/10 i 14494/40794: loss 5.7683634757995605\n",
            "e 1/10 i 14495/40794: loss 5.894442558288574\n",
            "e 1/10 i 14496/40794: loss 5.970450401306152\n",
            "e 1/10 i 14497/40794: loss 6.170766830444336\n",
            "e 1/10 i 14498/40794: loss 5.829358100891113\n",
            "e 1/10 i 14499/40794: loss 6.08516263961792\n",
            "e 1/10 i 14500/40794: loss 5.765777587890625\n",
            "e 1/10 i 14501/40794: loss 6.1468353271484375\n",
            "e 1/10 i 14502/40794: loss 6.356804370880127\n",
            "e 1/10 i 14503/40794: loss 5.950447082519531\n",
            "e 1/10 i 14504/40794: loss 6.0182600021362305\n",
            "e 1/10 i 14505/40794: loss 5.549067974090576\n",
            "e 1/10 i 14506/40794: loss 5.936717510223389\n",
            "e 1/10 i 14507/40794: loss 5.814419746398926\n",
            "e 1/10 i 14508/40794: loss 5.769453048706055\n",
            "e 1/10 i 14509/40794: loss 5.894482135772705\n",
            "e 1/10 i 14510/40794: loss 5.9390153884887695\n",
            "e 1/10 i 14511/40794: loss 5.999826908111572\n",
            "e 1/10 i 14512/40794: loss 5.972841262817383\n",
            "e 1/10 i 14513/40794: loss 5.865661144256592\n",
            "e 1/10 i 14514/40794: loss 5.836770534515381\n",
            "e 1/10 i 14515/40794: loss 5.719148635864258\n",
            "e 1/10 i 14516/40794: loss 5.8904314041137695\n",
            "e 1/10 i 14517/40794: loss 5.878809452056885\n",
            "e 1/10 i 14518/40794: loss 5.768503665924072\n",
            "e 1/10 i 14519/40794: loss 5.7342424392700195\n",
            "e 1/10 i 14520/40794: loss 5.534579277038574\n",
            "e 1/10 i 14521/40794: loss 6.233952522277832\n",
            "e 1/10 i 14522/40794: loss 6.035491943359375\n",
            "e 1/10 i 14523/40794: loss 5.8454060554504395\n",
            "e 1/10 i 14524/40794: loss 5.767972946166992\n",
            "e 1/10 i 14525/40794: loss 6.189707279205322\n",
            "e 1/10 i 14526/40794: loss 5.965494632720947\n",
            "e 1/10 i 14527/40794: loss 6.035521507263184\n",
            "e 1/10 i 14528/40794: loss 6.009587287902832\n",
            "e 1/10 i 14529/40794: loss 6.127652645111084\n",
            "e 1/10 i 14530/40794: loss 5.874115467071533\n",
            "e 1/10 i 14531/40794: loss 5.895864486694336\n",
            "e 1/10 i 14532/40794: loss 5.829665660858154\n",
            "e 1/10 i 14533/40794: loss 6.172757625579834\n",
            "e 1/10 i 14534/40794: loss 5.7853007316589355\n",
            "e 1/10 i 14535/40794: loss 6.084641933441162\n",
            "e 1/10 i 14536/40794: loss 6.042815685272217\n",
            "e 1/10 i 14537/40794: loss 5.633707046508789\n",
            "e 1/10 i 14538/40794: loss 6.024066925048828\n",
            "e 1/10 i 14539/40794: loss 5.816426753997803\n",
            "e 1/10 i 14540/40794: loss 6.080348491668701\n",
            "e 1/10 i 14541/40794: loss 6.080013751983643\n",
            "e 1/10 i 14542/40794: loss 5.969429016113281\n",
            "e 1/10 i 14543/40794: loss 5.7754011154174805\n",
            "e 1/10 i 14544/40794: loss 5.791701793670654\n",
            "e 1/10 i 14545/40794: loss 5.888182640075684\n",
            "e 1/10 i 14546/40794: loss 5.57330322265625\n",
            "e 1/10 i 14547/40794: loss 6.153447151184082\n",
            "e 1/10 i 14548/40794: loss 6.150572299957275\n",
            "e 1/10 i 14549/40794: loss 6.069546699523926\n",
            "e 1/10 i 14550/40794: loss 5.689242362976074\n",
            "e 1/10 i 14551/40794: loss 5.776546955108643\n",
            "e 1/10 i 14552/40794: loss 5.878449440002441\n",
            "e 1/10 i 14553/40794: loss 5.741745471954346\n",
            "e 1/10 i 14554/40794: loss 6.2951178550720215\n",
            "e 1/10 i 14555/40794: loss 5.965802192687988\n",
            "e 1/10 i 14556/40794: loss 5.879063606262207\n",
            "e 1/10 i 14557/40794: loss 5.847339153289795\n",
            "e 1/10 i 14558/40794: loss 5.824704170227051\n",
            "e 1/10 i 14559/40794: loss 5.98607873916626\n",
            "e 1/10 i 14560/40794: loss 6.195404529571533\n",
            "e 1/10 i 14561/40794: loss 6.3399553298950195\n",
            "e 1/10 i 14562/40794: loss 6.189762592315674\n",
            "e 1/10 i 14563/40794: loss 5.650515079498291\n",
            "e 1/10 i 14564/40794: loss 5.791806221008301\n",
            "e 1/10 i 14565/40794: loss 6.55806303024292\n",
            "e 1/10 i 14566/40794: loss 5.713665008544922\n",
            "e 1/10 i 14567/40794: loss 5.989533424377441\n",
            "e 1/10 i 14568/40794: loss 5.942939281463623\n",
            "e 1/10 i 14569/40794: loss 5.92582368850708\n",
            "e 1/10 i 14570/40794: loss 5.749000072479248\n",
            "e 1/10 i 14571/40794: loss 6.3173604011535645\n",
            "e 1/10 i 14572/40794: loss 5.955662250518799\n",
            "e 1/10 i 14573/40794: loss 6.219718933105469\n",
            "e 1/10 i 14574/40794: loss 5.895601749420166\n",
            "e 1/10 i 14575/40794: loss 5.99945592880249\n",
            "e 1/10 i 14576/40794: loss 5.978583335876465\n",
            "e 1/10 i 14577/40794: loss 6.023240089416504\n",
            "e 1/10 i 14578/40794: loss 6.133534908294678\n",
            "e 1/10 i 14579/40794: loss 5.946096420288086\n",
            "e 1/10 i 14580/40794: loss 5.974259853363037\n",
            "e 1/10 i 14581/40794: loss 6.033848762512207\n",
            "e 1/10 i 14582/40794: loss 5.700766563415527\n",
            "e 1/10 i 14583/40794: loss 5.731578350067139\n",
            "e 1/10 i 14584/40794: loss 5.774971008300781\n",
            "e 1/10 i 14585/40794: loss 5.736953258514404\n",
            "e 1/10 i 14586/40794: loss 6.031131744384766\n",
            "e 1/10 i 14587/40794: loss 5.967648506164551\n",
            "e 1/10 i 14588/40794: loss 6.022911548614502\n",
            "e 1/10 i 14589/40794: loss 5.946019649505615\n",
            "e 1/10 i 14590/40794: loss 6.043298721313477\n",
            "e 1/10 i 14591/40794: loss 5.807354927062988\n",
            "e 1/10 i 14592/40794: loss 5.889983177185059\n",
            "e 1/10 i 14593/40794: loss 5.719300746917725\n",
            "e 1/10 i 14594/40794: loss 5.6911797523498535\n",
            "e 1/10 i 14595/40794: loss 5.671030521392822\n",
            "e 1/10 i 14596/40794: loss 6.23712158203125\n",
            "e 1/10 i 14597/40794: loss 6.173614025115967\n",
            "e 1/10 i 14598/40794: loss 6.170256614685059\n",
            "e 1/10 i 14599/40794: loss 5.9040751457214355\n",
            "e 1/10 i 14600/40794: loss 5.92411994934082\n",
            "e 1/10 i 14601/40794: loss 6.21703577041626\n",
            "e 1/10 i 14602/40794: loss 5.901836395263672\n",
            "e 1/10 i 14603/40794: loss 5.880305290222168\n",
            "e 1/10 i 14604/40794: loss 5.972448348999023\n",
            "e 1/10 i 14605/40794: loss 5.94355583190918\n",
            "e 1/10 i 14606/40794: loss 5.987523078918457\n",
            "e 1/10 i 14607/40794: loss 6.0946946144104\n",
            "e 1/10 i 14608/40794: loss 6.018165588378906\n",
            "e 1/10 i 14609/40794: loss 5.8859710693359375\n",
            "e 1/10 i 14610/40794: loss 6.09991455078125\n",
            "e 1/10 i 14611/40794: loss 5.699182987213135\n",
            "e 1/10 i 14612/40794: loss 5.796188831329346\n",
            "e 1/10 i 14613/40794: loss 5.999824047088623\n",
            "e 1/10 i 14614/40794: loss 5.741744518280029\n",
            "e 1/10 i 14615/40794: loss 6.047318458557129\n",
            "e 1/10 i 14616/40794: loss 5.941958904266357\n",
            "e 1/10 i 14617/40794: loss 6.049513339996338\n",
            "e 1/10 i 14618/40794: loss 5.604682922363281\n",
            "e 1/10 i 14619/40794: loss 5.729866981506348\n",
            "e 1/10 i 14620/40794: loss 6.004993915557861\n",
            "e 1/10 i 14621/40794: loss 5.800084114074707\n",
            "e 1/10 i 14622/40794: loss 5.943184852600098\n",
            "e 1/10 i 14623/40794: loss 5.720203876495361\n",
            "e 1/10 i 14624/40794: loss 5.862754821777344\n",
            "e 1/10 i 14625/40794: loss 6.128891944885254\n",
            "e 1/10 i 14626/40794: loss 5.881237983703613\n",
            "e 1/10 i 14627/40794: loss 5.987366199493408\n",
            "e 1/10 i 14628/40794: loss 5.998386383056641\n",
            "e 1/10 i 14629/40794: loss 5.740377426147461\n",
            "e 1/10 i 14630/40794: loss 5.812793254852295\n",
            "e 1/10 i 14631/40794: loss 5.822817802429199\n",
            "e 1/10 i 14632/40794: loss 6.056473731994629\n",
            "e 1/10 i 14633/40794: loss 5.922440052032471\n",
            "e 1/10 i 14634/40794: loss 6.2112274169921875\n",
            "e 1/10 i 14635/40794: loss 5.801331996917725\n",
            "e 1/10 i 14636/40794: loss 5.980143070220947\n",
            "e 1/10 i 14637/40794: loss 6.114249229431152\n",
            "e 1/10 i 14638/40794: loss 6.0309247970581055\n",
            "e 1/10 i 14639/40794: loss 5.936822414398193\n",
            "e 1/10 i 14640/40794: loss 5.807518482208252\n",
            "e 1/10 i 14641/40794: loss 5.581216335296631\n",
            "e 1/10 i 14642/40794: loss 5.884014129638672\n",
            "e 1/10 i 14643/40794: loss 6.002953052520752\n",
            "e 1/10 i 14644/40794: loss 5.971286773681641\n",
            "e 1/10 i 14645/40794: loss 5.942612171173096\n",
            "e 1/10 i 14646/40794: loss 5.956462860107422\n",
            "e 1/10 i 14647/40794: loss 5.821172714233398\n",
            "e 1/10 i 14648/40794: loss 5.915412425994873\n",
            "e 1/10 i 14649/40794: loss 5.915683269500732\n",
            "e 1/10 i 14650/40794: loss 5.7886247634887695\n",
            "e 1/10 i 14651/40794: loss 6.13637113571167\n",
            "e 1/10 i 14652/40794: loss 6.01149320602417\n",
            "e 1/10 i 14653/40794: loss 5.737305641174316\n",
            "e 1/10 i 14654/40794: loss 6.125430583953857\n",
            "e 1/10 i 14655/40794: loss 5.830605506896973\n",
            "e 1/10 i 14656/40794: loss 5.740102767944336\n",
            "e 1/10 i 14657/40794: loss 5.9124579429626465\n",
            "e 1/10 i 14658/40794: loss 5.86879301071167\n",
            "e 1/10 i 14659/40794: loss 6.162728786468506\n",
            "e 1/10 i 14660/40794: loss 5.762973785400391\n",
            "e 1/10 i 14661/40794: loss 5.732616424560547\n",
            "e 1/10 i 14662/40794: loss 5.697898864746094\n",
            "e 1/10 i 14663/40794: loss 5.826476097106934\n",
            "e 1/10 i 14664/40794: loss 5.8081955909729\n",
            "e 1/10 i 14665/40794: loss 6.346358776092529\n",
            "e 1/10 i 14666/40794: loss 6.052224159240723\n",
            "e 1/10 i 14667/40794: loss 5.844740390777588\n",
            "e 1/10 i 14668/40794: loss 5.9130473136901855\n",
            "e 1/10 i 14669/40794: loss 5.87190580368042\n",
            "e 1/10 i 14670/40794: loss 6.003140926361084\n",
            "e 1/10 i 14671/40794: loss 6.177513599395752\n",
            "e 1/10 i 14672/40794: loss 6.104208469390869\n",
            "e 1/10 i 14673/40794: loss 5.824143409729004\n",
            "e 1/10 i 14674/40794: loss 6.018456935882568\n",
            "e 1/10 i 14675/40794: loss 5.625453472137451\n",
            "e 1/10 i 14676/40794: loss 5.852025032043457\n",
            "e 1/10 i 14677/40794: loss 5.881009101867676\n",
            "e 1/10 i 14678/40794: loss 5.931771755218506\n",
            "e 1/10 i 14679/40794: loss 5.844808578491211\n",
            "e 1/10 i 14680/40794: loss 5.840628623962402\n",
            "e 1/10 i 14681/40794: loss 5.852433204650879\n",
            "e 1/10 i 14682/40794: loss 5.936007499694824\n",
            "e 1/10 i 14683/40794: loss 5.881136417388916\n",
            "e 1/10 i 14684/40794: loss 6.097685813903809\n",
            "e 1/10 i 14685/40794: loss 5.823707580566406\n",
            "e 1/10 i 14686/40794: loss 5.903381824493408\n",
            "e 1/10 i 14687/40794: loss 5.621874809265137\n",
            "e 1/10 i 14688/40794: loss 5.682253837585449\n",
            "e 1/10 i 14689/40794: loss 5.790987491607666\n",
            "e 1/10 i 14690/40794: loss 5.956855773925781\n",
            "e 1/10 i 14691/40794: loss 5.760165214538574\n",
            "e 1/10 i 14692/40794: loss 5.902574062347412\n",
            "e 1/10 i 14693/40794: loss 5.8319501876831055\n",
            "e 1/10 i 14694/40794: loss 5.9784345626831055\n",
            "e 1/10 i 14695/40794: loss 5.919310569763184\n",
            "e 1/10 i 14696/40794: loss 5.877213478088379\n",
            "e 1/10 i 14697/40794: loss 5.829574108123779\n",
            "e 1/10 i 14698/40794: loss 6.1573381423950195\n",
            "e 1/10 i 14699/40794: loss 5.668572902679443\n",
            "e 1/10 i 14700/40794: loss 6.157314300537109\n",
            "e 1/10 i 14701/40794: loss 5.7690653800964355\n",
            "e 1/10 i 14702/40794: loss 6.00706148147583\n",
            "e 1/10 i 14703/40794: loss 6.093807220458984\n",
            "e 1/10 i 14704/40794: loss 6.050516128540039\n",
            "e 1/10 i 14705/40794: loss 5.754342079162598\n",
            "e 1/10 i 14706/40794: loss 5.833450794219971\n",
            "e 1/10 i 14707/40794: loss 5.944457054138184\n",
            "e 1/10 i 14708/40794: loss 5.810445785522461\n",
            "e 1/10 i 14709/40794: loss 5.7818169593811035\n",
            "e 1/10 i 14710/40794: loss 5.821447849273682\n",
            "e 1/10 i 14711/40794: loss 5.547501087188721\n",
            "e 1/10 i 14712/40794: loss 5.6745805740356445\n",
            "e 1/10 i 14713/40794: loss 5.916994094848633\n",
            "e 1/10 i 14714/40794: loss 5.8160810470581055\n",
            "e 1/10 i 14715/40794: loss 5.983529090881348\n",
            "e 1/10 i 14716/40794: loss 5.8143110275268555\n",
            "e 1/10 i 14717/40794: loss 6.030986785888672\n",
            "e 1/10 i 14718/40794: loss 6.02640438079834\n",
            "e 1/10 i 14719/40794: loss 5.954251289367676\n",
            "e 1/10 i 14720/40794: loss 6.013077735900879\n",
            "e 1/10 i 14721/40794: loss 5.838811874389648\n",
            "e 1/10 i 14722/40794: loss 6.056764602661133\n",
            "e 1/10 i 14723/40794: loss 6.004900932312012\n",
            "e 1/10 i 14724/40794: loss 5.676003932952881\n",
            "e 1/10 i 14725/40794: loss 5.785135746002197\n",
            "e 1/10 i 14726/40794: loss 6.178768634796143\n",
            "e 1/10 i 14727/40794: loss 5.7581562995910645\n",
            "e 1/10 i 14728/40794: loss 5.993697643280029\n",
            "e 1/10 i 14729/40794: loss 6.015052795410156\n",
            "e 1/10 i 14730/40794: loss 5.837998867034912\n",
            "e 1/10 i 14731/40794: loss 5.869739532470703\n",
            "e 1/10 i 14732/40794: loss 5.615210056304932\n",
            "e 1/10 i 14733/40794: loss 5.963416576385498\n",
            "e 1/10 i 14734/40794: loss 6.053800582885742\n",
            "e 1/10 i 14735/40794: loss 5.9902167320251465\n",
            "e 1/10 i 14736/40794: loss 5.816469669342041\n",
            "e 1/10 i 14737/40794: loss 6.1200690269470215\n",
            "e 1/10 i 14738/40794: loss 6.103884696960449\n",
            "e 1/10 i 14739/40794: loss 5.787001132965088\n",
            "e 1/10 i 14740/40794: loss 5.783444404602051\n",
            "e 1/10 i 14741/40794: loss 6.063824653625488\n",
            "e 1/10 i 14742/40794: loss 6.062377452850342\n",
            "e 1/10 i 14743/40794: loss 5.816462516784668\n",
            "e 1/10 i 14744/40794: loss 5.7479071617126465\n",
            "e 1/10 i 14745/40794: loss 5.750804424285889\n",
            "e 1/10 i 14746/40794: loss 6.145530700683594\n",
            "e 1/10 i 14747/40794: loss 5.859529495239258\n",
            "e 1/10 i 14748/40794: loss 5.820196628570557\n",
            "e 1/10 i 14749/40794: loss 6.1732282638549805\n",
            "e 1/10 i 14750/40794: loss 5.824683666229248\n",
            "e 1/10 i 14751/40794: loss 5.893238067626953\n",
            "e 1/10 i 14752/40794: loss 5.8025898933410645\n",
            "e 1/10 i 14753/40794: loss 6.022897243499756\n",
            "e 1/10 i 14754/40794: loss 6.032479286193848\n",
            "e 1/10 i 14755/40794: loss 6.010868549346924\n",
            "e 1/10 i 14756/40794: loss 6.100000858306885\n",
            "e 1/10 i 14757/40794: loss 6.029139518737793\n",
            "e 1/10 i 14758/40794: loss 5.853819847106934\n",
            "e 1/10 i 14759/40794: loss 6.056823253631592\n",
            "e 1/10 i 14760/40794: loss 6.1407647132873535\n",
            "e 1/10 i 14761/40794: loss 5.559421539306641\n",
            "e 1/10 i 14762/40794: loss 5.70040225982666\n",
            "e 1/10 i 14763/40794: loss 6.268291473388672\n",
            "e 1/10 i 14764/40794: loss 6.012355327606201\n",
            "e 1/10 i 14765/40794: loss 5.869091987609863\n",
            "e 1/10 i 14766/40794: loss 5.582916259765625\n",
            "e 1/10 i 14767/40794: loss 5.775882244110107\n",
            "e 1/10 i 14768/40794: loss 6.2225189208984375\n",
            "e 1/10 i 14769/40794: loss 6.059131145477295\n",
            "e 1/10 i 14770/40794: loss 6.175179958343506\n",
            "e 1/10 i 14771/40794: loss 5.61956787109375\n",
            "e 1/10 i 14772/40794: loss 5.900561332702637\n",
            "e 1/10 i 14773/40794: loss 5.925997734069824\n",
            "e 1/10 i 14774/40794: loss 5.69792366027832\n",
            "e 1/10 i 14775/40794: loss 6.035039901733398\n",
            "e 1/10 i 14776/40794: loss 5.804360389709473\n",
            "e 1/10 i 14777/40794: loss 6.323644638061523\n",
            "e 1/10 i 14778/40794: loss 6.441326141357422\n",
            "e 1/10 i 14779/40794: loss 5.846834182739258\n",
            "e 1/10 i 14780/40794: loss 6.120358467102051\n",
            "e 1/10 i 14781/40794: loss 5.860401630401611\n",
            "e 1/10 i 14782/40794: loss 5.915093421936035\n",
            "e 1/10 i 14783/40794: loss 6.055421352386475\n",
            "e 1/10 i 14784/40794: loss 5.89164924621582\n",
            "e 1/10 i 14785/40794: loss 6.120725631713867\n",
            "e 1/10 i 14786/40794: loss 5.657852649688721\n",
            "e 1/10 i 14787/40794: loss 6.002594470977783\n",
            "e 1/10 i 14788/40794: loss 5.905542373657227\n",
            "e 1/10 i 14789/40794: loss 5.762186050415039\n",
            "e 1/10 i 14790/40794: loss 6.108561038970947\n",
            "e 1/10 i 14791/40794: loss 6.095266342163086\n",
            "e 1/10 i 14792/40794: loss 6.087197303771973\n",
            "e 1/10 i 14793/40794: loss 5.790321350097656\n",
            "e 1/10 i 14794/40794: loss 6.139962673187256\n",
            "e 1/10 i 14795/40794: loss 6.222196578979492\n",
            "e 1/10 i 14796/40794: loss 5.617443561553955\n",
            "e 1/10 i 14797/40794: loss 6.126137733459473\n",
            "e 1/10 i 14798/40794: loss 5.972438335418701\n",
            "e 1/10 i 14799/40794: loss 6.116851329803467\n",
            "e 1/10 i 14800/40794: loss 5.847301959991455\n",
            "e 1/10 i 14801/40794: loss 6.074391841888428\n",
            "e 1/10 i 14802/40794: loss 5.9177961349487305\n",
            "e 1/10 i 14803/40794: loss 6.250994682312012\n",
            "e 1/10 i 14804/40794: loss 5.87491512298584\n",
            "e 1/10 i 14805/40794: loss 5.845455169677734\n",
            "e 1/10 i 14806/40794: loss 5.722995758056641\n",
            "e 1/10 i 14807/40794: loss 5.704203128814697\n",
            "e 1/10 i 14808/40794: loss 5.88681173324585\n",
            "e 1/10 i 14809/40794: loss 6.0952911376953125\n",
            "e 1/10 i 14810/40794: loss 6.178070068359375\n",
            "e 1/10 i 14811/40794: loss 5.9639716148376465\n",
            "e 1/10 i 14812/40794: loss 5.861744403839111\n",
            "e 1/10 i 14813/40794: loss 6.171140193939209\n",
            "e 1/10 i 14814/40794: loss 6.0190043449401855\n",
            "e 1/10 i 14815/40794: loss 5.769681930541992\n",
            "e 1/10 i 14816/40794: loss 5.752397060394287\n",
            "e 1/10 i 14817/40794: loss 5.797418117523193\n",
            "e 1/10 i 14818/40794: loss 5.665901184082031\n",
            "e 1/10 i 14819/40794: loss 5.955515384674072\n",
            "e 1/10 i 14820/40794: loss 6.10761022567749\n",
            "e 1/10 i 14821/40794: loss 6.045257568359375\n",
            "e 1/10 i 14822/40794: loss 5.931221961975098\n",
            "e 1/10 i 14823/40794: loss 6.096462726593018\n",
            "e 1/10 i 14824/40794: loss 6.089305877685547\n",
            "e 1/10 i 14825/40794: loss 6.063323020935059\n",
            "e 1/10 i 14826/40794: loss 6.227362155914307\n",
            "e 1/10 i 14827/40794: loss 5.794028282165527\n",
            "e 1/10 i 14828/40794: loss 5.842644214630127\n",
            "e 1/10 i 14829/40794: loss 5.757473945617676\n",
            "e 1/10 i 14830/40794: loss 5.861832618713379\n",
            "e 1/10 i 14831/40794: loss 5.830386638641357\n",
            "e 1/10 i 14832/40794: loss 5.8256306648254395\n",
            "e 1/10 i 14833/40794: loss 6.11428165435791\n",
            "e 1/10 i 14834/40794: loss 5.893022060394287\n",
            "e 1/10 i 14835/40794: loss 6.020077705383301\n",
            "e 1/10 i 14836/40794: loss 5.863989353179932\n",
            "e 1/10 i 14837/40794: loss 6.197333812713623\n",
            "e 1/10 i 14838/40794: loss 6.191928863525391\n",
            "e 1/10 i 14839/40794: loss 6.050407409667969\n",
            "e 1/10 i 14840/40794: loss 5.84234094619751\n",
            "e 1/10 i 14841/40794: loss 5.710103511810303\n",
            "e 1/10 i 14842/40794: loss 5.887984752655029\n",
            "e 1/10 i 14843/40794: loss 6.102292060852051\n",
            "e 1/10 i 14844/40794: loss 5.986154556274414\n",
            "e 1/10 i 14845/40794: loss 5.999912738800049\n",
            "e 1/10 i 14846/40794: loss 5.6677565574646\n",
            "e 1/10 i 14847/40794: loss 5.714183807373047\n",
            "e 1/10 i 14848/40794: loss 5.5089430809021\n",
            "e 1/10 i 14849/40794: loss 6.021096229553223\n",
            "e 1/10 i 14850/40794: loss 5.847403526306152\n",
            "e 1/10 i 14851/40794: loss 6.344819068908691\n",
            "e 1/10 i 14852/40794: loss 6.275688171386719\n",
            "e 1/10 i 14853/40794: loss 5.927332878112793\n",
            "e 1/10 i 14854/40794: loss 6.074297904968262\n",
            "e 1/10 i 14855/40794: loss 5.958109378814697\n",
            "e 1/10 i 14856/40794: loss 6.222156047821045\n",
            "e 1/10 i 14857/40794: loss 5.588824272155762\n",
            "e 1/10 i 14858/40794: loss 6.034418106079102\n",
            "e 1/10 i 14859/40794: loss 5.8110737800598145\n",
            "e 1/10 i 14860/40794: loss 5.765639781951904\n",
            "e 1/10 i 14861/40794: loss 5.886219501495361\n",
            "e 1/10 i 14862/40794: loss 5.854085922241211\n",
            "e 1/10 i 14863/40794: loss 6.175755023956299\n",
            "e 1/10 i 14864/40794: loss 5.927825927734375\n",
            "e 1/10 i 14865/40794: loss 5.813824653625488\n",
            "e 1/10 i 14866/40794: loss 6.076551914215088\n",
            "e 1/10 i 14867/40794: loss 5.745856285095215\n",
            "e 1/10 i 14868/40794: loss 5.924951076507568\n",
            "e 1/10 i 14869/40794: loss 5.924638271331787\n",
            "e 1/10 i 14870/40794: loss 5.886911869049072\n",
            "e 1/10 i 14871/40794: loss 5.659624099731445\n",
            "e 1/10 i 14872/40794: loss 6.0607709884643555\n",
            "e 1/10 i 14873/40794: loss 5.87929630279541\n",
            "e 1/10 i 14874/40794: loss 5.871564865112305\n",
            "e 1/10 i 14875/40794: loss 6.094717502593994\n",
            "e 1/10 i 14876/40794: loss 6.148504257202148\n",
            "e 1/10 i 14877/40794: loss 6.060115337371826\n",
            "e 1/10 i 14878/40794: loss 5.890495777130127\n",
            "e 1/10 i 14879/40794: loss 5.804372787475586\n",
            "e 1/10 i 14880/40794: loss 5.4742279052734375\n",
            "e 1/10 i 14881/40794: loss 6.0259623527526855\n",
            "e 1/10 i 14882/40794: loss 6.154505729675293\n",
            "e 1/10 i 14883/40794: loss 5.635625839233398\n",
            "e 1/10 i 14884/40794: loss 5.710148334503174\n",
            "e 1/10 i 14885/40794: loss 5.933626174926758\n",
            "e 1/10 i 14886/40794: loss 5.643592834472656\n",
            "e 1/10 i 14887/40794: loss 5.948894023895264\n",
            "e 1/10 i 14888/40794: loss 5.771280765533447\n",
            "e 1/10 i 14889/40794: loss 5.91398286819458\n",
            "e 1/10 i 14890/40794: loss 5.915428638458252\n",
            "e 1/10 i 14891/40794: loss 6.031832695007324\n",
            "e 1/10 i 14892/40794: loss 6.067623615264893\n",
            "e 1/10 i 14893/40794: loss 5.947744369506836\n",
            "e 1/10 i 14894/40794: loss 6.0326151847839355\n",
            "e 1/10 i 14895/40794: loss 5.785741329193115\n",
            "e 1/10 i 14896/40794: loss 6.623021125793457\n",
            "e 1/10 i 14897/40794: loss 5.871112823486328\n",
            "e 1/10 i 14898/40794: loss 6.1591477394104\n",
            "e 1/10 i 14899/40794: loss 6.164681911468506\n",
            "e 1/10 i 14900/40794: loss 6.1508708000183105\n",
            "e 1/10 i 14901/40794: loss 5.877053260803223\n",
            "e 1/10 i 14902/40794: loss 5.8801116943359375\n",
            "e 1/10 i 14903/40794: loss 5.498997211456299\n",
            "e 1/10 i 14904/40794: loss 5.82448673248291\n",
            "e 1/10 i 14905/40794: loss 5.74836540222168\n",
            "e 1/10 i 14906/40794: loss 6.105762958526611\n",
            "e 1/10 i 14907/40794: loss 5.798766136169434\n",
            "e 1/10 i 14908/40794: loss 5.928067684173584\n",
            "e 1/10 i 14909/40794: loss 5.883617401123047\n",
            "e 1/10 i 14910/40794: loss 6.384784698486328\n",
            "e 1/10 i 14911/40794: loss 6.086920738220215\n",
            "e 1/10 i 14912/40794: loss 6.172998428344727\n",
            "e 1/10 i 14913/40794: loss 6.0613789558410645\n",
            "e 1/10 i 14914/40794: loss 5.940559387207031\n",
            "e 1/10 i 14915/40794: loss 6.0110182762146\n",
            "e 1/10 i 14916/40794: loss 6.0435709953308105\n",
            "e 1/10 i 14917/40794: loss 5.817415714263916\n",
            "e 1/10 i 14918/40794: loss 5.983504295349121\n",
            "e 1/10 i 14919/40794: loss 5.863683223724365\n",
            "e 1/10 i 14920/40794: loss 6.012576580047607\n",
            "e 1/10 i 14921/40794: loss 6.008096218109131\n",
            "e 1/10 i 14922/40794: loss 5.967628002166748\n",
            "e 1/10 i 14923/40794: loss 6.095912933349609\n",
            "e 1/10 i 14924/40794: loss 5.750515460968018\n",
            "e 1/10 i 14925/40794: loss 5.973827838897705\n",
            "e 1/10 i 14926/40794: loss 5.462528705596924\n",
            "e 1/10 i 14927/40794: loss 6.134605407714844\n",
            "e 1/10 i 14928/40794: loss 5.855521202087402\n",
            "e 1/10 i 14929/40794: loss 6.2556586265563965\n",
            "e 1/10 i 14930/40794: loss 5.890265941619873\n",
            "e 1/10 i 14931/40794: loss 5.823482036590576\n",
            "e 1/10 i 14932/40794: loss 5.769270896911621\n",
            "e 1/10 i 14933/40794: loss 6.0123114585876465\n",
            "e 1/10 i 14934/40794: loss 5.781691551208496\n",
            "e 1/10 i 14935/40794: loss 6.0615715980529785\n",
            "e 1/10 i 14936/40794: loss 5.632873058319092\n",
            "e 1/10 i 14937/40794: loss 6.0001420974731445\n",
            "e 1/10 i 14938/40794: loss 6.210879325866699\n",
            "e 1/10 i 14939/40794: loss 6.064935207366943\n",
            "e 1/10 i 14940/40794: loss 5.849762439727783\n",
            "e 1/10 i 14941/40794: loss 5.797049045562744\n",
            "e 1/10 i 14942/40794: loss 6.040711879730225\n",
            "e 1/10 i 14943/40794: loss 5.785412788391113\n",
            "e 1/10 i 14944/40794: loss 6.150552749633789\n",
            "e 1/10 i 14945/40794: loss 5.7507710456848145\n",
            "e 1/10 i 14946/40794: loss 6.008598804473877\n",
            "e 1/10 i 14947/40794: loss 5.798895359039307\n",
            "e 1/10 i 14948/40794: loss 5.881848335266113\n",
            "e 1/10 i 14949/40794: loss 6.07866096496582\n",
            "e 1/10 i 14950/40794: loss 5.823643684387207\n",
            "e 1/10 i 14951/40794: loss 6.105072975158691\n",
            "e 1/10 i 14952/40794: loss 5.976842880249023\n",
            "e 1/10 i 14953/40794: loss 5.642467975616455\n",
            "e 1/10 i 14954/40794: loss 6.138710975646973\n",
            "e 1/10 i 14955/40794: loss 6.3245320320129395\n",
            "e 1/10 i 14956/40794: loss 5.8652238845825195\n",
            "e 1/10 i 14957/40794: loss 5.8892669677734375\n",
            "e 1/10 i 14958/40794: loss 6.017053604125977\n",
            "e 1/10 i 14959/40794: loss 5.630766868591309\n",
            "e 1/10 i 14960/40794: loss 5.8215742111206055\n",
            "e 1/10 i 14961/40794: loss 5.88031005859375\n",
            "e 1/10 i 14962/40794: loss 5.879255294799805\n",
            "e 1/10 i 14963/40794: loss 5.844211101531982\n",
            "e 1/10 i 14964/40794: loss 6.069117069244385\n",
            "e 1/10 i 14965/40794: loss 6.030816555023193\n",
            "e 1/10 i 14966/40794: loss 5.719849586486816\n",
            "e 1/10 i 14967/40794: loss 5.869718551635742\n",
            "e 1/10 i 14968/40794: loss 6.001175403594971\n",
            "e 1/10 i 14969/40794: loss 5.839439392089844\n",
            "e 1/10 i 14970/40794: loss 6.114250659942627\n",
            "e 1/10 i 14971/40794: loss 5.956918239593506\n",
            "e 1/10 i 14972/40794: loss 5.762368679046631\n",
            "e 1/10 i 14973/40794: loss 6.007412433624268\n",
            "e 1/10 i 14974/40794: loss 5.842763423919678\n",
            "e 1/10 i 14975/40794: loss 5.84742546081543\n",
            "e 1/10 i 14976/40794: loss 5.975617408752441\n",
            "e 1/10 i 14977/40794: loss 5.857360363006592\n",
            "e 1/10 i 14978/40794: loss 5.928221702575684\n",
            "e 1/10 i 14979/40794: loss 5.940340518951416\n",
            "e 1/10 i 14980/40794: loss 5.886119365692139\n",
            "e 1/10 i 14981/40794: loss 5.687629699707031\n",
            "e 1/10 i 14982/40794: loss 5.785836219787598\n",
            "e 1/10 i 14983/40794: loss 5.95646333694458\n",
            "e 1/10 i 14984/40794: loss 5.70753812789917\n",
            "e 1/10 i 14985/40794: loss 6.1247334480285645\n",
            "e 1/10 i 14986/40794: loss 5.769477367401123\n",
            "e 1/10 i 14987/40794: loss 5.969717502593994\n"
          ]
        }
      ],
      "source": [
        "train(model, train_dataloader, optimizer, criterion, device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), f'{base_dir}/models/lstm_with_random_token_embedding.pt')"
      ],
      "metadata": {
        "id": "0-677MUqzMYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test(model, test_dataloader, criterion, device)"
      ],
      "metadata": {
        "id": "8RMwBnBFzO5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azom5bf9nU__"
      },
      "source": [
        "#### LSTM Variant B: Pre-trained embeddings & frozen\n",
        "\n",
        "An LSTM model with pre-trained GloVe embeddings as input that will be frozen while the LM is training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3xaQCLEiTAZ"
      },
      "source": [
        "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
        "\n",
        "üéØ Goal: Download **GloVe embeddings**.\n",
        "      \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kd7HcRw0iTAa"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader\n",
        "# Download the \"glove-wiki-gigaword-100\" embeddings\n",
        "glove_vectors = gensim.downloader.load('glove-wiki-gigaword-100')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxjduZePiTAa"
      },
      "source": [
        "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
        "\n",
        "üéØ Goal: Create an embedding layer with dimensions that match the input of `VanillaLSTM` model and initialize it with random weights.\n",
        "    \n",
        "üíª API hint: Use `torch.nn.Embedding` class.\n",
        "      \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SxSFs2NiTAa"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "initial_embedding_weight = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9V9MhJv_iTAa"
      },
      "source": [
        "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
        "\n",
        "üéØ Goal: Add each GloVe embedding in the respective position in the Embedding layer created in previous step.\n",
        "\n",
        "üíª API hint: Use `.key_to_index` and `.word_to_index` functions.\n",
        "      \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpb9htFliTAa"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "\n",
        "# "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fw2Vln9miTAb"
      },
      "source": [
        "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
        "\n",
        "üéØ Goal: Instantiate the **model, optimizer and loss**.\n",
        "      \n",
        "üíª Implementation hint: Choose your training settings according to the task you need to do.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85tKcWG0iTAb"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "model = ...\n",
        "optimizer = ...\n",
        "criterion = ...\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'The model has {num_params:,} trainable parameters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_Uezm3ZiTAb"
      },
      "source": [
        "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
        "\n",
        "üéØ Goal: Run **training and testing pipelines** on **10% data** (train and test) and compute perplexity.\n",
        "      \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "om8GetFxiTAb"
      },
      "outputs": [],
      "source": [
        "train(model, train_dataloader, optimizer, criterion)\n",
        "torch.save(model.state_dict(), 'models/lstm_with_frozen_glove_token_embedding.pt')\n",
        "test(model, train_dataloader, criterion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35jsDup2iTAb"
      },
      "source": [
        "#### LSTM Variant C: Pre-trained embeddings & trainable\t\n",
        "An LSTM model with pre-trained GloVe embeddings as input that will be further trained along with the LM.\n",
        "\n",
        "_Note: Use the same embedding layer you instantiated with GloVe embeddings in the previous step_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCr4iCXqmZPm",
        "outputId": "30f3bef5-b15c-401b-bd7b-c5ea933dd2ba"
      },
      "source": [
        "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
        "\n",
        "üéØ Goal: Instantiate the **model, optimizer and loss**.\n",
        "      \n",
        "üíª Implementation hint: Choose your training settings according to the task you need to do.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiM8ADB0ssYv"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "model = ...\n",
        "optimizer = ...\n",
        "criterion = ...\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'The model has {num_params:,} trainable parameters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeEbRuZ4iTAc"
      },
      "source": [
        "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
        "\n",
        "üéØ Goal: Run **training and testing pipelines** on **10% data** (train and test) and compute perplexity.\n",
        "      \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vgy6soessg1"
      },
      "outputs": [],
      "source": [
        "train(model, train_dataloader, optimizer, criterion)\n",
        "torch.save(model.state_dict(), 'models/lstm_with_nonfreezed_glove_token_embedding.pt')\n",
        "test(model, train_dataloader, criterion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuBG_yHto6Qz"
      },
      "source": [
        "---\n",
        "\n",
        "<a name=\"22\"></a>\n",
        "## 2.2 Transformer-variants\n",
        "\n",
        "For all Transformer vairants we will use the architecture of **DistilGPT2** model. DistilGPT2 (short for Distilled-GPT2) is an English-language model pre-trained with the supervision of the smallest version of Generative Pre-trained Transformer 2 (GPT-2). Like GPT-2, DistilGPT2 can be used to generate text. See more details in the [HuggingFace model card](https://huggingface.co/distilgpt2). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieaNrQIJzow-"
      },
      "source": [
        "### 2.2.1 Train DistilGPT-2 from scratch\n",
        "\n",
        "You will perform the following steps:\n",
        "\n",
        "1. Load the config of the DistilGPT-2 model using the Transformers library.\n",
        "2. Load Model class from the config and the respective tokenizer.\n",
        "3. Change input dataset to fit with the tokenization mechanism of DistilGPT-2.\n",
        "4. Split dataset into train and test.\n",
        "5. Create DataLoaders for train and test subsets.\n",
        "6. Train the model from stratch.\n",
        "7. Test the model and compute perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUWqgNzvzoj-"
      },
      "outputs": [],
      "source": [
        "model_name = \"distilgpt2\"\n",
        "tokenizer_checkpoint = \"distilgpt2\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dsz8Xg36iTAd"
      },
      "source": [
        "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
        "\n",
        "üéØ Goal: Load model config, model class and tokenizer.\n",
        "      \n",
        "üíª Implementation hint: You should load the **model instance** and not the pre-trained model weights. You should load the pre-trained tokenizer though.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLaG6iasiTAd"
      },
      "outputs": [],
      "source": [
        "MAX_SEQ_LENGTH = 128\n",
        "\n",
        "# YOUR CODE HERE\n",
        "model_config = ...\n",
        "gpt2_scratch_model = ...\n",
        "gpt_tokenizer = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7BA6SG-iTAd"
      },
      "source": [
        "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
        "\n",
        "üéØ Goal: Implement the following steps according to the in-line comments.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIEmTqi7iTAe"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "\n",
        "# add pad_token the same as the EOS token to not increase vocab size\n",
        "gpt_tokenizer.pad_token = gpt_tokenizer.eos_token\n",
        "\n",
        "# tokenize wikitext_dataset with pre-trained DistilGPT2 tokenizer\n",
        "encoded_dataset = ...\n",
        "\n",
        "# filter out sentences with length more than MAX_SEQ_LENGTH\n",
        "limited_encoded_dataset = ...\n",
        "\n",
        "# create input_ids and attention_mask columns in the dataset\n",
        "limited_encoded_dataset = ...\n",
        "\n",
        "limited_encoded_dataset = limited_encoded_dataset.remove_columns(\"text\")\n",
        "limited_encoded_dataset = limited_encoded_dataset.with_format(\"torch\")\n",
        "limited_encoded_dataset = limited_encoded_dataset.map(lambda example:\n",
        "                                                      {\"labels\": example[\"input_ids\"]})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnTTvoyMiTAe"
      },
      "source": [
        "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
        "\n",
        "üéØ Goal:  **Split** the `limited_encoded_dataset` into train and test subsets.\n",
        "    \n",
        "üíª API hint: Use `torch.utils.data.random_split` method with the given `TRAIN_RATIO`.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbv9vnLyiTAe"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "TRAIN_RATIO = 0.9\n",
        "dataset_length = len(limited_encoded_dataset)\n",
        "train_length = ...\n",
        "test_length = ...\n",
        "\n",
        "transformer_train_dataset, transformer_test_dataset = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WydKF1tIiTAf"
      },
      "source": [
        "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
        "\n",
        "üéØ Goal: Set hyperparameters according to the objective of the model.\n",
        "      \n",
        "üíª Implementation hint: You can play arround with different values for `learning_rate`.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JV7Zq35AiTAf"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=f\"{model_name}-wikitext103\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    logging_steps=100,\n",
        "    learning_rate=...,\n",
        "    save_steps=10000,\n",
        "    weight_decay=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLOGX_LQiTAg"
      },
      "source": [
        "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
        "\n",
        "üéØ Goal: Run **training** using the `Trainer` class on **10% of data**.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyBsdxwmo4E7"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForLanguageModeling(tokenizer=gpt_tokenizer, mlm=False)\n",
        "\n",
        "trainer = Trainer(...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9gVEQ6nUvcp",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "trainer.train()\n",
        "torch.save(trainer.model.state_dict(), 'models/distilgpt2-lm-from-scratch.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZsEBG8RiTAg"
      },
      "source": [
        "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
        "\n",
        "üéØ Goal: Run **testing** using the `Trainer` class and compute perplexity.\n",
        "\n",
        "      \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HiuwHEY6iTAg",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "eval_result = trainer.evaluate()\n",
        "perplexity_from_scratch = math.exp(eval_result[\"eval_loss\"])\n",
        "print(f\"The perplexity on the test dataset is {perplexity_from_scratch:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHzd5C0inhLP"
      },
      "source": [
        "### 2.2.2 Run Pre-trained GPT-2 model\n",
        "\n",
        "After training your trained-from-scratch Transformer model in the previous section, you will now use a pre-trained model to find its perplexity. Therefore, we will only perform testing of the pre-trained model on the test dataset. \n",
        "\n",
        "You will perform the following steps:\n",
        "\n",
        "1. Load pre-trained model and tokenizer\n",
        "2. Run testing and compute perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJpB8U-1iTAh"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2TokenizerFast, AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers import Trainer, TrainingArguments\n",
        "import torch\n",
        "\n",
        "model_id = \"distilgpt2\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4Ei6aFliTAh"
      },
      "source": [
        "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
        "\n",
        "üéØ Goal: Load pre-trained model and tokenizer.\n",
        "      \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yz53m57qiTAh"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "gpt2_pretrained_model = ...\n",
        "tokenizer_pretrained_gpt = ...\n",
        "tokenizer_pretrained_gpt.pad_token = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6TGvwLXiTAh"
      },
      "source": [
        "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
        "\n",
        "üéØ Goal: Set hyperparameters to set up the Trainer.\n",
        "      \n",
        "üíª Implementation hint: We will use only the inference part on the trainer.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHrdXr64iTAh"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=f\"pretrained_{model_id}-wikitext103\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    logging_steps=100,\n",
        "    learning_rate=...,\n",
        "    save_steps=10000,\n",
        "    weight_decay=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-hvHgXTiTAi"
      },
      "source": [
        "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
        "\n",
        "üéØ Goal: Run **testing** using the `Trainer` class and compute perplexity.\n",
        "      \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8n7jLG2iTAi"
      },
      "outputs": [],
      "source": [
        "pretrained_trainer = Trainer(...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rm_7YZY6iTAi"
      },
      "outputs": [],
      "source": [
        "eval_result = pretrained_trainer.evaluate()\n",
        "perplexity_pretrained_model = math.exp(eval_result[\"eval_loss\"])\n",
        "print(f\"The perplexity of pretrained {model_id} on the test dataset is {perplexity_pretrained_model:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FxdACC5iTAi"
      },
      "source": [
        "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #8e7cc3;background-color:#e4e1eb;border-radius: 15px;\">\n",
        "    \n",
        "üéâ  Excellent work! By this point, you will have implemented all language model variants.\n",
        "\n",
        "#### Part 2 - Checklist\n",
        "Here are the core building blocks you created and that you will need for Part 3:\n",
        "   \n",
        "- [X] LSTM-variants checkpoints.\n",
        "- [X] LSTM-variants ppl scores.\n",
        "- [X] Transformer-variants ppl scores.\n",
        "\n",
        "_Note: Don't forget to include the tensorboard log to every model you trained, as discussed in the `README.md` of `tensorboard/` dir._\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBdlJJKhiTAi"
      },
      "source": [
        "---\n",
        "\n",
        "<a name=\"3\"></a>\n",
        "# PART 3: Fine-tune on the Text Paraphrasing task üöÄ\n",
        "\n",
        "In this part, we will fine-tune and test the language models into the downstream task of text Paraphrasing. \n",
        "\n",
        "For this task, we will use the [MRPC dataset](https://paperswithcode.com/dataset/mrpc). Microsoft Research Paraphrase Corpus (MRPC) is a corpus consisting of 5,801 sentence pairs collected from newswire articles. Each pair is labeled if it is a paraphrase or not by human annotators. \n",
        "\n",
        " ![mrpc.png](https://github.com/CS-552/a1-ReinBentdal/blob/main/docs/mrpc.png?raw=1)\n",
        " \n",
        "## Models\n",
        "For this dataset, we will select only the ones that correspond to text paraphrasing (label 1). With those, we will test the model's ability to take as input a sentence and produce as output the paraphrased one. \n",
        "\n",
        "### Encoder-Decoder architectures: \n",
        "To create a sequence2sequence model, we will create an Encoder-decoder model with an attention mechanism similar to the week 3 exercises.\n",
        "\n",
        "More specifically you need to implement the following:\n",
        "- Preprocess the dataset to match with the format of the model's input.\n",
        "- Build a Encoder-Decoder model that will be trained from stratch on the text paraphrasing task.\n",
        "- Train and test your architectures and compute the train/validation loss score. \n",
        "\n",
        "### Transformer-based architectures:\n",
        "\n",
        "You will also run experiments with the pre-trained Transformer-based model as we did in Part 2. \n",
        "You will be using again DistilGPT2. More specifically you need to implement the following:\n",
        "\n",
        "- Preprocess the dataset to match with the format of the model's input.\n",
        "- Run training (fine-tuning) of the model on train dataset.\n",
        "- Run inference on the test set and compute evaluation scores (see section below).\n",
        "\n",
        "\n",
        "#### Evaluation for the Transformer model\n",
        "\n",
        "You will evaluate your model using ROUGE scores. \n",
        " \n",
        "**ROUGE score** stands for Recall-Oriented Understudy for Gisting Evaluation. In its simplest form ROUGE score is the quotient of the matching words under the total count of words in reference sentence. Regarding the denominator ROUGE is a recall oriented metric. \n",
        "\n",
        "![rouge.png](https://github.com/CS-552/a1-ReinBentdal/blob/main/docs/rouge.png?raw=1)\n",
        "\n",
        "**ROUGE-L score** is based on the length of the longest common subsequence (LCS). To counter the disadvantages of a pure recall metric as in ROUGE-N, Rouge-L calculates the weighted harmonic mean (or f-measure) combining the precision score and the recall score.\n",
        "\n",
        "![rouge_l.png](https://github.com/CS-552/a1-ReinBentdal/blob/main/docs/rouge_l.png?raw=1)\n",
        "\n",
        "‚ÑπÔ∏è Source: [Original article](https://clementbm.github.io/theory/2021/12/23/rouge-bleu-scores.html#bleu)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qanO0ZkaiTAj"
      },
      "source": [
        "### Load MPRC dataset and extract the paraphrased ones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0jmgYZoiTAj"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSMYsGCfiTAk"
      },
      "outputs": [],
      "source": [
        "# load dataset\n",
        "mrpc_dataset = load_dataset(\"glue\", \"mrpc\")\n",
        "MAX_SEQ_LENGTH = 64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1M57YPoiTAk"
      },
      "source": [
        "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
        "\n",
        "üéØ Goal: Keep only the **paraphrased pair** of sentences.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVjicww_iTAk"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "mrpc_dataset = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFpqFEzMiTAk"
      },
      "source": [
        "<a name=\"31\"></a>\n",
        "## 3.1 Train Encoder-Decoder models on Text Paraphrasing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVaWIpbWiTAk"
      },
      "source": [
        "In this part, you will preprocess the dataset to make it suitable for the Encoder-Decoder model by adding `<start>` and `<stop>` tokens on each sentences and then padding to the maximum sequence length. From now on, we will refer to sentence 1 as context and sentence 2 as reference. Finally, you will compute the train/validation loss score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMwsM3auiTAk"
      },
      "source": [
        "### Data Preprocessing for encoder-decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hx7Wnw_TiTAk"
      },
      "source": [
        "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
        "\n",
        "üéØ Goal: Add  `<start>` and `<stop>` tokens and pad the input to `MAX_SEQ_LENGTH` length.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lv1G-Pf7iTAl"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "rnn_word_to_idx = rnn_dataset.word_to_index\n",
        "\n",
        "# tokenizing the texts in the mrpc\n",
        "mrpc_dataset = ...\n",
        "# pad sequences\n",
        "mrpc_dataset = ...\n",
        "mrpc_dataset = mrpc_dataset.remove_columns([\"sentence1\", \"sentence2\"])\n",
        "mrpc_dataset = mrpc_dataset.with_format(\"torch\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Xj_5kHoiTAl"
      },
      "outputs": [],
      "source": [
        "mrpc_train, mrpc_validation, mrpc_test = mrpc_dataset[\"train\"], mrpc_dataset[\"validation\"], mrpc_dataset[\"test\"]\n",
        "\n",
        "mrpc_train_dataloader = DataLoader(mrpc_train, batch_size=8, shuffle=True)\n",
        "mrpc_validation_dataloader = DataLoader(mrpc_validation, batch_size=8, shuffle=False)\n",
        "mrpc_test_dataloader = DataLoader(mrpc_test, batch_size=8, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xQLggBjiTAl"
      },
      "source": [
        "### Run model fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQYokthFiTAl"
      },
      "outputs": [],
      "source": [
        "# import your end-dec model\n",
        "from src.utils import ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kO08RQdCiTAm"
      },
      "source": [
        "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
        "\n",
        "üéØ Goal:  Implement training and testing pipelines.\n",
        "  \n",
        "üíª Implementation hint: Check the pipelines we created in the exercises sessions.\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwsAT6t-k-lU"
      },
      "outputs": [],
      "source": [
        "def seq2seq_train(model, train_loader, eval_loader, optimizer, criterion, num_epoch):\n",
        "    \n",
        "    best_eval_loss = 1e3 # used to do early stopping\n",
        "        \n",
        "    # Training loop\n",
        "    for epoch in range(num_epoch):\n",
        "        running_loss = 0\n",
        "        epoch_loss = 0\n",
        "        for i, data in (enumerate(train_loader)):\n",
        "             \n",
        "            # YOUR CODE HERE\n",
        "            pass\n",
        "\n",
        "    return epoch_loss / len(train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGtrP2mSk-lV"
      },
      "outputs": [],
      "source": [
        "def seq2seq_eval(model, eval_loader, criterion):\n",
        "    # this function should be called in the train loop to monitor the performance in validation set while training.\n",
        "    \n",
        "    running_loss = 0\n",
        "    epoch_loss = 0\n",
        "    for i, data in (enumerate(eval_loader)):\n",
        "        # YOUR CODE HERE\n",
        "        pass\n",
        "\n",
        "    return epoch_loss / len(eval_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3glI_f1iTAn"
      },
      "source": [
        "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
        "\n",
        "üéØ Goal: Instantiate the **model, optimizer and loss**.\n",
        "      \n",
        "üíª Implementation hint: Choose your training settings according to the task you need to do.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PM69YsT9iTAn"
      },
      "outputs": [],
      "source": [
        "seq2seq_with_attention_model = EncoderDecoder(...)\n",
        "optimizer = ...\n",
        "criterion = ...\n",
        "num_params = sum(p.numel() for p in seq2seq_with_attention_model.parameters() if p.requires_grad)\n",
        "print(f'The model has {num_params:,} trainable parameters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gh2RofH6iTAn"
      },
      "source": [
        "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
        "\n",
        "üéØ Goal: Run **training and testing pipelines**.\n",
        "      \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONqeXDy3iTAn"
      },
      "outputs": [],
      "source": [
        "seq2seq_train(...)\n",
        "# saving the model\n",
        "torch.save(seq2seq_with_attention_model.state_dict(), \"models/rnn_seq2seq_with_attention.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4W06_v7iiTAo"
      },
      "source": [
        "<a name=\"32\"></a>\n",
        "## 3.2 Run Transformer on Text Paraphrasing\n",
        "\n",
        "In this part you will need to concatinate the paraphrased pair of sentences into one sequence to serve as input. Then we will use this input to pass it to the DistilGPT2 model for fine-tuning and testing.\n",
        "The input should be the following:\n",
        "```\n",
        "<sentence_1> <eos> <sentence_2> <eos>\n",
        "```\n",
        "where `<eos>` is the tokenizer's end-of-sequence token.\n",
        "\n",
        "From now on, we will refer to sentence 1 as `context` and sentence 2 as `reference`. \n",
        "\n",
        "Here, we use a decoder-only model (DistilGPT2) which gets the **context** as input and generates the **reference** sequence (token-by-token). Given this token-by-token generation, the nature of the model is very similar to a language model; the major difference is that in general causal language models try to predict the next token for the whole input, whereas in this case, the model should generate only the **reference**. (i.e., the **context** should be masked for loss computation).  \n",
        "\n",
        "Finally, you will compute the ROUGE scores as follows:\n",
        "\n",
        "1. You will generate 5 sequences given each context.\n",
        "2. You will compute the ROUGE-L score among these 5 generations and the **context** => `ROUGE(context, generationX)`\n",
        "3. You will select the best generation (among the 5 ones) as the predicted reference.\n",
        "4. You will compute the ROUGE-(1, 2, L) scores between the top generation (from step 3) and the **reference** => `ROUGE(reference, top-generation)`\n",
        "5. You will provide the average ROUGE-(1, 2, L) scores for all the test dataset samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lp3K01PyiTAo"
      },
      "source": [
        "### Data Preprocessing for Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEn_yV-LiTAo"
      },
      "source": [
        "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
        "\n",
        "üéØ Goal: Load pre-trained **model** and **tokenizer**.\n",
        "      \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltHbaf6riTAo"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_id = \"distilgpt2\"\n",
        "mrpc_dataset = load_dataset(\"glue\", \"mrpc\")\n",
        "\n",
        "# YOUR CODE HERE\n",
        "tokenizer_pretrained_gpt = ...\n",
        "gpt2_pretrained_model = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMuY-DEciTAo"
      },
      "source": [
        "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
        "\n",
        "üéØ Goal: Concatenate sentences, pass them to the tokenizer and clip to `MAX_SEQ_LENGTH` length.\n",
        "      \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2vZsSofiTAp"
      },
      "outputs": [],
      "source": [
        "tokenizer_pretrained_gpt.pad_token = tokenizer_pretrained_gpt.eos_token\n",
        "eos_token = tokenizer_pretrained_gpt.eos_token\n",
        "eos_token_id = tokenizer_pretrained_gpt.eos_token_id\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# concatenate sentences along with <eos> and pass them to the tokenizer\n",
        "mrpc_dataset = ...\n",
        "\n",
        "# cut input and attention mask to MAX_SEQ_LENGTH\n",
        "mrpc_dataset = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEP81F7ik-lX"
      },
      "source": [
        "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
        "\n",
        "üéØ Goal: Apply the masking technique described above (mask context sequence).\n",
        "      \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_PMOpb0iTAp"
      },
      "outputs": [],
      "source": [
        "def get_sample_label(sample):\n",
        "    # this function masks the context (by assigning -100), and makes the paraphrase the target labels\n",
        "    \n",
        "    # YOUR CODE HERE\n",
        "    \n",
        "    return {\"labels\": output_label} \n",
        "\n",
        "mrpc_dataset = mrpc_dataset.map(get_sample_label)\n",
        "\n",
        "mrpc_dataset = mrpc_dataset.with_format(\"torch\")\n",
        "mrpc_train_dataset, mrpc_eval_dataset = mrpc_dataset[\"train\"], mrpc_dataset[\"validation\"] "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WWoQKHXiTAp"
      },
      "source": [
        "### Run model fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wn2VyeWHiTAp"
      },
      "source": [
        "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
        "\n",
        "üéØ Goal: Set hyperparameters according to the objective of the model.\n",
        "      \n",
        "üíª Implementation hint: You can play arround with different values for `learning_rate`.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A46RPZ1xiTAq"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer, AutoModelForCausalLM\n",
        "\n",
        "# create the finetuning trainer\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=f\"finetune_{model_id}-MRPC\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    logging_steps=100,\n",
        "    learning_rate=...,\n",
        "    num_train_epochs=20,\n",
        "    save_steps=10000,\n",
        "    weight_decay=0.01,\n",
        "    report_to=\"none\")\n",
        "\n",
        "gpt2_pretrained_model.transformer.wte.weight.requires_grad = False\n",
        "gpt2_pretrained_model.lm_head.weight.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xlS2JYqiTAq"
      },
      "source": [
        "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
        "\n",
        "üéØ Goal: Run **training** using the `Trainer` class.\n",
        "      \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Obg-hCQtiTAq",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "paraphrasing_trainer = Trainer(...)\n",
        "\n",
        "paraphrasing_trainer.train()\n",
        "torch.save(paraphrasing_trainer.model.state_dict(), 'models/finetune-distilgpt2-mrpc.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0honxSuiTAq"
      },
      "source": [
        "### Evaluate model with ROUGE scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-9rIf4tiTAq"
      },
      "source": [
        "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
        "\n",
        "üéØ Goal: For each sample in evaluation set, generate 5 outputs and perform the ROUGE evaluation as presented in the question description above.\n",
        "\n",
        "üíª Implementation hint: Use the following API call to get top-k generations\n",
        "    \n",
        "    generated_sequences = paraphrasing_trainer.model.generate(\n",
        "        context_ids,\n",
        "        do_sample=True, \n",
        "        max_length=MAX_SEQ_LENGTH, \n",
        "        top_k=20, \n",
        "        top_p=0.95, \n",
        "        no_repeat_ngram_size=2, \n",
        "        num_return_sequences=5\n",
        "    )\n",
        "\n",
        "_Note 1: For simplicity, you can ignore the contexts that have more than 1 sentence._\n",
        "\n",
        "_Note 2: On the generated reference, if there is more that 1 sentence generated, keep only the first one._\n",
        "\n",
        "_Note 3: To split into sentences, you can use [`nltk.sent_tokenize()`](https://www.nltk.org/api/nltk.tokenize.html)._\n",
        "\n",
        "_Note 4: Use the [`evaluate.load('rouge')`](https://huggingface.co/spaces/evaluate-metric/rouge) function to compute the ROUGE metrics._\n",
        "\n",
        " \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gG4wxsZTk-lZ"
      },
      "outputs": [],
      "source": [
        "rouge_values = []\n",
        "\n",
        "for sample in tqdm(mrpc_eval_dataset):\n",
        "    \n",
        "    # pick the best candidate given ROUGE similarity to context\n",
        "    \n",
        "            \n",
        "    # compute the ROUGE value of the best candidate with the reference\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbWpKRKpiTAr"
      },
      "source": [
        "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #8e7cc3;background-color:#e4e1eb;border-radius: 15px;\">\n",
        "\n",
        "üéâ Excellent work! You just finished the code implementation parts of the assignment. \n",
        "\n",
        "#### Part 3 - Checklist\n",
        "Here are the elements you will need for the report in Part 4:\n",
        "   \n",
        "- [X] LSTM-variants scores on perplexity and their checkpoints.\n",
        "- [X] DistilGPT2 score on perplexity and its checkpoint.\n",
        "- [X] Encoder-decoder variant train/validation loss score and its checkpoint.\n",
        "- [X] DistilGPT2 ROUGE scores and its fine-tuned checkpoint.\n",
        "\n",
        "_Note: Don't forget to include the tensorboard log to every model you trained._\n",
        "\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drEstqDkUvcp"
      },
      "source": [
        "---\n",
        "\n",
        "<a name=\"4\"></a>\n",
        "# PART 4: Write your report üìò\n",
        "\n",
        "Fill in the tables with the respective scores. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaCft7HUw4uh"
      },
      "source": [
        "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;text-align:center;\">\n",
        "\n",
        "#### Perplexity results on Language Models\n",
        "\n",
        "| Model - Variant | PPL |\n",
        "|:--------- | :-----: |\n",
        "| LSTM Variant A - Embeddings trained from scratch | YOUR SCORE HERE |\n",
        "| LSTM Variant B - Pre-trained embeddings & frozen | YOUR SCORE HERE |\n",
        "| LSTM Variant C - Pre-trained embeddings & trainable | YOUR SCORE HERE |\n",
        "||||\n",
        "| DistilGPT2 - Trained from scratch | YOUR SCORE HERE |\n",
        "| Pre-trained DistilGPT2 | YOUR SCORE HERE |\n",
        "    \n",
        "#### Performance scores on Text Paraphrasing\n",
        "| Model - Variant | ROUGE-1 | ROUGE-2 | ROUGE-L | ROUGE-Lsum |\n",
        "|:--------- | :-----: | :-----: |  :-----: |  :-----: | \n",
        "| Pre-trained DistilGPT2 | YOUR SCORE HERE |YOUR SCORE HERE |  YOUR SCORE HERE |YOUR SCORE HERE |\n",
        "\n",
        "</div>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f6af81c6d3e64bec96b00107ed8907b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_687c43a5a5ef4487a1d7f513b68426e1",
              "IPY_MODEL_0681cf2e39274366bbea0c7f6321025d",
              "IPY_MODEL_c75c377f24a04bfdabf87a7e2abf9aad"
            ],
            "layout": "IPY_MODEL_89eb2c4a7f4443cb905834933285e8ae"
          }
        },
        "687c43a5a5ef4487a1d7f513b68426e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70eb72e4013c4161baf971ef72f5b5c0",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_5207855594244b4f9ca9c6efee6b92e7",
            "value": "Map: 100%"
          }
        },
        "0681cf2e39274366bbea0c7f6321025d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f385584a70a345f48304492ae47d3721",
            "max": 432434,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_474e8a45002a4a7ba2c6fb7b7bf31db6",
            "value": 432434
          }
        },
        "c75c377f24a04bfdabf87a7e2abf9aad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7df46e35c8494bfb95768fe30ec99419",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_10c65e32626f412e8722f991f9df83d5",
            "value": " 432296/432434 [01:06&lt;00:00, 5973.31 examples/s]"
          }
        },
        "89eb2c4a7f4443cb905834933285e8ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "70eb72e4013c4161baf971ef72f5b5c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5207855594244b4f9ca9c6efee6b92e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f385584a70a345f48304492ae47d3721": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "474e8a45002a4a7ba2c6fb7b7bf31db6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7df46e35c8494bfb95768fe30ec99419": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10c65e32626f412e8722f991f9df83d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f488326046cb4b21af5b5442ee6dee2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_65a3aa92603b4e5c8de4bc521d062da4",
              "IPY_MODEL_d1e44425f4fe4d0dae39fe06b56bd084",
              "IPY_MODEL_244464bc81d34b50b4c1990bde6189c7"
            ],
            "layout": "IPY_MODEL_e5e8e9bf6ddd49649e614ac10a37bee2"
          }
        },
        "65a3aa92603b4e5c8de4bc521d062da4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca2d9b64f55a4271b6ac9372804bc4a7",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_2204d598eced4892958d56533f7e323c",
            "value": "Map: 100%"
          }
        },
        "d1e44425f4fe4d0dae39fe06b56bd084": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_358aea7c7dca490b9af79202fe330939",
            "max": 362607,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_982635f64cf4436893162236db8878e6",
            "value": 362607
          }
        },
        "244464bc81d34b50b4c1990bde6189c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a04be79799414c9c8b78e00d6f9ae8c6",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_38a53dfbf1a24c63b76dd8f866640036",
            "value": " 361909/362607 [00:41&lt;00:00, 10709.19 examples/s]"
          }
        },
        "e5e8e9bf6ddd49649e614ac10a37bee2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "ca2d9b64f55a4271b6ac9372804bc4a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2204d598eced4892958d56533f7e323c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "358aea7c7dca490b9af79202fe330939": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "982635f64cf4436893162236db8878e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a04be79799414c9c8b78e00d6f9ae8c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38a53dfbf1a24c63b76dd8f866640036": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e47f84316c8245bf803b8ebf96059ea0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_209712e84d1447aebdbe7419316ab5e9",
              "IPY_MODEL_ca437d85f8b640768c4781804a418040",
              "IPY_MODEL_0e11e2d4ab964764beb3c69cae414d5e"
            ],
            "layout": "IPY_MODEL_1c4c00e452f9426491fa9f22fc20e37d"
          }
        },
        "209712e84d1447aebdbe7419316ab5e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be9f8ea727cf411480d1c602175947b7",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_724986e44a2442e499e11e4fc3bec667",
            "value": "Map: 100%"
          }
        },
        "ca437d85f8b640768c4781804a418040": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac41c7c0beff4770bedd083c705fcc77",
            "max": 362607,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2cf7c4bd748c4ff6a812d91363cc89f0",
            "value": 362607
          }
        },
        "0e11e2d4ab964764beb3c69cae414d5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_600db3a945f741f2b175ad81d07525f5",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_44c3c527aad14caab979d432335900a1",
            "value": " 362418/362607 [00:48&lt;00:00, 9386.70 examples/s]"
          }
        },
        "1c4c00e452f9426491fa9f22fc20e37d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "be9f8ea727cf411480d1c602175947b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "724986e44a2442e499e11e4fc3bec667": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ac41c7c0beff4770bedd083c705fcc77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2cf7c4bd748c4ff6a812d91363cc89f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "600db3a945f741f2b175ad81d07525f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44c3c527aad14caab979d432335900a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}